<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Notes on studying kaggle.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Notes on Kaggle</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://necromuralist.github.io/Kaggle-Competitions/">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script><link rel="prefetch" href="posts/will-a-gbdt-performance-drop-if-we-remove-the-first-tree/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-default navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://necromuralist.github.io/Kaggle-Competitions/">

                <span id="blog-title">Notes on Kaggle</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li class="active">
<a href=".">The Cloistered Monkey <span class="sr-only">(active)</span></a>
                </li>
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
	<span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="https://necromuralist.github.io/Kaggle-Competitions/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/will-a-gbdt-performance-drop-if-we-remove-the-first-tree/" class="u-url">Will a GBDT's performance drop if we remove the first tree?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Cloistered Monkey
            </span></p>
            <p class="dateline"><a href="posts/will-a-gbdt-performance-drop-if-we-remove-the-first-tree/" rel="bookmark"><time class="published dt-published" datetime="2018-08-04T19:02:32-07:00" title="2018-08-04 19:02">2018-08-04 19:02</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <p>
We'll look at how <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/"><b>Gradient Boosting</b></a> works and  answer the question:
</p>

<blockquote>
<p>
Will the performance of a  GBDT model drop dramatically if we remove the first tree?
</p>
</blockquote>

<div class="highlight"><pre><span></span><span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">seaborn</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
</pre></div>

<div id="outline-container-org89591f4" class="outline-2">
<h2 id="org89591f4">Define the Sigmoid</h2>
<div class="outline-text-2" id="text-org89591f4">
<p>
This is a function to calculate the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> which we'll use to convert probabilities to classifications.
</p>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>

<div id="outline-container-org4faadf2" class="outline-2">
<h2 id="org4faadf2">Make the dataset</h2>
<div class="outline-text-2" id="text-org4faadf2">
<p>
This will be a very simple synthetic dataset. We'll use numpy's <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html">randn</a> function to generate a dataset of random values that come from a normal distribution and then predict that the class is <i>1</i> if the value is positive and that it is <i>0</i> otherwise.
</p>
</div>

<div id="outline-container-org57f0ff0" class="outline-3">
<h3 id="org57f0ff0">Generating the Random Data</h3>
<div class="outline-text-3" id="text-org57f0ff0">
<p>
First we'll generate an array of 5000 random numbers taken from a normal distribution.
</p>

<div class="highlight"><pre><span></span><span class="n">X_all</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>

<p>
Next we'll create a target array which will have 1's for all non-negative numbers and 0's otherwise.
</p>

<div class="highlight"><pre><span></span><span class="n">y_all</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_all</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>

<p>
Now we'll use sklearn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a> function to create the training and testing data, with half the data using for training and half for testing.
</p>

<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_all</span><span class="p">,</span> <span class="n">y_all</span><span class="p">,</span>
						    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
						    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>

<div id="outline-container-org62c188c" class="outline-2">
<h2 id="org62c188c">A Decision Tree Classifier</h2>
<div class="outline-text-2" id="text-org62c188c">
<p>
We'll build a <a href="https://en.wikipedia.org/wiki/Decision_tree">Decision Tree</a> using sklearn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">Decision Tree Classifier</a> for this model.
</p>
</div>

<div id="outline-container-org455de52" class="outline-3">
<h3 id="org455de52">Building and Training the Model</h3>
<div class="outline-text-3" id="text-org455de52">
<p>
This will be a very simple tree with a depth of 1 (a <a href="https://en.wikipedia.org/wiki/Decision_stump">decision stump</a>).
</p>

<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>

<div id="outline-container-orgf501bcb" class="outline-3">
<h3 id="orgf501bcb">Measuring the Accuracy</h3>
<div class="outline-text-3" id="text-orgf501bcb">
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for a single decision stump: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for a single decision stump: 1.0
</p>


<p>
The 'tree' was able to classify all the testing data correctly.
</p>
</div>
</div>
</div>

<div id="outline-container-orge3095d6" class="outline-2">
<h2 id="orge3095d6">A Gradient Boosting Classifier</h2>
<div class="outline-text-2" id="text-orge3095d6">
<p>
While the Decision Tree Classifier only needed 1 layer, We will need 800 trees in the Gradient Boosting Classifier for it to classify the data correctly.
</p>

<p>
For convenience we will use sklearn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier</a>, the situation will be similar if you use <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a> or other Gradient Boosting implementations.
</p>
</div>

<div id="outline-container-org8b68ce0" class="outline-3">
<h3 id="org8b68ce0">Training and Fitting the Model</h3>
<div class="outline-text-3" id="text-org8b68ce0">
<div class="highlight"><pre><span></span><span class="n">gbc_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
				 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
				 <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbc_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for Gradient Booing: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbc_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for Gradient Booing: 1.0
</p>
</div>
</div>

<div id="outline-container-org8323da9" class="outline-3">
<h3 id="org8323da9">Checking the log Loss Metric</h3>
<div class="outline-text-3" id="text-org8323da9">
<p>
The <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.predict_proba"><code>predict_proba</code></a> method will give us the probabilities that a data-point belongs to a class.
</p>

<div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbc_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>

<p>
We'll use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">log loss</a> as our metric to see how the model did.
</p>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"Test logloss: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
</pre></div>

<p>
Test logloss: 0.0003138777469167194
</p>
</div>
</div>

<div id="outline-container-org3f968b2" class="outline-3">
<h3 id="org3f968b2">Some Helper Functions</h3>
<div class="outline-text-3" id="text-org3f968b2">
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">predicted_probabilities</span><span class="p">):</span>
    <span class="sd">"""applies sigmoid to predictions before calling log_loss</span>

<span class="sd">    Args:</span>
<span class="sd">     y_true: the actual classifications</span>
<span class="sd">     predicted_probabilities: probabilities that class was 1</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">predicted_probabilities</span><span class="p">))</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_loss</span><span class="p">(</span><span class="n">cumulative_predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="sd">"""prints the log-loss for the predictions</span>

<span class="sd">    Args:</span>
<span class="sd">     cumulative_predictions (numpy.Array): The cumulative predictions for the model</span>
<span class="sd">    """</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">" - Logloss using all trees:           {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
	<span class="n">compute_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">cumulative_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">" - Logloss using all trees but last:  {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
	<span class="n">compute_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">cumulative_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">" - Logloss using all trees but first: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
	<span class="n">compute_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">cumulative_predictions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">cumulative_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>

<div id="outline-container-org7ee3f5a" class="outline-3">
<h3 id="org7ee3f5a">Checking The loss</h3>
<div class="outline-text-3" id="text-org7ee3f5a">
<p>
Now we get the cummulative sum of the <b>decision function</b> (<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.staged_decision_function"><code>staged_decision_function</code></a>) for our trees. The <i>i</i>-th element is a sum of trees \[0 \ldots i-1\]. We cannot use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba">staged_predict_proba</a>, since we want to maniputate raw scores
(not probabilities). And only in the end convert the scores to probabilities using the <code>sigmoid</code>.
</p>

<div class="highlight"><pre><span></span><span class="n">gbc_cumulative_predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">gbc_model</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X_test</span><span class="p">)])[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> 
</pre></div>

<p>
Here we'll use the <code>compute_loss</code> function that we defined above to find the log loss with and without some trees.
</p>

<div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">gbc_cumulative_predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>

<ul class="org-ul">
<li>Logloss using all trees:           0.0003138777469167194</li>
<li>Logloss using all trees but last:  0.0003138777469168316</li>
<li>Logloss using all trees but first: 0.0003202438533122706</li>
</ul>
<p>
You can see that when you remove the first tree there is a difference, but not as much as you might expect. Of particular note is that if we get rid of the last tree the model performs the same as is does with it. 
</p>

<p>
Let's take a look at the plot of the cummulative decision function with differing numbers of trees. We'll plot the Cumulative Decision Score when predicting the classification of 1 as the number of stages (trees?) increases.
</p>

<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_predictions</span><span class="p">(</span><span class="n">cumulative_predictions</span><span class="p">,</span> <span class="n">identifier</span><span class="p">):</span>
    <span class="sd">"""plots the cumulative predictions</span>

<span class="sd">    Args:</span>
<span class="sd">     identifier (str): something to identify the model</span>
<span class="sd">     cumulative_predictions: predictions from trees</span>
<span class="sd">    """</span>
    <span class="n">figure</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">axe</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cumulative_predictions</span><span class="p">[:,</span> <span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"({}) Score vs Trees"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">identifier</span><span class="p">))</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Number of Trees'</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Cumulative Decision Score'</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">gbc_cumulative_predictions</span><span class="p">,</span> <span class="s2">"eta=0.01"</span><span class="p">)</span>
</pre></div>


<div class="figure">
<p><img src="posts/will-a-gbdt-performance-drop-if-we-remove-the-first-tree/cumulative_plot.png" alt=" cumulative_plot.png"></p>
</div>


<p>
You can see that the decision function improves almost linearly until about the 800th iteration and then the improvement stops. The slope of this line is connected to the learning rate, which we set in the Gradient Boosting Model.
</p>

<p>
If you remember the main formula for boosting, it is written something like this:
</p>

<p>
\[ F(x) = const + \sum\limits_{i=1}^{n}\gamma_i h_i(x) \]
</p>

<p>
In our case, \(\gamma_i\) are constant and equal to the learning rate \[\eta = 0.01\] It takes about <i>800</i> iterations to get a score of <i>8</i>, which means at every iteration the score goes up about <i>0.01</i>. This means that the first 800 terms are approximately equal to <i>0.01</i>, and the rest are almost <i>0</i>. 
</p>

<p>
Which in turn means that if we drop the last tree we lower \(F(x)\) by <i>0</i> and if we drop the first tree we lower \(F(x)\) by <i>0.01</i>, which results in a very, very small performance drop. So, even in the case of a simple dataset which can be solved with a single decision stump, with a Gradient Boosting Model we need to sum a lot of trees (roughly \(\frac{1}{\eta}\)) to approximate our golden single decision stump.
</p>
</div>

<div id="outline-container-org5d4d792" class="outline-4">
<h4 id="org5d4d792">A Bigger \(\eta\)</h4>
<div class="outline-text-4" id="text-org5d4d792">
<p>
To prove that last point let's try a larger learning rate of <i>8</i>.
</p>

<div class="highlight"><pre><span></span><span class="n">big_eta</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
				     <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">big_eta</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">big_eta</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Test logloss: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
</pre></div>

<p>
Test logloss: 3.0443902682648675e-06
</p>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for Big Eta: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">big_eta</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for Big Eta: 1.0
</p>

<div class="highlight"><pre><span></span><span class="n">big_eta_cumulative_predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">big_eta</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X_test</span><span class="p">)])[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> 
</pre></div>

<div class="highlight"><pre><span></span><span class="n">print_loss</span><span class="p">(</span><span class="n">big_eta_cumulative_predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>

<ul class="org-ul">
<li>Logloss using all trees:           3.0443902682648675e-06</li>
<li>Logloss using all trees but last:  3.0836959336434695e-06</li>
<li>Logloss using all trees but first: 2.0247047109196954</li>
</ul>
<div class="highlight"><pre><span></span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">big_eta_cumulative_predictions</span><span class="p">,</span> <span class="s2">"eta=8"</span><span class="p">)</span>
</pre></div>


<div class="figure">
<p><img src="posts/will-a-gbdt-performance-drop-if-we-remove-the-first-tree/big_eta_plot.png" alt="big_eta_plot.png"></p>
</div>

<p>
Now we see that it is crucial to have the first tree in the ensemble, the last one maybe not so much.
</p>

<p>
Even though the dataset is synthetic, a similar intuition will work with real data, except the Gradient Boosting Model can diverge with higher learning rates and a more complex dataset. 
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org7dfacbe" class="outline-2">
<h2 id="org7dfacbe">A More Realistic Data Set</h2>
<div class="outline-text-2" id="text-org7dfacbe">
<p>
Sklearn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html">make_hastie_10_2</a> function makes a ten-dimensional data set with two classifications as targets.
</p>
</div>
<div id="outline-container-orga146d40" class="outline-3">
<h3 id="orga146d40">Building the Data Set</h3>
<div class="outline-text-3" id="text-orga146d40">
<div class="highlight"><pre><span></span><span class="n">X_hastie</span><span class="p">,</span> <span class="n">y_hastie</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train_hastie</span><span class="p">,</span> <span class="n">X_test_hastie</span><span class="p">,</span> <span class="n">y_train_hastie</span><span class="p">,</span> <span class="n">y_test_hastie</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_hastie</span><span class="p">,</span>
    <span class="n">y_hastie</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>

<div id="outline-container-orgd26d240" class="outline-3">
<h3 id="orgd26d240">Our Decision Stump Again</h3>
<div class="outline-text-3" id="text-orgd26d240">
<p>
Let's try the decision stump on the new dataset.
</p>
<div class="highlight"><pre><span></span><span class="n">stump</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">stump</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_hastie</span><span class="p">,</span> <span class="n">y_train_hastie</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for a single decision stump: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">stump</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">,</span> <span class="n">y_test_hastie</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for a single decision stump: 0.5405
</p>

<p>
It dids't do quite so well, what about a decision tree in general?
</p>

<div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_hastie</span><span class="p">,</span> <span class="n">y_train_hastie</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for the Decision Tree: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">,</span> <span class="n">y_test_hastie</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for the Decision Tree: 0.7765
</p>

<p>
It does a little better, anyway.
</p>
</div>
</div>

<div id="outline-container-orgd302037" class="outline-3">
<h3 id="orgd302037">The Gradient Boosting Model</h3>
<div class="outline-text-3" id="text-orgd302037">
<div class="highlight"><pre><span></span><span class="n">gbc2_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
					<span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
					<span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbc2_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_hastie</span><span class="p">,</span> <span class="n">y_train_hastie</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbc2_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for Gradient Boosting: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">gbc2_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">,</span> <span class="n">y_test_hastie</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for Gradient Boosting: 0.9436666666666667
</p>

<div class="highlight"><pre><span></span><span class="n">gbc2_cumulative_predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">gbc2_model</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">)])[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">gbc2_cumulative_predictions</span><span class="p">,</span> <span class="n">y_test_hastie</span><span class="p">)</span>
</pre></div>

<ul class="org-ul">
<li>Logloss using all trees:           0.18059661273530772</li>
<li>Logloss using all trees but last:  0.180624604367416</li>
<li>Logloss using all trees but first: 0.18064882642423313</li>
</ul>
<p>
In this case, although there is a difference, removing the last doesn't seem to be much better than removing the first.
</p>
</div>
</div>

<div id="outline-container-org0b93572" class="outline-3">
<h3 id="org0b93572">XGBoost</h3>
<div class="outline-text-3" id="text-org0b93572">
<p>
We stated earlier that the result should be the same with other implementations of Gradient Boosting so let's see what happens if we use XGBoost instead.
</p>

<div class="highlight"><pre><span></span><span class="n">xg_model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">xg_model</span><span class="p">)</span>
<span class="n">xg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_hastie</span><span class="p">,</span> <span class="n">y_train_hastie</span><span class="p">)</span>
</pre></div>

<pre class="example">
XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=5000,
       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1)

</pre>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy for XGBoost: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xg_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">,</span> <span class="n">y_test_hastie</span><span class="p">)))</span>
</pre></div>

<p>
Accuracy for XGBoost: 0.9408333333333333
</p>

<div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">xg_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">xg_cumulative_predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xg_model</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X_test_hastie</span><span class="p">)])[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">print_loss</span><span class="p">(</span><span class="n">xg_cumulative_loss</span><span class="p">)</span>
</pre></div>

<p>
If you read the documentation for the python package for <a href="https://xgboost.readthedocs.io/en/latest/python/index.html">xgboost</a> you'll see that they didn't implement the <code>staged_decision_function</code> for the <code>XGBClassifier</code> so you can't actually run the previous block. The XGBoost classifier ran much faster than the sklearn version did, and did almost as well in accuracy.
</p>
</div>
</div>
</div>

<div id="outline-container-orge02c380" class="outline-2">
<h2 id="orge02c380">Source</h2>
<div class="outline-text-2" id="text-orge02c380">
<ul class="org-ul">
<li>“Will Performance of GBDT Model Drop Dramatically If We Remove the First Tree?” Coursera. Accessed August 5, 2018. <a href="https://www.coursera.org/learn/competitive-data-science/home/welcome">https://www.coursera.org/learn/competitive-data-science/home/welcome</a>.</li>
</ul>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/machine-learning-recap/" class="u-url">Machine Learning Recap</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Cloistered Monkey
            </span></p>
            <p class="dateline"><a href="posts/machine-learning-recap/" rel="bookmark"><time class="published dt-published" datetime="2018-08-04T18:13:44-07:00" title="2018-08-04 18:13">2018-08-04 18:13</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="outline-container-org255f5b7" class="outline-2">
<h2 id="org255f5b7">The Main Categories</h2>
<div class="outline-text-2" id="text-org255f5b7">
<p>
These are the four main categories of supervised machine learning algorithms that you'll encounter in kaggle competitions.
</p>

<ul class="org-ul">
<li>Linear Models
<ul class="org-ul">
<li>sklearn</li>
<li>vowpal rabbit (for really large datasets)</li>
</ul>
</li>
<li>Tree-Based Models
<ul class="org-ul">
<li>sklearn</li>
<li>xgboost: faster than sklearn</li>
</ul>
</li>
<li>k-Nearest Neighbors
<ul class="org-ul">
<li>sklearn</li>
</ul>
</li>
<li>Neural Networks</li>
</ul>
</div>
</div>
<div id="outline-container-org6a65f91" class="outline-2">
<h2 id="org6a65f91">The No Free Lunch Theorem</h2>
<div class="outline-text-2" id="text-org6a65f91">
<blockquote>
<p>
There is no method which outperforms all others for all tasks.
</p>
</blockquote>

<p>
You cannot assume that an algorithm that did well on one set of data will do well on another. All algorithms have weaknesses, so you have to test multiple algorithms on each data set.
</p>
</div>
</div>
<div id="outline-container-orge14962d" class="outline-2">
<h2 id="orge14962d">Summary</h2>
<div class="outline-text-2" id="text-orge14962d">
<ul class="org-ul">
<li>there is no one algorithm to rule them all</li>
<li>Linear models split spaces into two sub-spaces</li>
<li>tree-based models spit spaces into boxes</li>
<li>kNN relies on measuring the 'closeness' between points</li>
<li>Neural Networks provide non-linear decision boundaries</li>
</ul>
<p>
In general the two most powerful methods are <b>Gradient Boosted Decision Trees</b> and <b>Neural Networks</b>, but this won't always be the case.
</p>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/real-world-vs-kaggle/" class="u-url">Real-World vs Kaggle</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Cloistered Monkey
            </span></p>
            <p class="dateline"><a href="posts/real-world-vs-kaggle/" rel="bookmark"><time class="published dt-published" datetime="2018-08-04T18:02:24-07:00" title="2018-08-04 18:02">2018-08-04 18:02</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="outline-container-org5ac0b00" class="outline-2">
<h2 id="org5ac0b00">A Real World Pipeline</h2>
<div class="outline-text-2" id="text-org5ac0b00">
<ul class="org-ul">
<li>What is the business problem that you are trying to solve?</li>
<li>What is the formal version of the problem?</li>
<li>How do you collect data?</li>
<li>How do you preprocess the data?</li>
<li>How do you create the model?
<ul class="org-ul">
<li>what is the appropriate algorithm?</li>
<li>what is the correct metric?</li>
</ul>
</li>
<li>How do you evaluate the model in a real world situation?</li>
<li>How do you deploy the model?
<ul class="org-ul">
<li>How do you monitor its performance?</li>
<li>How do you update it over time?</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org9ed4319" class="outline-2">
<h2 id="org9ed4319">A Competition Pipeline</h2>
<div class="outline-text-2" id="text-org9ed4319">
<ul class="org-ul">
<li>How do you pre-process the data?</li>
<li>How do you create the model?</li>
</ul>
</div>
</div>
<div id="outline-container-orgeec38f0" class="outline-2">
<h2 id="orgeec38f0">So, how do you use a competition then?</h2>
<div class="outline-text-2" id="text-orgeec38f0">
<ul class="org-ul">
<li>It's good to learn about machine learning</li>
<li>It' not just about the algorithms, let the data guide what you do</li>
<li>Try to be creative and do things that haven't been done before</li>
</ul>
</div>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/kaggle-mechanics/" class="u-url">Kaggle Mechanics</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Cloistered Monkey
            </span></p>
            <p class="dateline"><a href="posts/kaggle-mechanics/" rel="bookmark"><time class="published dt-published" datetime="2018-08-04T17:18:12-07:00" title="2018-08-04 17:18">2018-08-04 17:18</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div id="outline-container-orgce3f9e2" class="outline-2">
<h2 id="orgce3f9e2">The Basics</h2>
<div class="outline-text-2" id="text-orgce3f9e2">
</div>
<div id="outline-container-org7f26de2" class="outline-3">
<h3 id="org7f26de2">Data</h3>
<div class="outline-text-3" id="text-org7f26de2">
<p>
Every competition provides data so you can create a model, but there isn't a standardized format. Sometimes it will be CSVz, sometimes excel spreadsheets, sometimes image files, the sources can vary so you should read the data descriptions and adapt what you do to the competition. You aren't always limited to the data that is provided. If you were creating an image recognition model, for instance, it might be okay to include outside images or pre-trained models, it depends on the particulars of the competition.
</p>
</div>
</div>
<div id="outline-container-orgbecc6e6" class="outline-3">
<h3 id="orgbecc6e6">Models</h3>
<div class="outline-text-3" id="text-orgbecc6e6">
<p>
This is what you are trying to create - a representation of the population based on the data you are given that will allow you to predict outcomes based on inputs not given in the data. The model needs two main features:
</p>
<ul class="org-ul">
<li>accuracy</li>
<li>reproducibility</li>
</ul>
<p>
Note that a model is not the same as an algorithm. You might have to combine multiple algorithms in order to build your model. The key to a model is that it maps inputs to outputs.
</p>
</div>
</div>
<div id="outline-container-orgcb52dec" class="outline-3">
<h3 id="orgcb52dec">Submission</h3>
<div class="outline-text-3" id="text-orgcb52dec">
<p>
Your submission is typically your predictions on a test set. This isn't always the case but it is the most common way that the competitions are run.
</p>
</div>
</div>
<div id="outline-container-orgfcbfdce" class="outline-3">
<h3 id="orgfcbfdce">Evaluation</h3>
<div class="outline-text-3" id="text-orgfcbfdce">
<p>
How can you tell how well your model does? You need a function that maps your model and a data set to a score that evaluates how well the model does. There are many different metrics to use (accuracy, precision, recall, etc.) but the competition will choose one and tell you what it is in the description so make sure you read the description to get it.
</p>
</div>
</div>
<div id="outline-container-org813331e" class="outline-3">
<h3 id="org813331e">Leaderboard</h3>
<div class="outline-text-3" id="text-org813331e">
<p>
This is the relative ranking of the participants in the competition. This is what makes it a competition. Even if your metric tells you your model is doing well, if you are ranked at the bottom, you still won't win. There are actually two leaderboards - public and private. The evaluation dataset is split by kaggle into two sets, public and private, and during the competition the results of testing on the public data are shown on the leaderborad. Once the competition is over the leaderboard is displayed using the evaluations using the private data.
</p>
</div>
</div>
</div>
<div id="outline-container-org4a5b81d" class="outline-2">
<h2 id="org4a5b81d">Other Competitions</h2>
<div class="outline-text-2" id="text-org4a5b81d">
<p>
<a href="https://www.kaggle.com/">Kaggle</a> isn't the only one running data-science competititons. Here are some others.
</p>

<ul class="org-ul">
<li>
<a href="https://www.drivendata.org/">Driven Data</a>: Data Science competititons aimed at social problems</li>
<li>
<a href="http://codalab.org/">Coda Lab</a>: Competitions using research datasets</li>
<li>
<a href="https://www.datasciencechallenge.org/">Data Science Challenge</a>: Using data-science to solve government-scale problems.</li>
<li>
<a href="https://www.datascience.net/fr/challenge#">Data Science dot net</a>: A european data-science competition site.</li>
</ul>
</div>
</div>
    </div>
    </article>
</div>







        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
