<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Optimizing classification metrics.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Optimizing Classification Metrics | Notes on Kaggle</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-classification-metrics/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script><meta name="author" content="Cloistered Monkey">
<link rel="prev" href="../optimizing-metrics/" title="Optimizing Metrics" type="text/html">
<link rel="next" href="../mean-encoding/" title="Mean Encoding" type="text/html">
<meta property="og:site_name" content="Notes on Kaggle">
<meta property="og:title" content="Optimizing Classification Metrics">
<meta property="og:url" content="https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-classification-metrics/">
<meta property="og:description" content="Optimizing classification metrics.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-09-23T15:10:09-07:00">
<meta property="article:tag" content="notes metrics classification">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-default navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://necromuralist.github.io/Kaggle-Competitions/">

                <span id="blog-title">Notes on Kaggle</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="https://necromuralist.github.io/">The Cloistered Monkey</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Google custom search --><form method="get" action="https://www.google.com/search" class="navbar-form navbar-right" role="search">
<div class="form-group">
<input type="text" name="q" class="form-control" placeholder="Search">
</div>
<button type="submit" class="btn btn-primary">
	<span class="glyphicon glyphicon-search"></span>
</button>
<input type="hidden" name="sitesearch" value="https://necromuralist.github.io/Kaggle-Competitions/">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.org" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Optimizing Classification Metrics</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Cloistered Monkey
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2018-09-23T15:10:09-07:00" itemprop="datePublished" title="2018-09-23 15:10">2018-09-23 15:10</time></a></p>
            
        <p class="sourceline"><a href="index.org" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div id="outline-container-org1ed19de" class="outline-2">
<h2 id="org1ed19de">Introduction</h2>
<div class="outline-text-2" id="text-org1ed19de">
<p>
The <i>target metric</i> is what the competition scores you on, but it isn't always the easiest metric to tune your model on. Sometimes you need to pick and <i>optimization metric</i> to tune your model that isn't exactly the same but works well enough.
</p>
</div>
</div>
<div id="outline-container-org7234689" class="outline-2">
<h2 id="org7234689">LogLoss</h2>
<div class="outline-text-2" id="text-org7234689">
<p>
To optimize log-loss you just have to match it to the right model.
</p>
<ul class="org-ul">
<li>Tree Based: XGBoost, LightGBM</li>
<li>Linear: sklearn.&lt;something&gt;Regression, sklearn.SGDRegressor, Vowpal Wabbit</li>
<li>Neural Nets: PyTorch, Keras, Tensorflow, etc.</li>
</ul>
<p>
Random Forests turn out to do poorly with Log Loss.
</p>
</div>
<div id="outline-container-orgbd53945" class="outline-3">
<h3 id="orgbd53945">Probability Calibration</h3>
<div class="outline-text-3" id="text-orgbd53945">
<p>
If you take all the rows with the same score, then the fraction of them that have a class of 1 should match the score (so if they all have a score of 0.8, then 80% of them should be 1 and 20% should be 0). If the fraction is off, then you need to calibrate the probabilities. To do this take your model and then send its outputs to a model that does better with Log Loss. So if you want to use a Random Forest, you would train your model using AUC as the metric then use the predictions to train another model like a neural net and have it use Log Loss as the metric.
</p>
</div>
<div id="outline-container-orgd9944e7" class="outline-4">
<h4 id="orgd9944e7">Platt Scaling</h4>
<div class="outline-text-4" id="text-orgd9944e7">
<p>
Fit a Logistic Regression to your predictions
</p>
</div>
</div>
<div id="outline-container-org0fc89fd" class="outline-4">
<h4 id="org0fc89fd">Isotonic Regression</h4>
<div class="outline-text-4" id="text-org0fc89fd">
<p>
Fit an Isotonic Regression to your predictions
</p>
</div>
</div>
<div id="outline-container-org57e9759" class="outline-4">
<h4 id="org57e9759">Stacking</h4>
<div class="outline-text-4" id="text-org57e9759">
<p>
Fit XGBoost or a neural net to your predictions
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbb94aaa" class="outline-2">
<h2 id="orgbb94aaa">Accuracy</h2>
<div class="outline-text-2" id="text-orgbb94aaa">
<p>
Accuracy is a difficult metric to optimize because it isn't differentiable. To optimize the accuracy metric you need to use a different metric (a proxy metric) like log-loss and then tune the threshold.
</p>
</div>
</div>

<div id="outline-container-org7c88824" class="outline-2">
<h2 id="org7c88824">Area Under the Curve (AUC)</h2>
<div class="outline-text-2" id="text-org7c88824">
<p>
Some models work with it so if you can choose one of these models.
</p>
<ul class="org-ul">
<li>Tree-Based: XGBoost, LightGBM</li>
<li>Linear: (don't use)</li>
<li>Neural Nets: PyTorch, Keras, TensorFlow (but not by default)</li>
</ul>
<p>
In practice you can optimize the model to log-loss.
</p>
</div>
</div>
<div id="outline-container-org573fd40" class="outline-2">
<h2 id="org573fd40">Quadratic Weighted Kappa</h2>
<div class="outline-text-2" id="text-org573fd40">
<ol class="org-ol">
<li>Optimize on the Mean Squared Error then optimize the thresholds.</li>
</ol>
</div>
</div>
<div id="outline-container-org51ec19c" class="outline-2">
<h2 id="org51ec19c">Other Sources</h2>
<div class="outline-text-2" id="text-org51ec19c">
</div>
<div id="outline-container-org2912366" class="outline-3">
<h3 id="org2912366">Classification</h3>
<div class="outline-text-3" id="text-org2912366">
<ul class="org-ul">
<li><a href="http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references">Evaluation Metrics for Classification Problems</a></li>
<li><a href="https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria">Descision Trees: <i>Gini</i> vs <i>Entropy</i></a></li>
<li><a href="http://www.navan.name/roc/">Understanding ROC Curves</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgb184bd5" class="outline-3">
<h3 id="orgb184bd5">Ranking</h3>
<div class="outline-text-3" id="text-orgb184bd5">
<ul class="org-ul">
<li>
<a href="https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf">Learning to Rank Using Gradient Descent</a> - source of pairwise AUC optimization</li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf">From RankNet to LambdaRank</a></li>
<li>
<a href="https://sourceforge.net/p/lemur/wiki/RankLib/">RankLib (implementation of the two previous papers</a>)</li>
<li><a href="https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview/">Learning to Rank Overview</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgbc4493f" class="outline-3">
<h3 id="orgbc4493f">Clustering</h3>
<div class="outline-text-3" id="text-orgbc4493f">
<ul class="org-ul">
<li><a href="http://nlp.uned.es/docs/amigo2007a.pdf">Comparison of clustering metrics</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgeaad512" class="outline-2">
<h2 id="orgeaad512">Practice Quiz</h2>
<div class="outline-text-2" id="text-orgeaad512">
<ol class="org-ol">
<li>What would be a logloss value for a binary classification task if we use a constant predictor \(f(x)=0.5\)? Round to two decimal places.</li>
</ol>
<p>
-0.69 (marked as wrong)
</p>

<ol class="org-ol">
<li>What is the best constant predictor for the Mean Absolute Error?
<ul class="org-ul">
<li>Target 50th percentile, Target median</li>
</ul>
</li>

<li>The best constant predictor for the Mean Squared Error is:

<ul class="org-ul">
<li>Target Mean, average of the target vector</li>
</ul>
</li>
<li>The best Constant prediction for the Area Under the Curve is:
<ul class="org-ul">
<li>Any constant will lead to the same AUC value (should also mark target median, target mean, 1, 0.5, Target Mean divided by target variance - since any constant will lead to the same value, they are all the same)</li>
</ul>
</li>
<li>Suppose the target metric is \(r^2\), what optimization loss should we use for our models?
<ul class="org-ul">
<li>RMSE, MSE</li>
</ul>
</li>
<li>Calculate the AUC for these predictions:</li>
</ol>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col class="org-right">
<col class="org-right">
</colgroup>
<thead><tr>
<th scope="col" class="org-right">target</th>
<th scope="col" class="org-right">prediction</th>
</tr></thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.39</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.52</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.91</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.85</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.49</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.02</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.44</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span># from pypi
from sklearn.metrics import roc_auc_score
y_true = [1,0,1,1,1,0,0]
y_score = [0.39,0.52,0.91,0.85,0.49,0.02,0.44]
print(roc_auc_score(y_true, y_score))
</pre></div>

<pre class="example">
0.75

</pre>
</div>
</div>
<div id="outline-container-org2df8792" class="outline-2">
<h2 id="org2df8792">Quiz</h2>
<div class="outline-text-2" id="text-org2df8792">
</div>
<div id="outline-container-org6f8a96f" class="outline-3">
<h3 id="org6f8a96f">One</h3>
<div class="outline-text-3" id="text-org6f8a96f">
<p>
Suppose we solve a binary classification task and our solution is scored with Log Loss. What predictions are preferable if all the true target values are 0?
</p>

<ul class="org-ul">
<li class="off">
<code>[ ]</code> (0.4, 0.5, 0.5, 0.6) - marked wrong</li>
<li class="on">
<code>[X]</code> (0.5, 0.5, 0.5, 0.5)</li>
<li class="off">
<code>[ ]</code> (0, 0, 0, 1)</li>
</ul>
<div class="highlight"><pre><span></span>import numpy
one = numpy.array([0.4, 0.5, 0.5, 0.6])
two = numpy.array([0.5, 0.5, 0.5, 0.5])
three = numpy.array([0, 0, 0, 1])

for guess in (one, two, three):
    print(sum(-numpy.log(1 - guess))/len(guess))
</pre></div>

<pre class="example">
0.7033526791900091
0.6931471805599453
inf
/home/hades/.virtualenvs/machine-learning-studies/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log
  import sys

</pre>
</div>
</div>

<div id="outline-container-org632c6a9" class="outline-3">
<h3 id="org632c6a9">Two</h3>
<div class="outline-text-3" id="text-org632c6a9">
<p>
Suppose we solve a regression task and we optimize MSE. If we manage to lower MSE loss on either the training set or the test set, how would this affect the Pearson Correlation coefficient between the target vector and the predictions on the same set.
</p>

<p>
The correlation will also be lowered.
The correlation will not change.
</p>
<ul class="org-ul">
<li class="off">
<code>[ ]</code> The correlation will become larger. - marked wrong</li>
<li class="on">
<code>[X]</code> Any behavior is possible.</li>
</ul>
</div>
</div>

<div id="outline-container-orgee20baa" class="outline-3">
<h3 id="orgee20baa">Three</h3>
<div class="outline-text-3" id="text-orgee20baa">
<p>
What would be a best constant prediction for a multi-class classification with four classes? The solution is scored with multi-class Log Loss. The number of objects in each class in the training set is 18, 3, 15, 24.
</p>

<ul class="org-ul">
<li>Guess one: 0,1,2,3</li>
</ul>
<div class="highlight"><pre><span></span>counts = numpy.array([18, 3, 15, 24])
print(counts/counts.sum())
</pre></div>

<pre class="example">
[0.3  0.05 0.25 0.4 ]

</pre>
</div>
</div>

<div id="outline-container-org51a35f1" class="outline-3">
<h3 id="org51a35f1">Four</h3>
<div class="outline-text-3" id="text-org51a35f1">
<p>
What is the best constant predictor for the r-squared metric?
</p>
<ul class="org-ul">
<li>one minus the target mean, target mean (0 points)</li>
<li>0.5 (0 points)</li>
<li>Target Mean (same as the MSE)</li>
</ul>
</div>
</div>
<div id="outline-container-org27205b7" class="outline-3">
<h3 id="org27205b7">Five</h3>
<div class="outline-text-3" id="text-org27205b7">
<p>
Select the Correct statements
</p>
<ul class="org-ul">
<li class="on">
<code>[X]</code> Optimization loss can be the same as the target metric</li>
<li class="on">
<code>[X]</code> Optimization loss can be different from the target metric</li>
<li class="off">
<code>[ ]</code> Optimization loss is always different from the target metric</li>
<li class="off">
<code>[ ]</code> Optimization loss is always the same as the target metric</li>
</ul>
</div>
</div>

<div id="outline-container-orgbea42f8" class="outline-3">
<h3 id="orgbea42f8">Six</h3>
<div class="outline-text-3" id="text-orgbea42f8">
<p>
Suppose the target metric is <b>M1</b> and the optimization loss is <b>M2</b>. We train a model and monitor its quality on a holdout set using the metrics <b>M1</b> and <b>M2</b>.
</p>

<p>
Select the correct statement:
</p>

<ul class="org-ul">
<li class="off">
<code>[ ]</code> If the best <b>M1</b> score is attained at iteration <i>N</i>, then the best <b>M2</b> score is always attained after the n-th iteration.</li>
<li class="off">
<code>[ ]</code> If the best <b>M1</b> score is attained at iteration <i>N</i>, then the best <b>M2</b> score is always attained before the n-th iteration.</li>
<li class="off">
<code>[ ]</code> If the best <b>M1</b> score is attained at iteration <i>N</i>, then the best <b>M2</b> score is always attained at the n-th iteration.</li>
<li class="on">
<code>[X]</code> There is no definite relation between the best iterations for the <b>M1</b> score and the <b>M2</b> score.</li>
</ul>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/notes-metrics-classification/" rel="tag">notes metrics classification</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../optimizing-metrics/" rel="prev" title="Optimizing Metrics">Previous post</a>
            </li>
            <li class="next">
                <a href="../mean-encoding/" rel="next" title="Mean Encoding">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a><br>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
