#+BEGIN_COMMENT
.. title: Feature Extraction From Text and Images
.. slug: feature-extraction-from-text-and-images
.. date: 2018-08-13 07:17:52 UTC-07:00
.. tags: feature extraction, text, images, notes
.. category: notes
.. link: 
.. description: Getting features from text and image data.
.. type: text
#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 1

* How do you convert text to data?
** Two Main Methods
*** Bag Of Words
**** Vectorization
    This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
    + Sklearn implements this with [[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html][CountVectorizer]]
**** Term Frequency/Inverse Document Frequency (TF/IDF)
     This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
     + Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable
     + Inverse Document Frequency: Normalize columns to make emphasize more important features
     + Sklearn implements this with [[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html][TfidVectorizer]]
**** N-Grams
     This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
     - [[http://sklearn.feature_extraction.text.CountVectorizer][Count Vectorizer]] is sklearn's implementation
**** Text Preprocessing
   - lowercase:
     Change all the words to lower-case
   - lemmatization:
     Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)
   - stemming:
     Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)
   - stopwords:
     Remove common words (e.g. a, and, or, etc.)       
**** The Bag Of Words Pipeline
     1. Preprocessing (lowercase, lemmatization, stemming, stopwords)
     2. Create n-grams
     3. Postprocessing: TF/IDF
*** Embeddings (e.g. Word2Vec)
    - Uses neural-nets
    - much smaller vectors than bag of words
    - But each word gets a vector so many more vectors
    - similar words have similar word-vectors
* Links
** Text
*** Bag Of Words
    - [[http://scikit-learn.org/stable/modules/feature_extraction.html][SKlearn on feature extraction]]
    - [[https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/][blog post]] on extracting features from text
*** Word2Vec
    - [[https://www.tensorflow.org/tutorials/representation/word2vec][TensorFlow tutorial]]
    - [[https://rare-technologies.com/word2vec-tutorial/][Blog post tutorial]] by the author of gensim
    - [[https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/][Text Classification post]]
    - [[https://taylorwhitten.github.io/blog/word2vec][Another introduction]]
*** Natural Language Processing with Python
    - [[http://www.nltk.org/][nltk]]
    - [[https://textblob.readthedocs.io/en/dev/][TextBlob]]
** Images
*** Pre-trained Models
    - [[https://keras.io/applications/][Keras]]
    - [[https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11][How To use a pre-trained model]]
*** Fine-Tuning
    - [[https://www.tensorflow.org/hub/tutorials/image_retraining][Re-train a tensorflow image classifier]]
    - [[https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html][Fine-tuning deep learning models in keras]]
