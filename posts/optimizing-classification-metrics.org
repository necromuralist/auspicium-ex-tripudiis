#+BEGIN_COMMENT
.. title: Optimizing Classification Metrics
.. slug: optimizing-classification-metrics
.. date: 2018-09-23 15:10:09 UTC-07:00
.. tags: notes metrics classification
.. category: notes
.. link: 
.. description: Optimizing classification metrics.
.. type: text
#+END_COMMENT
* Introduction
  The /target metric/ is what the competition scores you on, but it isn't always the easiest metric to tune your model on. Sometimes you need to pick and /optimization metric/ to tune your model that isn't exactly the same but works well enough.
* LogLoss
  To optimize log-loss you just have to match it to the right model.
  - Tree Based: XGBoost, LightGBM
  - Linear: sklearn.<something>Regression, sklearn.SGDRegressor, Vowpal Wabbit
  - Neural Nets: PyTorch, Keras, Tensorflow, etc.

Random Forests turn out to do poorly with Log Loss.
** Probability Calibration
   If you take all the rows with the same score, then the fraction of them that have a class of 1 should match the score (so if they all have a score of 0.8, then 80% of them should be 1 and 20% should be 0). If the fraction is off, then you need to calibrate the probabilities. To do this take your model and then send its outputs to a model that does better with Log Loss. So if you want to use a Random Forest, you would train your model using AUC as the metric then use the predictions to train another model like a neural net and have it use Log Loss as the metric.
*** Platt Scaling
    Fit a Logistic Regression to your predictions
*** Isotonic Regression
    Fit an Isotonic Regression to your predictions
*** Stacking
    Fit XGBoost or a neural net to your predictions
* Accuracy
  Accuracy is a difficult metric to optimize because it isn't differentiable. To optimize the accuracy metric you need to use a different metric (a proxy metric) like log-loss and then tune the threshold.

* Area Under the Curve (AUC)
  Some models work with it so if you can choose one of these models.
  - Tree-Based: XGBoost, LightGBM
  - Linear: (don't use)
  - Neural Nets: PyTorch, Keras, TensorFlow (but not by default)

In practice you can optimize the model to log-loss.
* Quadratic Weighted Kappa
  1. Optimize on the Mean Squared Error then optimize the thresholds.
* Other Sources
** Classification
   - [[http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references][Evaluation Metrics for Classification Problems]]
   - [[https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria][Descision Trees: /Gini/ vs /Entropy/]]
   - [[http://www.navan.name/roc/][Understanding ROC Curves]]
** Ranking
   - [[https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf][Learning to Rank Using Gradient Descent]] - source of pairwise AUC optimization
   - [[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf][From RankNet to LambdaRank]]
   - [[https://sourceforge.net/p/lemur/wiki/RankLib/][RankLib (implementation of the two previous papers]])
   - [[https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview/][Learning to Rank Overview]]
** Clustering
   - [[http://nlp.uned.es/docs/amigo2007a.pdf][Comparison of clustering metrics]]
* Practice Quiz
1. What would be a logloss value for a binary classification task if we use a constant predictor $f(x)=0.5$? Round to two decimal places.
-0.69 (marked as wrong)

2. What is the best constant predictor for the Mean Absolute Error?
   - Target 50th percentile, Target median

3. The best constant predictor for the Mean Squared Error is:

   - Target Mean, average of the target vector
4. The best Constant prediction for the Area Under the Curve is:
   - Any constant will lead to the same AUC value (should also mark target median, target mean, 1, 0.5, Target Mean divided by target variance - since any constant will lead to the same value, they are all the same)
5. Suppose the target metric is $r^2$, what optimization loss should we use for our models?
   - RMSE, MSE
6. Calculate the AUC for these predictions:

| target | prediction |
|--------+------------|
|      1 |       0.39 |
|      0 |       0.52 |
|      1 |       0.91 |
|      1 |       0.85 |
|      1 |       0.49 |
|      0 |       0.02 |
|      0 |       0.44 |

#+BEGIN_SRC ipython :session metrics :results output :exports both
# from pypi
from sklearn.metrics import roc_auc_score
y_true = [1,0,1,1,1,0,0]
y_score = [0.39,0.52,0.91,0.85,0.49,0.02,0.44]
print(roc_auc_score(y_true, y_score))
#+END_SRC

#+RESULTS:
: 0.75
* Quiz
** One
   Suppose we solve a binary classification task and our solution is scored with Log Loss. What predictions are preferable if all the true target values are 0?

   - [ ] (0.4, 0.5, 0.5, 0.6) - marked wrong
   - [X] (0.5, 0.5, 0.5, 0.5)
   - [ ] (0, 0, 0, 1)

#+BEGIN_SRC ipython :session metrics :results output :exports both
import numpy
one = numpy.array([0.4, 0.5, 0.5, 0.6])
two = numpy.array([0.5, 0.5, 0.5, 0.5])
three = numpy.array([0, 0, 0, 1])

for guess in (one, two, three):
    print(sum(-numpy.log(1 - guess))/len(guess))
#+END_SRC

#+RESULTS:
: 0.7033526791900091
: 0.6931471805599453
: inf
: /home/hades/.virtualenvs/machine-learning-studies/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log
:   import sys

** Two 
   Suppose we solve a regression task and we optimize MSE. If we manage to lower MSE loss on either the training set or the test set, how would this affect the Pearson Correlation coefficient between the target vector and the predictions on the same set.

The correlation will also be lowered.
The correlation will not change.
 - [ ] The correlation will become larger. - marked wrong
 - [X] Any behavior is possible.

** Three
   What would be a best constant prediction for a multi-class classification with four classes? The solution is scored with multi-class Log Loss. The number of objects in each class in the training set is 18, 3, 15, 24.

   - Guess one: 0,1,2,3

#+BEGIN_SRC ipython :session metrics :results output :exports both
counts = numpy.array([18, 3, 15, 24])
print(counts/counts.sum())
#+END_SRC

#+RESULTS:
: [0.3  0.05 0.25 0.4 ]

** Four
   What is the best constant predictor for the r-squared metric?
   - one minus the target mean, target mean (0 points)
   - 0.5 (0 points)
   - Target Mean (same as the MSE)
** Five
   Select the Correct statements
 - [X] Optimization loss can be the same as the target metric
 - [X] Optimization loss can be different from the target metric
 - [ ] Optimization loss is always different from the target metric
 - [ ] Optimization loss is always the same as the target metric

** Six
   Suppose the target metric is *M1* and the optimization loss is *M2*. We train a model and monitor its quality on a holdout set using the metrics *M1* and *M2*.

Select the correct statement:

 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained after the n-th iteration.
 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained before the n-th iteration.
 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained at the n-th iteration.
 - [X] There is no definite relation between the best iterations for the *M1* score and the *M2* score.
