#+BEGIN_COMMENT
.. title: Optimizing Classification Metrics
.. slug: optimizing-classification-metrics
.. date: 2018-09-23 15:10:09 UTC-07:00
.. tags: notes, metrics, classification
.. category: notes
.. link: 
.. description: Optimizing classification metrics.
.. type: text
#+END_COMMENT
* Introduction
  The /target metric/ is what the competition scores you on, but it isn't always the easiest metric to tune your model on. Sometimes you need to pick and /optimization metric/ to tune your model that isn't exactly the same but works well enough.
* LogLoss
  To optimize log-loss you just have to match it to the right model.
  - Tree Based: XGBoost, LightGBM
  - Linear: sklearn.<something>Regression, sklearn.SGDRegressor, Vowpal Wabbit
  - Neural Nets: PyTorch, Keras, Tensorflow, etc.

Random Forests turn out to do poorly with Log Loss.
** Probability Calibration
   If you take all the rows with the same score, then the fraction of them that have a class of 1 should match the score (so if they all have a score of 0.8, then 80% of them should be 1 and 20% should be 0). If the fraction is off, then you need to calibrate the probabilities. To do this take your model and then send its outputs to a model that does better with Log Loss. So if you want to use a Random Forest, you would train your model using AUC as the metric then use the predictions to train another model like a neural net and have it use Log Loss as the metric.
*** Platt Scaling
    Fit a Logistic Regression to your predictions
*** Isotonic Regression
    Fit an Isotonic Regression to your predictions
*** Stacking
    Fit XGBoost or a neural net to your predictions
* Accuracy
  Accuracy is a difficult metric to optimize because it isn't differentiable. To optimize the accuracy metric you need to use a different metric (a proxy metric) like log-loss and then tune the threshold.

* Area Under the Curve (AUC)
  Some models work with it so if you can choose one of these models.
  - Tree-Based: XGBoost, LightGBM
  - Linear: (don't use)
  - Neural Nets: PyTorch, Keras, TensorFlow (but not by default)

In practice you can optimize the model to log-loss.
* Quadratic Weighted Kappa
  1. Optimize on the Mean Squared Error then optimize the thresholds.
* Other Sources
** Classification
   - [[http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references][Evaluation Metrics for Classification Problems]]
   - [[https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria][Descision Trees: /Gini/ vs /Entropy/]]
   - [[http://www.navan.name/roc/][Understanding ROC Curves]]
** Ranking
   - [[https://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf][Learning to Rank Using Gradient Descent]] - source of pairwise AUC optimization
   - [[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf][From RankNet to LambdaRank]]
   - [[https://sourceforge.net/p/lemur/wiki/RankLib/][RankLib (implementation of the two previous papers]])
   - [[https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview/][Learning to Rank Overview]]
** Clustering
   - [[http://nlp.uned.es/docs/amigo2007a.pdf][Comparison of clustering metrics]]
