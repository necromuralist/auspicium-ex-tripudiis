#+BEGIN_COMMENT
.. title: Classification Metrics Quiz
.. slug: classification-metrics-quiz
.. date: 2018-10-07 13:01:15 UTC-07:00
.. tags: quiz, metrics, classification
.. category: quiz
.. link: 
.. description: Quizzes on optimizing classification metrics.
.. type: text
.. status: private
#+END_COMMENT

* Practice Quiz
1. What would be a logloss value for a binary classification task if we use a constant predictor $f(x)=0.5$? Round to two decimal places.
-0.69 (marked as wrong)

2. What is the best constant predictor for the Mean Absolute Error?
   - Target 50th percentile, Target median

3. The best constant predictor for the Mean Squared Error is:

   - Target Mean, average of the target vector
4. The best Constant prediction for the Area Under the Curve is:
   - Any constant will lead to the same AUC value (should also mark target median, target mean, 1, 0.5, Target Mean divided by target variance - since any constant will lead to the same value, they are all the same)
5. Suppose the target metric is $r^2$, what optimization loss should we use for our models?
   - RMSE, MSE
6. Calculate the AUC for these predictions:

| target | prediction |
|--------+------------|
|      1 |       0.39 |
|      0 |       0.52 |
|      1 |       0.91 |
|      1 |       0.85 |
|      1 |       0.49 |
|      0 |       0.02 |
|      0 |       0.44 |

#+BEGIN_SRC ipython :session metrics :results output :exports both
# from pypi
from sklearn.metrics import roc_auc_score
y_true = [1,0,1,1,1,0,0]
y_score = [0.39,0.52,0.91,0.85,0.49,0.02,0.44]
print(roc_auc_score(y_true, y_score))
#+END_SRC

#+RESULTS:
: 0.75
* Quiz
** One
   Suppose we solve a binary classification task and our solution is scored with Log Loss. What predictions are preferable if all the true target values are 0?

   - [ ] (0.4, 0.5, 0.5, 0.6) - marked wrong
   - [X] (0.5, 0.5, 0.5, 0.5)
   - [ ] (0, 0, 0, 1)

#+BEGIN_SRC ipython :session metrics :results output :exports both
import numpy
one = numpy.array([0.4, 0.5, 0.5, 0.6])
two = numpy.array([0.5, 0.5, 0.5, 0.5])
three = numpy.array([0, 0, 0, 1])

for guess in (one, two, three):
    print(sum(-numpy.log(1 - guess))/len(guess))
#+END_SRC

#+RESULTS:
: 0.7033526791900091
: 0.6931471805599453
: inf
: /home/hades/.virtualenvs/machine-learning-studies/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log
:   import sys

** Two 
   Suppose we solve a regression task and we optimize MSE. If we manage to lower MSE loss on either the training set or the test set, how would this affect the Pearson Correlation coefficient between the target vector and the predictions on the same set.

The correlation will also be lowered.
The correlation will not change.
 - [ ] The correlation will become larger. - marked wrong
 - [X] Any behavior is possible.

** Three
   What would be a best constant prediction for a multi-class classification with four classes? The solution is scored with multi-class Log Loss. The number of objects in each class in the training set is 18, 3, 15, 24.

   - Guess one: 0,1,2,3

#+BEGIN_SRC ipython :session metrics :results output :exports both
counts = numpy.array([18, 3, 15, 24])
print(counts/counts.sum())
#+END_SRC

#+RESULTS:
: [0.3  0.05 0.25 0.4 ]

** Four
   What is the best constant predictor for the r-squared metric?
   - one minus the target mean, target mean (0 points)
   - 0.5 (0 points)
   - Target Mean (same as the MSE)
** Five
   Select the Correct statements
 - [X] Optimization loss can be the same as the target metric
 - [X] Optimization loss can be different from the target metric
 - [ ] Optimization loss is always different from the target metric
 - [ ] Optimization loss is always the same as the target metric

** Six
   Suppose the target metric is *M1* and the optimization loss is *M2*. We train a model and monitor its quality on a holdout set using the metrics *M1* and *M2*.

Select the correct statement:

 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained after the n-th iteration.
 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained before the n-th iteration.
 - [ ] If the best *M1* score is attained at iteration /N/, then the best *M2* score is always attained at the n-th iteration.
 - [X] There is no definite relation between the best iterations for the *M1* score and the *M2* score.
