<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/cat_notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 04 Sep 2018 17:07:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Validation</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org1dae6f9" class="outline-2"&gt;
&lt;h2 id="org1dae6f9"&gt;Validation and Overfitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1dae6f9"&gt;
&lt;p&gt;
To prevent your model from overfitting to the training set, you can hold out some of the training set and use it to validate the model after it has been fit to the rest of the training set.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Underfitting: your model isn't complex enough to capture the data&lt;/li&gt;
&lt;li&gt;Overfitting: your model is too complex and it is modelling noise in the data&lt;/li&gt;
&lt;li&gt;In a competition, if your model does well on the validation set but not on the test set, then it probably overfit the data you had&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes validation</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</guid><pubDate>Tue, 04 Sep 2018 15:01:59 GMT</pubDate></item><item><title>Springleaf Competition</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orge357c55" class="outline-2"&gt;
&lt;h2 id="orge357c55"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge357c55"&gt;
&lt;p&gt;
The data comes from the &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt; competition. Springleaf makes personal loans and wanted to be able to predict who would respond to their direct mail offers. Submissions are evaluated on the area under the ROC curve between the predicted probability and the target. The submission file should have an ID and the probability that the person would respond.
&lt;/p&gt;

&lt;pre class="example"&gt;
ID,target
1,0.35
3,0.01
6,0.93333
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>example competition notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</guid><pubDate>Tue, 04 Sep 2018 13:35:25 GMT</pubDate></item><item><title>Exploratory Data Analysis</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgfd31e4e" class="outline-2"&gt;
&lt;h2 id="orgfd31e4e"&gt;Building Your Intuition About the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfd31e4e"&gt;
&lt;p&gt;
The first thing to do is to see if you can build up a mental model of what the data is about.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb70a1bc" class="outline-3"&gt;
&lt;h3 id="orgb70a1bc"&gt;Get Domain Knowledge&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb70a1bc"&gt;
&lt;p&gt;
Find out something about the topic that the data describes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2b3ad6a" class="outline-3"&gt;
&lt;h3 id="org2b3ad6a"&gt;Check if the Data is intuitive&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2b3ad6a"&gt;
&lt;p&gt;
See if there are any strange values and see if it makes sense given the data description.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgee6065f" class="outline-3"&gt;
&lt;h3 id="orgee6065f"&gt;Understand how the data was generated&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgee6065f"&gt;
&lt;p&gt;
Is any of it synthetic? Was the training data generated the same way as the testing data?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org52989b0" class="outline-2"&gt;
&lt;h2 id="org52989b0"&gt;Exploring Anonymized Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org52989b0"&gt;
&lt;p&gt;
Organizers sometimes encode data so you can't tell what it is.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4e95d36" class="outline-3"&gt;
&lt;h3 id="org4e95d36"&gt;Try To Decode the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4e95d36"&gt;
&lt;p&gt;
You generally won't be able to figure out what the data is if it's encoded, but sometimes they were created using simple shifting schemes that will let you figure out their original values.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org10716fa" class="outline-3"&gt;
&lt;h3 id="org10716fa"&gt;Explore Individual Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org10716fa"&gt;
&lt;p&gt;
Even if you don't know what the data is, it's important to know what type of data it is so that you can use the right data preprocessing for your model.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;df.dtypes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df.info()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x.value_counts()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x.isnull()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1890ea3" class="outline-2"&gt;
&lt;h2 id="org1890ea3"&gt;Visualizations&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1890ea3"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb8c9af1" class="outline-3"&gt;
&lt;h3 id="orgb8c9af1"&gt;Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb8c9af1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org83717d2" class="outline-4"&gt;
&lt;h4 id="org83717d2"&gt;Histograms&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org83717d2"&gt;
&lt;p&gt;
This is useful for seeing the shape of the data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.hist(x)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1a83eba" class="outline-4"&gt;
&lt;h4 id="org1a83eba"&gt;Index vs Value&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1a83eba"&gt;
&lt;p&gt;
This will show you how well distributed the data is within the data frame. Horizontal lines indicate repeated values and empty bands show areas where none of the data had this value. If you don't have vertical lines then the data was probably shuffled.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.plot(x, '.')&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1b77b3f" class="outline-4"&gt;
&lt;h4 id="org1b77b3f"&gt;Index vs Valuue Colorede By Class Label&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1b77b3f"&gt;
&lt;p&gt;
If you plot the classes with different colors you can see if there are clusters and clear separations between them. (&lt;code&gt;y&lt;/code&gt; in the function call has the target values).
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(range(len(x)), x, c=y)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge74baaa" class="outline-4"&gt;
&lt;h4 id="orge74baaa"&gt;Plot Other Things&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge74baaa"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Row vs Feature&lt;/li&gt;
&lt;li&gt;Nan-values&lt;/li&gt;
&lt;li&gt;Value Counts&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org86a14dd" class="outline-3"&gt;
&lt;h3 id="org86a14dd"&gt;Relationships&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org86a14dd"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga4bcc14" class="outline-4"&gt;
&lt;h4 id="orga4bcc14"&gt;Scatter Plots&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga4bcc14"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org6287d76"&gt;&lt;/a&gt;Pairwise Relationships&lt;br&gt;
&lt;div class="outline-text-5" id="text-org6287d76"&gt;
&lt;p&gt;
To make it eaiser to see relationships, you can plot them in pairs.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(x1, x2)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org74dff39"&gt;&lt;/a&gt;Compare the Test Set&lt;br&gt;
&lt;div class="outline-text-5" id="text-org74dff39"&gt;
&lt;p&gt;
Plot the features and the test-set (using different colors) to see how well your training data matches your test data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org9cdc141"&gt;&lt;/a&gt;Scatter Matrix&lt;br&gt;
&lt;div class="outline-text-5" id="text-org9cdc141"&gt;
&lt;p&gt;
Pandas has a convenience function that will plot all the pairwise scatter-plots for you.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pandas.scatter_matrix(X)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc5368c4" class="outline-4"&gt;
&lt;h4 id="orgc5368c4"&gt;Corellation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc5368c4"&gt;
&lt;p&gt;
Plot the corellation matrix to see if there are feature-pairs that are related so you can make a new feature out of them and see if they help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;X.corr(), pyplot.matshow()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
A straight correllation matrix might give you a sense of how correllated the features are, but it's more useful to use a clustering algorithm to see if there are groups within the correlation matrix.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org86aa3b0" class="outline-4"&gt;
&lt;h4 id="org86aa3b0"&gt;Plot Statistics&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org86aa3b0"&gt;
&lt;p&gt;
Try plotting mean, differences, combination counts, etc. and see if you can create groups out of them.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc480036" class="outline-2"&gt;
&lt;h2 id="orgc480036"&gt;Data Set Cleaning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc480036"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2c12ae8" class="outline-3"&gt;
&lt;h3 id="org2c12ae8"&gt;Duplicated and Constant Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2c12ae8"&gt;
&lt;p&gt;
Sometimes a feature will have the same value in all the rows. If it's this way in both the training and test sets you can just remove it, but if there are different values in the test set you have to figure out how to handle them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.nunique(axis&lt;/code&gt;"columns") &lt;code&gt;= 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Sometimes columns will get duplicated in which case you should drop one of them.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.T.drop_duplicates()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
This can happen with rows as well, but it can be harder to decide whether this is a mistake or not.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org146bdce" class="outline-3"&gt;
&lt;h3 id="org146bdce"&gt;Non-shuffled Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org146bdce"&gt;
&lt;p&gt;
If you plot the mean as a horizontal line the data should be evenly distributed around it, if not it might not have been shuffled and there could be an inadvertent pattern in the data. You might not be able to use it, but you should understand all the things about the data that you can find out.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga8dad9f" class="outline-2"&gt;
&lt;h2 id="orga8dad9f"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga8dad9f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb924588" class="outline-3"&gt;
&lt;h3 id="orgb924588"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb924588"&gt;
&lt;p&gt;
Suppose we are given a data set with features &lt;i&gt;X&lt;/i&gt;, &lt;i&gt;Y&lt;/i&gt;, and &lt;i&gt;Z&lt;/i&gt;. Can you recover &lt;i&gt;z&lt;/i&gt; as a function of &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt;?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Z = X/Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X - Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X + Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = XY&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2f3e94e" class="outline-3"&gt;
&lt;h3 id="org2f3e94e"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2f3e94e"&gt;
&lt;p&gt;
What value do the red dots have?
0.5 (wrong)
2 (next try)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org21dc155" class="outline-3"&gt;
&lt;h3 id="org21dc155"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org21dc155"&gt;
&lt;p&gt;
What hypothesis about X can we not reject based on the plots?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X is a counter or label encoded categorical feature&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can be the temperature (in Celsius) in different cities at different times. (the values are probably out of range)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can take a value of zero (The log plot would have values at 0 but it doesn't)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X takes only discrete values (the horizontal lines indicate that there are repeated values with discrete values)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; 2 &amp;lt;= X &amp;lt; 3 happens more frequently than 3 &amp;lt;= X &amp;lt; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e93faf" class="outline-3"&gt;
&lt;h3 id="org0e93faf"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0e93faf"&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Target is completely determined by coordinates (x,y)(x,y)(x,y), i.e. the label of the point is completely determined by point's position (x,y)(x,y)(x,y). Saying the same in other words: if we only had two features (x,y)(x,y)(x,y), we could build a classifier, that is accurate 100% of time.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The top right plot is better than the top left in that everything you get from the top left can also be gotten from the top right, but not the other way around.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; standard deviation for jittering is the largest on the bottom right.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7baac80" class="outline-2"&gt;
&lt;h2 id="org7baac80"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7baac80"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html"&gt;Sorting Correlation Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes data</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</guid><pubDate>Tue, 04 Sep 2018 04:24:53 GMT</pubDate></item><item><title>Feature Extraction From Text and Images</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org74c7872"&gt;How do you convert text to data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orga937f80"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orga832511"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org6ebf169"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org74c7872" class="outline-2"&gt;
&lt;h2 id="org74c7872"&gt;How do you convert text to data?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org74c7872"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org274b922" class="outline-3"&gt;
&lt;h3 id="org274b922"&gt;Two Main Methods&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org274b922"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org47dbf05" class="outline-4"&gt;
&lt;h4 id="org47dbf05"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org47dbf05"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgb8c8631"&gt;&lt;/a&gt;Vectorization&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgb8c8631"&gt;
&lt;p&gt;
This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;CountVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orga56eb05"&gt;&lt;/a&gt;Term Frequency/Inverse Document Frequency (TF/IDF)&lt;br&gt;
&lt;div class="outline-text-5" id="text-orga56eb05"&gt;
&lt;p&gt;
This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency: Normalize columns to make emphasize more important features&lt;/li&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org04f849d"&gt;&lt;/a&gt;N-Grams&lt;br&gt;
&lt;div class="outline-text-5" id="text-org04f849d"&gt;
&lt;p&gt;
This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://sklearn.feature_extraction.text.countvectorizer"&gt;Count Vectorizer&lt;/a&gt; is sklearn's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgef78e4c"&gt;&lt;/a&gt;Text Preprocessing&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgef78e4c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;lowercase
Change all the words to lower-case&lt;/li&gt;
&lt;li&gt;lemmatization
Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)&lt;/li&gt;
&lt;li&gt;stemming
Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)&lt;/li&gt;
&lt;li&gt;stopwords
Remove common words (e.g. a, and, or, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orge8c4797"&gt;&lt;/a&gt;The Bag Of Words Pipeline&lt;br&gt;
&lt;div class="outline-text-5" id="text-orge8c4797"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Preprocessing (lowercase, lemmatization, stemming, stopwords)&lt;/li&gt;
&lt;li&gt;Create n-grams&lt;/li&gt;
&lt;li&gt;Postprocessing: TF/IDF&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org83df017" class="outline-4"&gt;
&lt;h4 id="org83df017"&gt;Embeddings (e.g. Word2Vec)&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org83df017"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses neural-nets&lt;/li&gt;
&lt;li&gt;much smaller vectors than bag of words&lt;/li&gt;
&lt;li&gt;But each word gets a vector so many more vectors&lt;/li&gt;
&lt;li&gt;similar words have similar word-vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga937f80" class="outline-2"&gt;
&lt;h2 id="orga937f80"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga937f80"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1b7954e" class="outline-3"&gt;
&lt;h3 id="org1b7954e"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1b7954e"&gt;
&lt;p&gt;
TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. Choose the correct statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; IDF scales features inversely proprotionally to a number of word occurrences over documents&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; IDF scales features proportionally to the frequency of the a word's occurrences&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; TF normalizes sums of the row values to 1&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; TF normalizes sums of the column values to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9830be7" class="outline-3"&gt;
&lt;h3 id="org9830be7"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9830be7"&gt;
&lt;p&gt;
Which of these methods can be used to preprocess text?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; stemming&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lower-case transformation&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lemmatization&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Stopword removal&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumping&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumbing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7757a35" class="outline-3"&gt;
&lt;h3 id="org7757a35"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7757a35"&gt;
&lt;p&gt;
What is the main purpose of lemmatization and stemming?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to remove words which are not useful&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; to remove inflectional forms and sometimes derivationally related forms of a word to a common base form&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; To reduce the significance of common words&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to induce common word amplification standards to the most useful for machine learning algorithms form&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org916272d" class="outline-3"&gt;
&lt;h3 id="org916272d"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org916272d"&gt;
&lt;p&gt;
To learn Word2Vec embeddings we need:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; GloVe embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Labels for the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Labels for each word in the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga832511" class="outline-2"&gt;
&lt;h2 id="orga832511"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga832511"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org288ffdf" class="outline-3"&gt;
&lt;h3 id="org288ffdf"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org288ffdf"&gt;
&lt;p&gt;
Select true statements about n-grams.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining should always be applied before computing n-grams (there is no such thing as Levenshteining)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; n-grams always help increase significance of important words (n-grams are about counts, not importance)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams features are typically sparse (n-grams count occurrences of words and not every word will be found in every document)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams can help utilize local context around each word (n-grams encode sequences of words)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org34de6b2" class="outline-3"&gt;
&lt;h3 id="org34de6b2"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org34de6b2"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Bag of words usually produces longer vectors than Word2Vec (The number of features with BOW is equal to the number of unique words, Word2Vec limit is set beforehand)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Semantically similar words usually have similar word2vec embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; You do not need bag of words features in a competition if you have word2vec features (both approaches are useful and can work together)
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The meaning of each value in the Bag of Words matrix is unknown (The meaning of each value is how many times it occurred)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4c77f05" class="outline-3"&gt;
&lt;h3 id="org4c77f05"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4c77f05"&gt;
&lt;p&gt;
Suppose in a new competition we are given a dataset of 2D medical images. We want to extract image descriptors from a hidden layer of a neural network pretrained on the ImageNet dataset. We will then use extracted descriptors to train a simple logistic regression model to classify images from our dataset.
&lt;/p&gt;

&lt;p&gt;
We are considering using two networks: ResNet-50 with an ImageNet accuracy of X and VGG-16 with an ImageNet accuracy of Y (X &amp;lt; Y). Select the true statements.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; With one pretrained CNN model you can get only one vector of descriptors for an image&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet 50 will always be better than the ones from VG-16 in our pipeline&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; It is not clear what descriptors are better on our dataset. We should evaluate both.
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet-50 and VGG-16 are always very similar in cosine distance&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; For any image, descriptors from the last hidden layer of ResNet-50 are the same as the descriptors from the last hidden layer of VGG-16&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c22edd" class="outline-3"&gt;
&lt;h3 id="org8c22edd"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c22edd"&gt;
&lt;p&gt;
Data augmentation can be used at (1) train time and (2) test time
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; True, False&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, True&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True, True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6ebf169" class="outline-2"&gt;
&lt;h2 id="org6ebf169"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6ebf169"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c5af0f" class="outline-3"&gt;
&lt;h3 id="org8c5af0f"&gt;Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c5af0f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd94221f" class="outline-4"&gt;
&lt;h4 id="orgd94221f"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd94221f"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/feature_extraction.html"&gt;SKlearn on feature extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/"&gt;blog post&lt;/a&gt; on extracting features from text&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5740868" class="outline-4"&gt;
&lt;h4 id="org5740868"&gt;Word2Vec&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5740868"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rare-technologies.com/word2vec-tutorial/"&gt;Blog post tutorial&lt;/a&gt; by the author of gensim&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"&gt;Text Classification post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://taylorwhitten.github.io/blog/word2vec"&gt;Another introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb566b02" class="outline-4"&gt;
&lt;h4 id="orgb566b02"&gt;Natural Language Processing with Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb566b02"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/"&gt;nltk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga03e1f7" class="outline-3"&gt;
&lt;h3 id="orga03e1f7"&gt;Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga03e1f7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org301a6c6" class="outline-4"&gt;
&lt;h4 id="org301a6c6"&gt;Pre-trained Models&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org301a6c6"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://keras.io/applications/"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"&gt;How To use a pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge2b3820" class="outline-4"&gt;
&lt;h4 id="orge2b3820"&gt;Fine-Tuning&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge2b3820"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/image_retraining"&gt;Re-train a tensorflow image classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html"&gt;Fine-tuning deep learning models in keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>featureextraction text images notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</guid><pubDate>Mon, 13 Aug 2018 14:17:52 GMT</pubDate></item><item><title>Feature Preprocessing</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org02e9aab"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgd328052"&gt;Numeric Feature Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgb26fe51"&gt;Categorical And Ordinal Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org9e98319"&gt;Dates and Times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org6778d35"&gt;Coordinates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgc44ddc3"&gt;Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgd645cf5"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org02e9aab" class="outline-2"&gt;
&lt;h2 id="org02e9aab"&gt;Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org02e9aab"&gt;
&lt;p&gt;
In every competition you need to:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;pre-process the data&lt;/li&gt;
&lt;li&gt;generate new features from the existing ones&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd328052" class="outline-2"&gt;
&lt;h2 id="orgd328052"&gt;Numeric Feature Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd328052"&gt;
&lt;p&gt;
Some models (e.g. Linear Classification) need you to convert numeric-looking data to categorical data. Tree-based models don't need pre-processing as much as non tree-based models do.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org53b35fd" class="outline-3"&gt;
&lt;h3 id="org53b35fd"&gt;Scaling&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org53b35fd"&gt;
&lt;p&gt;
If features have different value ranges then the model will  treat them differently.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"&gt;sklearn.preprocessing.MinMaxScalar&lt;/a&gt;
Scale by the range of values&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"&gt;sklearn.preprocessing.StandardScalar&lt;/a&gt;
Scale the data to have a mean of 0 and a standard deviation of 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5a81a46" class="outline-3"&gt;
&lt;h3 id="org5a81a46"&gt;Outliers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5a81a46"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Clip (&lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html"&gt;numpy.clip&lt;/a&gt;)values to get rid of unusual values that mess with the model. (Winsorization)&lt;/li&gt;
&lt;li&gt;Rank Transform them (&lt;a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html"&gt;scipy.stats.rankdata&lt;/a&gt;) so that they&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9a96563" class="outline-3"&gt;
&lt;h3 id="org9a96563"&gt;Transformation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9a96563"&gt;
&lt;p&gt;
For linear models and neural networks, transforming the data can sometimes help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Log Transform: &lt;a href="https://duckduckgo.com/?q=numpy+log&amp;amp;t=canonical&amp;amp;ia=web"&gt;numpy.log(1 + x)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Power Less Than 1 Transform: &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html"&gt;numpy.sqrt(x + 2/3)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org52607ae" class="outline-3"&gt;
&lt;h3 id="org52607ae"&gt;Feature Generation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org52607ae"&gt;
&lt;p&gt;
Sometimes you can convert separate columns to get new ones. This requires exploratory data analysis.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Fractional Parts: people perceive numbers with fractions differently so sometime separating out the fractional part makes the model perform better because it is the more important part&lt;/li&gt;
&lt;li&gt;You can derive new values mathematically (e.g. distance using height and width)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb26fe51" class="outline-2"&gt;
&lt;h2 id="orgb26fe51"&gt;Categorical And Ordinal Features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb26fe51"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Ordinal Features have an ordering but even if they are labeled numerically (e.g. 1, 2, 3) they aren't numeric because there is no implication about the distance between each&lt;/li&gt;
&lt;li&gt;label and frequency encoding are commonly used for tree-based models&lt;/li&gt;
&lt;li&gt;one-hot encoding is more common for non-tree-based models&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc322d7b" class="outline-3"&gt;
&lt;h3 id="orgc322d7b"&gt;Label Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc322d7b"&gt;
&lt;p&gt;
Some models need numeric values or numeric values that don't have implied ordering so you want to encode them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"&gt;sklearn.preprocessing.LabelEncoder&lt;/a&gt;
Sorts the labels then encodes them as integers&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html"&gt;Pandas.factorize&lt;/a&gt;
Encodes the labels in the order they appear. This makes more sense if there is a meaning to the order in which labels appear.&lt;/li&gt;
&lt;li&gt;This is more commonly used with tree-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org53bb997" class="outline-3"&gt;
&lt;h3 id="org53bb997"&gt;Frequency Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org53bb997"&gt;
&lt;p&gt;
Encode the values to a fraction of all the labels. Can work with linear models if the frequency is correlated with the target value.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;titanic.groupby("Embarked").size()/len(titanic)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from scipy.stats import rankedata&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;More common with tree-based methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8aed5ab" class="outline-3"&gt;
&lt;h3 id="org8aed5ab"&gt;One Hot Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8aed5ab"&gt;
&lt;p&gt;
Creates one column for each label and puts a 1 in the column that matches. Works with both linear and tree-based models, but can be inefficient with tree-based models.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"&gt;pandas.get_dummies&lt;/a&gt; - converts strings into column encodings&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"&gt;sklearn.preprocessing.OneHotEncoder&lt;/a&gt; - converts numeric categorical data into column encodings&lt;/li&gt;
&lt;li&gt;One hot encoding is better for non-tree models.&lt;/li&gt;
&lt;li&gt;This allows easier feature interactions (encode combined features (e.g. gender and class) rather than encoding them separately) 
&lt;ul class="org-ul"&gt;
&lt;li&gt;This is more common with linear models and KNN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9e98319" class="outline-2"&gt;
&lt;h2 id="org9e98319"&gt;Dates and Times&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9e98319"&gt;
&lt;p&gt;
When working with seasonal data, sometimes the relative date-stamps are more important than the actual dates (how close to Christmas?).
Sometimes you want the time between events.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6778d35" class="outline-2"&gt;
&lt;h2 id="org6778d35"&gt;Coordinates&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6778d35"&gt;
&lt;p&gt;
Sometimes you want exact coordinates, but most times you want distances to some center.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc44ddc3" class="outline-2"&gt;
&lt;h2 id="orgc44ddc3"&gt;Missing Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc44ddc3"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;"missing" data might mean outliers - values that are probably wrong&lt;/li&gt;
&lt;li&gt;avoid replacing missing values before feature engineering - it can throw off what you do&lt;/li&gt;
&lt;li&gt;Gradient Boost Trees can handle isNaN, so you don't have to do anything&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org064dc8f" class="outline-3"&gt;
&lt;h3 id="org064dc8f"&gt;Numeric&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org064dc8f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1473416" class="outline-4"&gt;
&lt;h4 id="org1473416"&gt;Fill NA Approaches&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1473416"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;-999, -1, other numbers
&lt;ul class="org-ul"&gt;
&lt;li&gt;lets you categorize missing values&lt;/li&gt;
&lt;li&gt;throws some models off (e.g. linear models and neural networks)&lt;/li&gt;
&lt;li&gt;one solution is to create a new feature for missing values, but this has now increased the amount of data you need (curse of dimensionality)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;mean, median, some central tendency
&lt;ul class="org-ul"&gt;
&lt;li&gt;This can throw the model off&lt;/li&gt;
&lt;li&gt;it is sometimes better to ignore missing data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;recronstructed valud&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd645cf5" class="outline-2"&gt;
&lt;h2 id="orgd645cf5"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd645cf5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e28f21" class="outline-3"&gt;
&lt;h3 id="org0e28f21"&gt;Feature Pre-processing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0e28f21"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html"&gt;SKlearn's Preprocessing Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling"&gt;Andrew Ng on Feature Scaling and its effect on Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"&gt;Sebastian Raschka on Feature Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3b24ea9" class="outline-3"&gt;
&lt;h3 id="org3b24ea9"&gt;Feature Engineering&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3b24ea9"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"&gt;Machine Learning Mastery on Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering"&gt;Quora: What are some best practices in Feature Engineering?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>features preprocessing notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</guid><pubDate>Wed, 08 Aug 2018 04:41:10 GMT</pubDate></item><item><title>Machine Learning Recap</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org0a35d43" class="outline-2"&gt;
&lt;h2 id="org0a35d43"&gt;The Main Categories&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0a35d43"&gt;
&lt;p&gt;
These are the four main categories of supervised machine learning algorithms that you'll encounter in kaggle competitions.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Linear Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;vowpal rabbit (for really large datasets)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tree-Based Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;xgboost: faster than sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;k-Nearest Neighbors
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge4ba9cf" class="outline-2"&gt;
&lt;h2 id="orge4ba9cf"&gt;The No Free Lunch Theorem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge4ba9cf"&gt;
&lt;blockquote&gt;
&lt;p&gt;
There is no method which outperforms all others for all tasks.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
You cannot assume that an algorithm that did well on one set of data will do well on another. All algorithms have weaknesses, so you have to test multiple algorithms on each data set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf9bb943" class="outline-2"&gt;
&lt;h2 id="orgf9bb943"&gt;Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf9bb943"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;there is no one algorithm to rule them all&lt;/li&gt;
&lt;li&gt;Linear models split spaces into two sub-spaces&lt;/li&gt;
&lt;li&gt;tree-based models spit spaces into boxes&lt;/li&gt;
&lt;li&gt;kNN relies on measuring the 'closeness' between points&lt;/li&gt;
&lt;li&gt;Neural Networks provide non-linear decision boundaries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In general the two most powerful methods are &lt;b&gt;Gradient Boosted Decision Trees&lt;/b&gt; and &lt;b&gt;Neural Networks&lt;/b&gt;, but this won't always be the case.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics algorithms notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</guid><pubDate>Sun, 05 Aug 2018 01:13:44 GMT</pubDate></item><item><title>Real-World vs Kaggle</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/real-world-vs-kaggle/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org4e36b1f" class="outline-2"&gt;
&lt;h2 id="org4e36b1f"&gt;A Real World Pipeline&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4e36b1f"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;What is the business problem that you are trying to solve?&lt;/li&gt;
&lt;li&gt;What is the formal version of the problem?&lt;/li&gt;
&lt;li&gt;How do you collect data?&lt;/li&gt;
&lt;li&gt;How do you preprocess the data?&lt;/li&gt;
&lt;li&gt;How do you create the model?
&lt;ul class="org-ul"&gt;
&lt;li&gt;what is the appropriate algorithm?&lt;/li&gt;
&lt;li&gt;what is the correct metric?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;How do you evaluate the model in a real world situation?&lt;/li&gt;
&lt;li&gt;How do you deploy the model?
&lt;ul class="org-ul"&gt;
&lt;li&gt;How do you monitor its performance?&lt;/li&gt;
&lt;li&gt;How do you update it over time?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org400b906" class="outline-2"&gt;
&lt;h2 id="org400b906"&gt;A Competition Pipeline&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org400b906"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;How do you pre-process the data?&lt;/li&gt;
&lt;li&gt;How do you create the model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7c650ca" class="outline-2"&gt;
&lt;h2 id="org7c650ca"&gt;So, how do you use a competition then?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7c650ca"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;It's good to learn about machine learning&lt;/li&gt;
&lt;li&gt;It' not just about the algorithms, let the data guide what you do&lt;/li&gt;
&lt;li&gt;Try to be creative and do things that haven't been done before&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics kaggle</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/real-world-vs-kaggle/</guid><pubDate>Sun, 05 Aug 2018 01:02:24 GMT</pubDate></item><item><title>Kaggle Mechanics</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/kaggle-mechanics/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orga765a0c" class="outline-2"&gt;
&lt;h2 id="orga765a0c"&gt;The Basics&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga765a0c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4f59197" class="outline-3"&gt;
&lt;h3 id="org4f59197"&gt;Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4f59197"&gt;
&lt;p&gt;
Every competition provides data so you can create a model, but there isn't a standardized format. Sometimes it will be CSVz, sometimes excel spreadsheets, sometimes image files, the sources can vary so you should read the data descriptions and adapt what you do to the competition. You aren't always limited to the data that is provided. If you were creating an image recognition model, for instance, it might be okay to include outside images or pre-trained models, it depends on the particulars of the competition.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org67b1fb6" class="outline-3"&gt;
&lt;h3 id="org67b1fb6"&gt;Models&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org67b1fb6"&gt;
&lt;p&gt;
This is what you are trying to create - a representation of the population based on the data you are given that will allow you to predict outcomes based on inputs not given in the data. The model needs two main features:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;reproducibility&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Note that a model is not the same as an algorithm. You might have to combine multiple algorithms in order to build your model. The key to a model is that it maps inputs to outputs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb8b9a2" class="outline-3"&gt;
&lt;h3 id="orgdb8b9a2"&gt;Submission&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb8b9a2"&gt;
&lt;p&gt;
Your submission is typically your predictions on a test set. This isn't always the case but it is the most common way that the competitions are run.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org86a2d7a" class="outline-3"&gt;
&lt;h3 id="org86a2d7a"&gt;Evaluation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org86a2d7a"&gt;
&lt;p&gt;
How can you tell how well your model does? You need a function that maps your model and a data set to a score that evaluates how well the model does. There are many different metrics to use (accuracy, precision, recall, etc.) but the competition will choose one and tell you what it is in the description so make sure you read the description to get it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge011278" class="outline-3"&gt;
&lt;h3 id="orge011278"&gt;Leaderboard&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge011278"&gt;
&lt;p&gt;
This is the relative ranking of the participants in the competition. This is what makes it a competition. Even if your metric tells you your model is doing well, if you are ranked at the bottom, you still won't win. There are actually two leaderboards - public and private. The evaluation dataset is split by kaggle into two sets, public and private, and during the competition the results of testing on the public data are shown on the leaderborad. Once the competition is over the leaderboard is displayed using the evaluations using the private data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org05a15ef" class="outline-2"&gt;
&lt;h2 id="org05a15ef"&gt;Other Competitions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org05a15ef"&gt;
&lt;p&gt;
&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; isn't the only one running data-science competititons. Here are some others.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.drivendata.org/"&gt;Driven Data&lt;/a&gt;: Data Science competititons aimed at social problems&lt;/li&gt;
&lt;li&gt;&lt;a href="http://codalab.org/"&gt;Coda Lab&lt;/a&gt;: Competitions using research datasets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datasciencechallenge.org/"&gt;Data Science Challenge&lt;/a&gt;: Using data-science to solve government-scale problems.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datascience.net/fr/challenge#"&gt;Data Science dot net&lt;/a&gt;: A european data-science competition site.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics rules</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/kaggle-mechanics/</guid><pubDate>Sun, 05 Aug 2018 00:18:12 GMT</pubDate></item></channel></rss>