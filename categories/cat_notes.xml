<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/cat_notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 15 Sep 2018 00:10:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Data Leaks</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leaks/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgf47ccd1" class="outline-2"&gt;
&lt;h2 id="orgf47ccd1"&gt;What are data leaks?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf47ccd1"&gt;
&lt;p&gt;
&lt;i&gt;Data Leaks&lt;/i&gt; are unexpected errors that expose extra information that wouldn't be available in production.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org235e77b" class="outline-2"&gt;
&lt;h2 id="org235e77b"&gt;Time Series&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org235e77b"&gt;
&lt;p&gt;
Check if there are points in the training set that are in the future with regard to the test set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org095beaa" class="outline-2"&gt;
&lt;h2 id="org095beaa"&gt;Unexpected Information&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org095beaa"&gt;
&lt;p&gt;
Sometimes what looks like a non-predictive feature might prove to be useful because of the way the data-set was created.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;ID&lt;/li&gt;
&lt;li&gt;Row Order&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfd58a2d" class="outline-2"&gt;
&lt;h2 id="orgfd58a2d"&gt;Leaderboard Probing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfd58a2d"&gt;
&lt;p&gt;
This is a method to look for dataleaks based on the leader board.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7235bd8" class="outline-2"&gt;
&lt;h2 id="org7235bd8"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7235bd8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcb0b935" class="outline-3"&gt;
&lt;h3 id="orgcb0b935"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcb0b935"&gt;
&lt;p&gt;
Suppose that you have a credit scoring task where you have to create a machine learning model that approximates expert evaluation of an individual's creditworthiness. Which of the following can potentially be a data leakage?
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; An ID of a data point (row) in the train set corellates with the target variable (the data wasn't shuffled, this information won't work in a real-world scenario)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The first half of the data points in the train set have a score of 0 while the second half has scores &amp;gt; 0. (same as above)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Among he features you have a &lt;code&gt;company_id&lt;/code&gt;, an identifier of a company where this person works. It turns out that this feature is important and adding it to your model improves your score. (this is a normal feature, the fact that it improves your score just means it's an important feature)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6d1e538" class="outline-3"&gt;
&lt;h3 id="org6d1e538"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6d1e538"&gt;
&lt;p&gt;
What is the most foolproof way to set up a time-series competition?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Split, train, public and private parts of the data by time. Remove all features except IDs from the test set so that participants will generate all the features based on the past and join them themselves. (you need to remove all features tfrom the test set to guarantee there isn't a data-leakage)&lt;/li&gt;

&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Make a time based split for train/test and a random split for publit/private. (this is vulnerable to leaderboard probing)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; split public and private by time, remove time from the test set. (it is possible to reverse engineer the time-ordering and exploit future-peeking)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5440e65" class="outline-3"&gt;
&lt;h3 id="org5440e65"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5440e65"&gt;
&lt;p&gt;
Suppose you have a binary feature classification task and it is evaluated by the logloss metric. You know that there are 10,000 entries in the public chunk of the test set and that a constant prediction of 0.3 gets a score of 1.01. The mean of the target variable in the training set is 0.44. What is the mean of the target variable in the public part  of the test data (up to 4 decimal places.)
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;0.44 (wrong)&lt;/li&gt;
&lt;li&gt;0.132 (wrong)&lt;/li&gt;
&lt;li&gt;1.33 (wrong)&lt;/li&gt;
&lt;li&gt;0.7712&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
\[
\frac{N_1}{N} = \frac{-L - \ln (1-C)}{\ln C - \ln (1 - C)}\\
= 0.7712\\
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7a1136d" class="outline-3"&gt;
&lt;h3 id="org7a1136d"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7a1136d"&gt;
&lt;p&gt;
Suppose you are solving an image classification task, what is the classification of the logo for this course?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Angry robot (wrong - should be a number)&lt;/li&gt;
&lt;li&gt;1 (wrong - check image name)&lt;/li&gt;
&lt;li&gt;3 (the URL for the image had the answer in it)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>dataleaks notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leaks/</guid><pubDate>Sat, 08 Sep 2018 23:11:07 GMT</pubDate></item><item><title>Validation</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org5d1619b"&gt;Validation and Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org61ce815"&gt;Three Main Methods of Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org7d5f2a4"&gt;Stratification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org5e1cb52"&gt;Data Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org7deb52b"&gt;Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#orgd4c219d"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org390c014"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org1c800a9"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5d1619b" class="outline-2"&gt;
&lt;h2 id="org5d1619b"&gt;Validation and Overfitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5d1619b"&gt;
&lt;p&gt;
To prevent your model from overfitting to the training set, you can hold out some of the training set and use it to validate the model after it has been fit to the rest of the training set.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Underfitting: your model isn't complex enough to capture the data&lt;/li&gt;
&lt;li&gt;Overfitting: your model is too complex and it is modelling noise in the data&lt;/li&gt;
&lt;li&gt;In a competition, if your model does well on the validation set but not on the test set, then it probably overfit the data you had&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org61ce815" class="outline-2"&gt;
&lt;h2 id="org61ce815"&gt;Three Main Methods of Splitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org61ce815"&gt;
&lt;p&gt;
These are methods for splitting your training set into training and validation sets. Once you have a model, re-train it over the entire training set before applying it to the test set.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4e15e80" class="outline-3"&gt;
&lt;h3 id="org4e15e80"&gt;Holdout&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4e15e80"&gt;
&lt;p&gt;
This method just splits the data into one training and one validation set.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html"&gt;sklearn.model_selection.ShuffleSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ngroups=1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge1d1b4f" class="outline-3"&gt;
&lt;h3 id="orge1d1b4f"&gt;K-Fold&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge1d1b4f"&gt;
&lt;p&gt;
Make &lt;i&gt;k&lt;/i&gt; splits of the training set, then use each of the validation sets while training on all the data not in the validation set. This differs from doing holdout k-times since we guarantee that the validation sets don't overlap. Uses the average score for the k-folds.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"&gt;sklearn.model_selection.KFold&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0b8340b" class="outline-3"&gt;
&lt;h3 id="org0b8340b"&gt;Leave One Out&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0b8340b"&gt;
&lt;p&gt;
This is like k-folds except we always use a validation set of size 1 - so we are iterating over each point in the data set and using it as the training set. This can be useful if the data set is small.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html"&gt;sklearn.model_selection.LeaveOneOut&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7d5f2a4" class="outline-2"&gt;
&lt;h2 id="org7d5f2a4"&gt;Stratification&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7d5f2a4"&gt;
&lt;p&gt;
Sometimes you need to make sure your validation sets have the same distribution as your set as a whole.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;small datasets&lt;/li&gt;
&lt;li&gt;unbalanced datasects&lt;/li&gt;
&lt;li&gt;multiclass classification&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"&gt;StratifieShuffleSplit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5e1cb52" class="outline-2"&gt;
&lt;h2 id="org5e1cb52"&gt;Data Splitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5e1cb52"&gt;
&lt;p&gt;
If you have time-based data, there's two ways to split the training data - randomly within the entire timespan, or put the first part of the data in the training set and put the second part of the data in the validation set. If the test-set is a time that is beyond the training data, then using the time-based split will produce a model that is better for the testing data.
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Row-wise split
This is the most common case, where rows are randomly chosen from the training data. This assumes the rows are independent.&lt;/li&gt;
&lt;li&gt;Time-wise split
This is the case where you are predicting future values of a time-series. In this case, the further back in time a row is, the less like the future value it is.&lt;/li&gt;
&lt;li&gt;By-ID
In this case several rows map to one ID, and the ID maps to a target. For example, you might have several x-rays for one patient that map to one diagnosis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
The main point of this is that you want to set up your validation set to match the way the train-test sets were split.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7deb52b" class="outline-2"&gt;
&lt;h2 id="org7deb52b"&gt;Problems&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7deb52b"&gt;
&lt;p&gt;
The point of doing the training-validation split is that you think the validation set(s) will approximate the test set. But what if that's not true?
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf30b6ce" class="outline-3"&gt;
&lt;h3 id="orgf30b6ce"&gt;Causes of score differences&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf30b6ce"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Too little data&lt;/li&gt;
&lt;li&gt;The data is too diverse and inconsistent&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd8025cd" class="outline-3"&gt;
&lt;h3 id="orgd8025cd"&gt;Submission Differs from Validation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd8025cd"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Even K-fold validation has variation (check that the standard deviation across folds encompasses the leaderboard values)&lt;/li&gt;
&lt;li&gt;Too little data on leaderboard (nothing you can do)&lt;/li&gt;
&lt;li&gt;Train and test are from different distributions&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd4c219d" class="outline-2"&gt;
&lt;h2 id="orgd4c219d"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd4c219d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbc574d5" class="outline-3"&gt;
&lt;h3 id="orgbc574d5"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbc574d5"&gt;
&lt;p&gt;
We did a K-Fold cross validation on a huge dataset and noticed that scores on each fold are roughly the same. Which validation type is of most practical use?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; K-Fold&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Holdout&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Leave one out&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2fe2ec7" class="outline-3"&gt;
&lt;h3 id="org2fe2ec7"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2fe2ec7"&gt;
&lt;p&gt;
We did a K-fold cross validation on a medium sized dataset and noticed that the validation scores varied widely. Which validation type is the most practical to use?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Leave One Out&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; K Fold&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Houldout&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b2c705" class="outline-3"&gt;
&lt;h3 id="org5b2c705"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5b2c705"&gt;
&lt;p&gt;
The features we generate depend on the train-test split. True or False?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb5fbbf" class="outline-3"&gt;
&lt;h3 id="orgdb5fbbf"&gt;Which of these can indicate an expected leaderboard shuffle in a competition?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb5fbbf"&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Little training and/or testing data&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Most of the competitors have similar scores&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Different public/private data or target distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org390c014" class="outline-2"&gt;
&lt;h2 id="org390c014"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org390c014"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org869ffd4" class="outline-3"&gt;
&lt;h3 id="org869ffd4"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org869ffd4"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; A performance increase on a fixed cross-validation split guarantees a performance increase on any cross-validation split. (You might be overfitting. You should change the splits to check for overfitting.)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The logic behind the validation split should mimic the logic behind the train-test split (this is the main rule for making a reliable validation)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Underfitting refers to not capturing enough patterns in the data&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; We use validation to estimate the quality of our model (this is the main purpose of validation)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The model that does on the validation set is guaranteed to do the best on the test set. (The test and validation sets might have different distributions, in which case the validation won't predict the test set score)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf2d2424" class="outline-3"&gt;
&lt;h3 id="orgf2d2424"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf2d2424"&gt;
&lt;p&gt;
Kaggle usually allows you to submit two final submissions that will be checked against the private leader board. One common practice is to use a model that did the best on the validation scores and another that did best on the public leader board. What is the logic behind using these two models?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; People rarely overfit the public leaderboard. You almost always have a lot of test data and it is hard to overfit.&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Validation is rarely valid in competitions. You must account for the case where validation worked and where it didn't.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The test set may have a different distribution than the target data. If this is true, then the model that did better on the public leaderboard will do better. If not, then the model that did better in validation will do better.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8f893ae" class="outline-3"&gt;
&lt;h3 id="org8f893ae"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8f893ae"&gt;
&lt;p&gt;
Suppose we have a dataset of marketing campaigns. Each campain runs for a few weeks and for each campaign our target is the number of new customers. A row in the dataset looks like this:
&lt;/p&gt;

&lt;p&gt;
&lt;i&gt;Campaign ID, Date, {some features},Number of New Customers&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
The dataset contains multiple campaigns where the training set has the dates at the start of each campaign and the test set has the dates at the end of each campaign. Which train/test split should you use?
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Random Split&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Combined Split (Each train and test set are divided by a date and the date might be for different campaigns, so it is a combination of campaign ID and date)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; ID-based split (wrong)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Time-based split (wrong)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8f8be71" class="outline-3"&gt;
&lt;h3 id="org8f8be71"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8f8be71"&gt;
&lt;p&gt;
Which of the following can you usually identify without the leaderboard?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Different scores/optimal parameters between different folds (this is determined during validation)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Train and test target data are from different distributions (You would need the test target values to figure this out, which you won't have)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The public leaderboard score will be unreliable because there is too little data (you can check this by making the size of the folds match the size of the public test set and see the variability)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The train and test data are from different distributions (you can often figure this out during Exploratory Data Analysis)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1c800a9" class="outline-2"&gt;
&lt;h2 id="org1c800a9"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1c800a9"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/cross_validation.html"&gt;Cross-validation in sklearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/"&gt;Model Selection for Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes validation</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</guid><pubDate>Tue, 04 Sep 2018 15:01:59 GMT</pubDate></item><item><title>Springleaf Competition</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org20ff5e4" class="outline-2"&gt;
&lt;h2 id="org20ff5e4"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org20ff5e4"&gt;
&lt;p&gt;
The data comes from the &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt; competition. Springleaf makes personal loans and wanted to be able to predict who would respond to their direct mail offers. Submissions are evaluated on the area under the ROC curve between the predicted probability and the target. The submission file should have an ID and the probability that the person would respond.
&lt;/p&gt;

&lt;pre class="example"&gt;
ID,target
1,0.35
3,0.01
6,0.93333
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>example competition notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</guid><pubDate>Tue, 04 Sep 2018 13:35:25 GMT</pubDate></item><item><title>Exploratory Data Analysis</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org12dd304"&gt;Building Your Intuition About the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org4fa52b8"&gt;Exploring Anonymized Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org770eca2"&gt;Visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#orgbae5e91"&gt;Data Set Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org14389ec"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org85eee71"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org12dd304" class="outline-2"&gt;
&lt;h2 id="org12dd304"&gt;Building Your Intuition About the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org12dd304"&gt;
&lt;p&gt;
The first thing to do is to see if you can build up a mental model of what the data is about.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfb94d79" class="outline-3"&gt;
&lt;h3 id="orgfb94d79"&gt;Get Domain Knowledge&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfb94d79"&gt;
&lt;p&gt;
Find out something about the topic that the data describes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4bdf112" class="outline-3"&gt;
&lt;h3 id="org4bdf112"&gt;Check if the Data is intuitive&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4bdf112"&gt;
&lt;p&gt;
See if there are any strange values and see if it makes sense given the data description.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbccbc42" class="outline-3"&gt;
&lt;h3 id="orgbccbc42"&gt;Understand how the data was generated&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbccbc42"&gt;
&lt;p&gt;
Is any of it synthetic? Was the training data generated the same way as the testing data?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4fa52b8" class="outline-2"&gt;
&lt;h2 id="org4fa52b8"&gt;Exploring Anonymized Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4fa52b8"&gt;
&lt;p&gt;
Organizers sometimes encode data so you can't tell what it is.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org589a093" class="outline-3"&gt;
&lt;h3 id="org589a093"&gt;Try To Decode the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org589a093"&gt;
&lt;p&gt;
You generally won't be able to figure out what the data is if it's encoded, but sometimes they were created using simple shifting schemes that will let you figure out their original values.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3c6ff0c" class="outline-3"&gt;
&lt;h3 id="org3c6ff0c"&gt;Explore Individual Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3c6ff0c"&gt;
&lt;p&gt;
Even if you don't know what the data is, it's important to know what type of data it is so that you can use the right data preprocessing for your model.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dtypes.html"&gt;df.dtypes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html"&gt;df.info()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"&gt;x.value_counts()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html"&gt;x.isnull()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org770eca2" class="outline-2"&gt;
&lt;h2 id="org770eca2"&gt;Visualizations&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org770eca2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0de1b4d" class="outline-3"&gt;
&lt;h3 id="org0de1b4d"&gt;Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0de1b4d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfd629df" class="outline-4"&gt;
&lt;h4 id="orgfd629df"&gt;Histograms&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfd629df"&gt;
&lt;p&gt;
This is useful for seeing the shape of the data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.hist(x)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge250957" class="outline-4"&gt;
&lt;h4 id="orge250957"&gt;Index vs Value&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge250957"&gt;
&lt;p&gt;
This will show you how well distributed the data is within the data frame. Horizontal lines indicate repeated values and empty bands show areas where none of the data had this value. If you don't have vertical lines then the data was probably shuffled.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.plot(x, '.')&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9ffe49a" class="outline-4"&gt;
&lt;h4 id="org9ffe49a"&gt;Index vs Value Colored By Class Label&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9ffe49a"&gt;
&lt;p&gt;
If you plot the classes with different colors you can see if there are clusters and clear separations between them. (&lt;code&gt;y&lt;/code&gt; in the function call has the target values).
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(range(len(x)), x, c=y)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge5900c3" class="outline-4"&gt;
&lt;h4 id="orge5900c3"&gt;Plot Other Things&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge5900c3"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Row vs Feature&lt;/li&gt;
&lt;li&gt;Nan-values&lt;/li&gt;
&lt;li&gt;Value Counts&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6018a12" class="outline-3"&gt;
&lt;h3 id="org6018a12"&gt;Relationships&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6018a12"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org811d4e0" class="outline-4"&gt;
&lt;h4 id="org811d4e0"&gt;Scatter Plots&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org811d4e0"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org3ac84f5"&gt;&lt;/a&gt;Pairwise Relationships&lt;br&gt;
&lt;div class="outline-text-5" id="text-org3ac84f5"&gt;
&lt;p&gt;
To make it eaiser to see relationships, you can plot them in pairs.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(x1, x2)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org44acdb0"&gt;&lt;/a&gt;Compare the Test Set&lt;br&gt;
&lt;div class="outline-text-5" id="text-org44acdb0"&gt;
&lt;p&gt;
Plot the features and the test-set (using different colors) to see how well your training data matches your test data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org6d46ea9"&gt;&lt;/a&gt;Scatter Matrix&lt;br&gt;
&lt;div class="outline-text-5" id="text-org6d46ea9"&gt;
&lt;p&gt;
Pandas has a convenience function that will plot all the pairwise scatter-plots for you.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pandas.scatter_matrix(X)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9654561" class="outline-4"&gt;
&lt;h4 id="org9654561"&gt;Corellation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9654561"&gt;
&lt;p&gt;
Plot the corellation matrix to see if there are feature-pairs that are related so you can make a new feature out of them and see if they help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;X.corr(), pyplot.matshow()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
A straight correllation matrix might give you a sense of how correllated the features are, but it's more useful to use a clustering algorithm to see if there are groups within the correlation matrix.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org66b9214" class="outline-4"&gt;
&lt;h4 id="org66b9214"&gt;Plot Statistics&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org66b9214"&gt;
&lt;p&gt;
Try plotting mean, differences, combination counts, etc. and see if you can create groups out of them.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbae5e91" class="outline-2"&gt;
&lt;h2 id="orgbae5e91"&gt;Data Set Cleaning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbae5e91"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgaf5b21b" class="outline-3"&gt;
&lt;h3 id="orgaf5b21b"&gt;Duplicated and Constant Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgaf5b21b"&gt;
&lt;p&gt;
Sometimes a feature will have the same value in all the rows. If it's this way in both the training and test sets you can just remove it, but if there are different values in the test set you have to figure out how to handle them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.nunique(axis&lt;/code&gt;"columns") &lt;code&gt;= 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Sometimes columns will get duplicated in which case you should drop one of them.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.T.drop_duplicates()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
This can happen with rows as well, but it can be harder to decide whether this is a mistake or not.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orged34d07" class="outline-3"&gt;
&lt;h3 id="orged34d07"&gt;Non-shuffled Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orged34d07"&gt;
&lt;p&gt;
If you plot the mean as a horizontal line the data should be evenly distributed around it, if not it might not have been shuffled and there could be an inadvertent pattern in the data. You might not be able to use it, but you should understand all the things about the data that you can find out.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org14389ec" class="outline-2"&gt;
&lt;h2 id="org14389ec"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org14389ec"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org619e5d1" class="outline-3"&gt;
&lt;h3 id="org619e5d1"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org619e5d1"&gt;
&lt;p&gt;
Suppose we are given a data set with features &lt;i&gt;X&lt;/i&gt;, &lt;i&gt;Y&lt;/i&gt;, and &lt;i&gt;Z&lt;/i&gt;. Can you recover &lt;i&gt;z&lt;/i&gt; as a function of &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt;?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Z = X/Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X - Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X + Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = XY&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2378fd0" class="outline-3"&gt;
&lt;h3 id="org2378fd0"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2378fd0"&gt;
&lt;p&gt;
What value do the red dots have?
0.5 (wrong)
2 (next try)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org23bcb3d" class="outline-3"&gt;
&lt;h3 id="org23bcb3d"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org23bcb3d"&gt;
&lt;p&gt;
What hypothesis about X can we not reject based on the plots?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X is a counter or label encoded categorical feature&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can be the temperature (in Celsius) in different cities at different times. (the values are probably out of range)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can take a value of zero (The log plot would have values at 0 but it doesn't)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X takes only discrete values (the horizontal lines indicate that there are repeated values with discrete values)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; 2 &amp;lt;= X &amp;lt; 3 happens more frequently than 3 &amp;lt;= X &amp;lt; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org72bedcb" class="outline-3"&gt;
&lt;h3 id="org72bedcb"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org72bedcb"&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Target is completely determined by coordinates (x,y)(x,y)(x,y), i.e. the label of the point is completely determined by point's position (x,y)(x,y)(x,y). Saying the same in other words: if we only had two features (x,y)(x,y)(x,y), we could build a classifier, that is accurate 100% of time.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The top right plot is better than the top left in that everything you get from the top left can also be gotten from the top right, but not the other way around.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; standard deviation for jittering is the largest on the bottom right.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org85eee71" class="outline-2"&gt;
&lt;h2 id="org85eee71"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org85eee71"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html"&gt;Sorting Correlation Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes data</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</guid><pubDate>Tue, 04 Sep 2018 04:24:53 GMT</pubDate></item><item><title>Feature Extraction From Text and Images</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgec870c9"&gt;How do you convert text to data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgfcdbc57"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgba8249f"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgc61632a"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgec870c9" class="outline-2"&gt;
&lt;h2 id="orgec870c9"&gt;How do you convert text to data?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgec870c9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org61b8bdb" class="outline-3"&gt;
&lt;h3 id="org61b8bdb"&gt;Two Main Methods&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org61b8bdb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org24b7dde" class="outline-4"&gt;
&lt;h4 id="org24b7dde"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org24b7dde"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org7ffa06a"&gt;&lt;/a&gt;Vectorization&lt;br&gt;
&lt;div class="outline-text-5" id="text-org7ffa06a"&gt;
&lt;p&gt;
This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;CountVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgca8778d"&gt;&lt;/a&gt;Term Frequency/Inverse Document Frequency (TF/IDF)&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgca8778d"&gt;
&lt;p&gt;
This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency: Normalize columns to make emphasize more important features&lt;/li&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org8949878"&gt;&lt;/a&gt;N-Grams&lt;br&gt;
&lt;div class="outline-text-5" id="text-org8949878"&gt;
&lt;p&gt;
This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://sklearn.feature_extraction.text.countvectorizer"&gt;Count Vectorizer&lt;/a&gt; is sklearn's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org4b8c6f5"&gt;&lt;/a&gt;Text Preprocessing&lt;br&gt;
&lt;div class="outline-text-5" id="text-org4b8c6f5"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;lowercase:
Change all the words to lower-case&lt;/li&gt;
&lt;li&gt;lemmatization:
Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)&lt;/li&gt;
&lt;li&gt;stemming:
Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)&lt;/li&gt;
&lt;li&gt;stopwords:
Remove common words (e.g. a, and, or, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org2c82575"&gt;&lt;/a&gt;The Bag Of Words Pipeline&lt;br&gt;
&lt;div class="outline-text-5" id="text-org2c82575"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Preprocessing (lowercase, lemmatization, stemming, stopwords)&lt;/li&gt;
&lt;li&gt;Create n-grams&lt;/li&gt;
&lt;li&gt;Postprocessing: TF/IDF&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc56f0f5" class="outline-4"&gt;
&lt;h4 id="orgc56f0f5"&gt;Embeddings (e.g. Word2Vec)&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc56f0f5"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses neural-nets&lt;/li&gt;
&lt;li&gt;much smaller vectors than bag of words&lt;/li&gt;
&lt;li&gt;But each word gets a vector so many more vectors&lt;/li&gt;
&lt;li&gt;similar words have similar word-vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfcdbc57" class="outline-2"&gt;
&lt;h2 id="orgfcdbc57"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfcdbc57"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org88fba06" class="outline-3"&gt;
&lt;h3 id="org88fba06"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org88fba06"&gt;
&lt;p&gt;
TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. Choose the correct statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; IDF scales features inversely proprotionally to a number of word occurrences over documents&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; IDF scales features proportionally to the frequency of the a word's occurrences&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; TF normalizes sums of the row values to 1&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; TF normalizes sums of the column values to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge6df840" class="outline-3"&gt;
&lt;h3 id="orge6df840"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge6df840"&gt;
&lt;p&gt;
Which of these methods can be used to preprocess text?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; stemming&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lower-case transformation&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lemmatization&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Stopword removal&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumping&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumbing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org55d36e7" class="outline-3"&gt;
&lt;h3 id="org55d36e7"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org55d36e7"&gt;
&lt;p&gt;
What is the main purpose of lemmatization and stemming?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to remove words which are not useful&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; to remove inflectional forms and sometimes derivationally related forms of a word to a common base form&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; To reduce the significance of common words&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to induce common word amplification standards to the most useful for machine learning algorithms form&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2518827" class="outline-3"&gt;
&lt;h3 id="org2518827"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2518827"&gt;
&lt;p&gt;
To learn Word2Vec embeddings we need:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; GloVe embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Labels for the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Labels for each word in the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgba8249f" class="outline-2"&gt;
&lt;h2 id="orgba8249f"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgba8249f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga3976f4" class="outline-3"&gt;
&lt;h3 id="orga3976f4"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga3976f4"&gt;
&lt;p&gt;
Select true statements about n-grams.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining should always be applied before computing n-grams (there is no such thing as Levenshteining)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; n-grams always help increase significance of important words (n-grams are about counts, not importance)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams features are typically sparse (n-grams count occurrences of words and not every word will be found in every document)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams can help utilize local context around each word (n-grams encode sequences of words)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcf624fa" class="outline-3"&gt;
&lt;h3 id="orgcf624fa"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcf624fa"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Bag of words usually produces longer vectors than Word2Vec (The number of features with BOW is equal to the number of unique words, Word2Vec limit is set beforehand)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Semantically similar words usually have similar word2vec embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; You do not need bag of words features in a competition if you have word2vec features (both approaches are useful and can work together)
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The meaning of each value in the Bag of Words matrix is unknown (The meaning of each value is how many times it occurred)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcdc7202" class="outline-3"&gt;
&lt;h3 id="orgcdc7202"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcdc7202"&gt;
&lt;p&gt;
Suppose in a new competition we are given a dataset of 2D medical images. We want to extract image descriptors from a hidden layer of a neural network pretrained on the ImageNet dataset. We will then use extracted descriptors to train a simple logistic regression model to classify images from our dataset.
&lt;/p&gt;

&lt;p&gt;
We are considering using two networks: ResNet-50 with an ImageNet accuracy of X and VGG-16 with an ImageNet accuracy of Y (X &amp;lt; Y). Select the true statements.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; With one pretrained CNN model you can get only one vector of descriptors for an image&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet 50 will always be better than the ones from VG-16 in our pipeline&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; It is not clear what descriptors are better on our dataset. We should evaluate both.
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet-50 and VGG-16 are always very similar in cosine distance&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; For any image, descriptors from the last hidden layer of ResNet-50 are the same as the descriptors from the last hidden layer of VGG-16&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab510b8" class="outline-3"&gt;
&lt;h3 id="orgab510b8"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgab510b8"&gt;
&lt;p&gt;
Data augmentation can be used at (1) train time and (2) test time
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; True, False&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, True&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True, True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc61632a" class="outline-2"&gt;
&lt;h2 id="orgc61632a"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc61632a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org543e595" class="outline-3"&gt;
&lt;h3 id="org543e595"&gt;Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org543e595"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org06bdd14" class="outline-4"&gt;
&lt;h4 id="org06bdd14"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org06bdd14"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/feature_extraction.html"&gt;SKlearn on feature extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/"&gt;blog post&lt;/a&gt; on extracting features from text&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org612c5b8" class="outline-4"&gt;
&lt;h4 id="org612c5b8"&gt;Word2Vec&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org612c5b8"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rare-technologies.com/word2vec-tutorial/"&gt;Blog post tutorial&lt;/a&gt; by the author of gensim&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"&gt;Text Classification post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://taylorwhitten.github.io/blog/word2vec"&gt;Another introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org91380e4" class="outline-4"&gt;
&lt;h4 id="org91380e4"&gt;Natural Language Processing with Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org91380e4"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/"&gt;nltk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc3ab4a9" class="outline-3"&gt;
&lt;h3 id="orgc3ab4a9"&gt;Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc3ab4a9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7db2bd8" class="outline-4"&gt;
&lt;h4 id="org7db2bd8"&gt;Pre-trained Models&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7db2bd8"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://keras.io/applications/"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"&gt;How To use a pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8f68f6b" class="outline-4"&gt;
&lt;h4 id="org8f68f6b"&gt;Fine-Tuning&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8f68f6b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/image_retraining"&gt;Re-train a tensorflow image classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html"&gt;Fine-tuning deep learning models in keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>featureextraction text images notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</guid><pubDate>Mon, 13 Aug 2018 14:17:52 GMT</pubDate></item><item><title>Feature Preprocessing</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orge84bc68"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org9c3de85"&gt;Numeric Feature Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orga4c23fc"&gt;Categorical And Ordinal Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org056dcc2"&gt;Dates and Times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org1db1b77"&gt;Coordinates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orge87c8a8"&gt;Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org4f27cf7"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge84bc68" class="outline-2"&gt;
&lt;h2 id="orge84bc68"&gt;Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge84bc68"&gt;
&lt;p&gt;
In every competition you need to:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;pre-process the data&lt;/li&gt;
&lt;li&gt;generate new features from the existing ones&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9c3de85" class="outline-2"&gt;
&lt;h2 id="org9c3de85"&gt;Numeric Feature Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9c3de85"&gt;
&lt;p&gt;
Some models (e.g. Linear Classification) need you to convert numeric-looking data to categorical data. Tree-based models don't need pre-processing as much as non tree-based models do.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgadbf40c" class="outline-3"&gt;
&lt;h3 id="orgadbf40c"&gt;Scaling&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgadbf40c"&gt;
&lt;p&gt;
If features have different value ranges then the model will  treat them differently.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"&gt;sklearn.preprocessing.MinMaxScalar&lt;/a&gt;
Scale by the range of values&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"&gt;sklearn.preprocessing.StandardScalar&lt;/a&gt;
Scale the data to have a mean of 0 and a standard deviation of 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8b5df0a" class="outline-3"&gt;
&lt;h3 id="org8b5df0a"&gt;Outliers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8b5df0a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Clip (&lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html"&gt;numpy.clip&lt;/a&gt;)values to get rid of unusual values that mess with the model. (Winsorization)&lt;/li&gt;
&lt;li&gt;Rank Transform them (&lt;a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html"&gt;scipy.stats.rankdata&lt;/a&gt;) so that they are ordered and have the same distance between them (they are uniformly distributed)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8fb0405" class="outline-3"&gt;
&lt;h3 id="org8fb0405"&gt;Transformation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8fb0405"&gt;
&lt;p&gt;
For linear models and neural networks, transforming the data can sometimes help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Log Transform: &lt;a href="https://duckduckgo.com/?q=numpy+log&amp;amp;t=canonical&amp;amp;ia=web"&gt;numpy.log(1 + x)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Power Less Than 1 Transform: &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html"&gt;numpy.sqrt(x + 2/3)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1fc0f4e" class="outline-3"&gt;
&lt;h3 id="org1fc0f4e"&gt;Feature Generation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1fc0f4e"&gt;
&lt;p&gt;
Sometimes you can convert separate columns to get new ones. This requires exploratory data analysis.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Fractional Parts: people perceive numbers with fractions differently so sometime separating out the fractional part makes the model perform better because it is the more important part&lt;/li&gt;
&lt;li&gt;You can derive new values mathematically (e.g. distance using height and width)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga4c23fc" class="outline-2"&gt;
&lt;h2 id="orga4c23fc"&gt;Categorical And Ordinal Features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga4c23fc"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Ordinal Features have an ordering but even if they are labeled numerically (e.g. 1, 2, 3) they aren't numeric because there is no implication about the distance between them&lt;/li&gt;
&lt;li&gt;label and frequency encoding are commonly used for tree-based models&lt;/li&gt;
&lt;li&gt;one-hot encoding is more common for non-tree-based models&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d87ed2" class="outline-3"&gt;
&lt;h3 id="org9d87ed2"&gt;Label Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d87ed2"&gt;
&lt;p&gt;
Some models need numeric values or numeric values that don't have implied ordering so you want to encode them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"&gt;sklearn.preprocessing.LabelEncoder&lt;/a&gt;
Sorts the labels then encodes them as integers&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html"&gt;Pandas.factorize&lt;/a&gt;
Encodes the labels in the order they appear. This makes more sense if there is a meaning to the order in which labels appear.&lt;/li&gt;
&lt;li&gt;This is more commonly used with tree-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb92baa9" class="outline-3"&gt;
&lt;h3 id="orgb92baa9"&gt;Frequency Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb92baa9"&gt;
&lt;p&gt;
Encode the values to a fraction of all the labels. Can work with linear models if the frequency is correlated with the target value.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;titanic.groupby("Embarked").size()/len(titanic)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from scipy.stats import rankedata&lt;/code&gt; (&lt;a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html"&gt;rankedata&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;More common with tree-based methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf010430" class="outline-3"&gt;
&lt;h3 id="orgf010430"&gt;One Hot Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf010430"&gt;
&lt;p&gt;
Creates one column for each label and puts a 1 in the column that matches. Works with both linear and tree-based models, but can be inefficient with tree-based models.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"&gt;pandas.get_dummies&lt;/a&gt; - converts strings into column encodings&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"&gt;sklearn.preprocessing.OneHotEncoder&lt;/a&gt; - converts numeric categorical data into column encodings&lt;/li&gt;
&lt;li&gt;One hot encoding is better for non-tree models.&lt;/li&gt;
&lt;li&gt;This allows easier feature interactions (encode combined features (e.g. gender and class) rather than encoding them separately) 
&lt;ul class="org-ul"&gt;
&lt;li&gt;This is more common with linear models and KNN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org056dcc2" class="outline-2"&gt;
&lt;h2 id="org056dcc2"&gt;Dates and Times&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org056dcc2"&gt;
&lt;p&gt;
When working with seasonal data, sometimes the relative date-stamps are more important than the actual dates (how close to Christmas?).
Sometimes you want the time between events.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1db1b77" class="outline-2"&gt;
&lt;h2 id="org1db1b77"&gt;Coordinates&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1db1b77"&gt;
&lt;p&gt;
Sometimes you want exact coordinates, but most times you want distances to some center.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge87c8a8" class="outline-2"&gt;
&lt;h2 id="orge87c8a8"&gt;Missing Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge87c8a8"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;"missing" data might mean outliers - values that are probably wrong&lt;/li&gt;
&lt;li&gt;avoid replacing missing values before feature engineering - it can throw off what you do&lt;/li&gt;
&lt;li&gt;Gradient Boost Trees can handle isNaN, so you don't have to do anything&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7bbbbae" class="outline-3"&gt;
&lt;h3 id="org7bbbbae"&gt;Numeric&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7bbbbae"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0c3fa7" class="outline-4"&gt;
&lt;h4 id="orga0c3fa7"&gt;Fill NA Approaches&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga0c3fa7"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;-999, -1, other numbers
&lt;ul class="org-ul"&gt;
&lt;li&gt;lets you categorize missing values&lt;/li&gt;
&lt;li&gt;throws some models off (e.g. linear models and neural networks)&lt;/li&gt;
&lt;li&gt;one solution is to create a new feature for missing values, but this has now increased the amount of data you need (curse of dimensionality)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;mean, median, some central tendency
&lt;ul class="org-ul"&gt;
&lt;li&gt;This can throw the model off&lt;/li&gt;
&lt;li&gt;it is sometimes better to ignore missing data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;recronstructed value&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4f27cf7" class="outline-2"&gt;
&lt;h2 id="org4f27cf7"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4f27cf7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb167197" class="outline-3"&gt;
&lt;h3 id="orgb167197"&gt;Feature Pre-processing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb167197"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html"&gt;SKlearn's Preprocessing Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling"&gt;Andrew Ng on Feature Scaling and its effect on Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"&gt;Sebastian Raschka on Feature Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge3f8582" class="outline-3"&gt;
&lt;h3 id="orge3f8582"&gt;Feature Engineering&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge3f8582"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"&gt;Machine Learning Mastery on Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering"&gt;Quora: What are some best practices in Feature Engineering?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>features preprocessing notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</guid><pubDate>Wed, 08 Aug 2018 04:41:10 GMT</pubDate></item><item><title>Machine Learning Recap</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org0692788" class="outline-2"&gt;
&lt;h2 id="org0692788"&gt;The Main Categories&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0692788"&gt;
&lt;p&gt;
These are the four main categories of supervised machine learning algorithms that you'll encounter in kaggle competitions.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Linear Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;vowpal rabbit (for really large datasets)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tree-Based Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;xgboost: faster than sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;k-Nearest Neighbors
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0c70821" class="outline-2"&gt;
&lt;h2 id="org0c70821"&gt;The No Free Lunch Theorem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0c70821"&gt;
&lt;blockquote&gt;
&lt;p&gt;
There is no method which outperforms all others for all tasks.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
You cannot assume that an algorithm that did well on one set of data will do well on another. All algorithms have weaknesses, so you have to test multiple algorithms on each data set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdbe3b31" class="outline-2"&gt;
&lt;h2 id="orgdbe3b31"&gt;Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdbe3b31"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;there is no one algorithm to rule them all&lt;/li&gt;
&lt;li&gt;Linear models split spaces into two sub-spaces&lt;/li&gt;
&lt;li&gt;tree-based models spit spaces into boxes&lt;/li&gt;
&lt;li&gt;kNN relies on measuring the 'closeness' between points&lt;/li&gt;
&lt;li&gt;Neural Networks provide non-linear decision boundaries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In general the two most powerful methods are &lt;b&gt;Gradient Boosted Decision Trees&lt;/b&gt; and &lt;b&gt;Neural Networks&lt;/b&gt;, but this won't always be the case.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics algorithms notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</guid><pubDate>Sun, 05 Aug 2018 01:13:44 GMT</pubDate></item><item><title>Real-World vs Kaggle</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/real-world-vs-kaggle/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org85c2bae" class="outline-2"&gt;
&lt;h2 id="org85c2bae"&gt;A Real World Pipeline&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org85c2bae"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;What is the business problem that you are trying to solve?&lt;/li&gt;
&lt;li&gt;What is the formal version of the problem?&lt;/li&gt;
&lt;li&gt;How do you collect data?&lt;/li&gt;
&lt;li&gt;How do you preprocess the data?&lt;/li&gt;
&lt;li&gt;How do you create the model?
&lt;ul class="org-ul"&gt;
&lt;li&gt;what is the appropriate algorithm?&lt;/li&gt;
&lt;li&gt;what is the correct metric?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;How do you evaluate the model in a real world situation?&lt;/li&gt;
&lt;li&gt;How do you deploy the model?
&lt;ul class="org-ul"&gt;
&lt;li&gt;How do you monitor its performance?&lt;/li&gt;
&lt;li&gt;How do you update it over time?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org32ef801" class="outline-2"&gt;
&lt;h2 id="org32ef801"&gt;A Competition Pipeline&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org32ef801"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;How do you pre-process the data?&lt;/li&gt;
&lt;li&gt;How do you create the model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5ae98da" class="outline-2"&gt;
&lt;h2 id="org5ae98da"&gt;So, how do you use a competition then?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5ae98da"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;It's good to learn about machine learning&lt;/li&gt;
&lt;li&gt;It' not just about the algorithms, let the data guide what you do&lt;/li&gt;
&lt;li&gt;Try to be creative and do things that haven't been done before&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics kaggle</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/real-world-vs-kaggle/</guid><pubDate>Sun, 05 Aug 2018 01:02:24 GMT</pubDate></item><item><title>Kaggle Mechanics</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/kaggle-mechanics/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgf34d8c8" class="outline-2"&gt;
&lt;h2 id="orgf34d8c8"&gt;The Basics&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf34d8c8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1a3618d" class="outline-3"&gt;
&lt;h3 id="org1a3618d"&gt;Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1a3618d"&gt;
&lt;p&gt;
Every competition provides data so you can create a model, but there isn't a standardized format. Sometimes it will be CSVz, sometimes excel spreadsheets, sometimes image files, the sources can vary so you should read the data descriptions and adapt what you do to the competition. You aren't always limited to the data that is provided. If you were creating an image recognition model, for instance, it might be okay to include outside images or pre-trained models, it depends on the particulars of the competition.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd1cf28f" class="outline-3"&gt;
&lt;h3 id="orgd1cf28f"&gt;Models&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd1cf28f"&gt;
&lt;p&gt;
This is what you are trying to create - a representation of the population based on the data you are given that will allow you to predict outcomes based on inputs not given in the data. The model needs two main features:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;li&gt;reproducibility&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Note that a model is not the same as an algorithm. You might have to combine multiple algorithms in order to build your model. The key to a model is that it maps inputs to outputs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb481425" class="outline-3"&gt;
&lt;h3 id="orgb481425"&gt;Submission&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb481425"&gt;
&lt;p&gt;
Your submission is typically your predictions on a test set. This isn't always the case but it is the most common way that the competitions are run.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd6219f8" class="outline-3"&gt;
&lt;h3 id="orgd6219f8"&gt;Evaluation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd6219f8"&gt;
&lt;p&gt;
How can you tell how well your model does? You need a function that maps your model and a data set to a score that evaluates how well the model does. There are many different metrics to use (accuracy, precision, recall, etc.) but the competition will choose one and tell you what it is in the description so make sure you read the description to get it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc848153" class="outline-3"&gt;
&lt;h3 id="orgc848153"&gt;Leaderboard&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc848153"&gt;
&lt;p&gt;
This is the relative ranking of the participants in the competition. This is what makes it a competition. Even if your metric tells you your model is doing well, if you are ranked at the bottom, you still won't win. There are actually two leaderboards - public and private. The evaluation dataset is split by kaggle into two sets, public and private, and during the competition the results of testing on the public data are shown on the leaderborad. Once the competition is over the leaderboard is displayed using the evaluations using the private data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc97d7d3" class="outline-2"&gt;
&lt;h2 id="orgc97d7d3"&gt;Other Competitions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc97d7d3"&gt;
&lt;p&gt;
&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; isn't the only one running data-science competititons. Here are some others.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.drivendata.org/"&gt;Driven Data&lt;/a&gt;: Data Science competititons aimed at social problems&lt;/li&gt;
&lt;li&gt;&lt;a href="http://codalab.org/"&gt;Coda Lab&lt;/a&gt;: Competitions using research datasets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datasciencechallenge.org/"&gt;Data Science Challenge&lt;/a&gt;: Using data-science to solve government-scale problems.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.datascience.net/fr/challenge#"&gt;Data Science dot net&lt;/a&gt;: A european data-science competition site.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics rules</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/kaggle-mechanics/</guid><pubDate>Sun, 05 Aug 2018 00:18:12 GMT</pubDate></item></channel></rss>