<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about features preprocessing notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/features-preprocessing-notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 15 Sep 2018 00:10:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Feature Preprocessing</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orge84bc68"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org9c3de85"&gt;Numeric Feature Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orga4c23fc"&gt;Categorical And Ordinal Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org056dcc2"&gt;Dates and Times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org1db1b77"&gt;Coordinates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orge87c8a8"&gt;Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org4f27cf7"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge84bc68" class="outline-2"&gt;
&lt;h2 id="orge84bc68"&gt;Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge84bc68"&gt;
&lt;p&gt;
In every competition you need to:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;pre-process the data&lt;/li&gt;
&lt;li&gt;generate new features from the existing ones&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9c3de85" class="outline-2"&gt;
&lt;h2 id="org9c3de85"&gt;Numeric Feature Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9c3de85"&gt;
&lt;p&gt;
Some models (e.g. Linear Classification) need you to convert numeric-looking data to categorical data. Tree-based models don't need pre-processing as much as non tree-based models do.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgadbf40c" class="outline-3"&gt;
&lt;h3 id="orgadbf40c"&gt;Scaling&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgadbf40c"&gt;
&lt;p&gt;
If features have different value ranges then the model will  treat them differently.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"&gt;sklearn.preprocessing.MinMaxScalar&lt;/a&gt;
Scale by the range of values&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"&gt;sklearn.preprocessing.StandardScalar&lt;/a&gt;
Scale the data to have a mean of 0 and a standard deviation of 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8b5df0a" class="outline-3"&gt;
&lt;h3 id="org8b5df0a"&gt;Outliers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8b5df0a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Clip (&lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html"&gt;numpy.clip&lt;/a&gt;)values to get rid of unusual values that mess with the model. (Winsorization)&lt;/li&gt;
&lt;li&gt;Rank Transform them (&lt;a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html"&gt;scipy.stats.rankdata&lt;/a&gt;) so that they are ordered and have the same distance between them (they are uniformly distributed)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8fb0405" class="outline-3"&gt;
&lt;h3 id="org8fb0405"&gt;Transformation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8fb0405"&gt;
&lt;p&gt;
For linear models and neural networks, transforming the data can sometimes help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Log Transform: &lt;a href="https://duckduckgo.com/?q=numpy+log&amp;amp;t=canonical&amp;amp;ia=web"&gt;numpy.log(1 + x)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Power Less Than 1 Transform: &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html"&gt;numpy.sqrt(x + 2/3)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1fc0f4e" class="outline-3"&gt;
&lt;h3 id="org1fc0f4e"&gt;Feature Generation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1fc0f4e"&gt;
&lt;p&gt;
Sometimes you can convert separate columns to get new ones. This requires exploratory data analysis.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Fractional Parts: people perceive numbers with fractions differently so sometime separating out the fractional part makes the model perform better because it is the more important part&lt;/li&gt;
&lt;li&gt;You can derive new values mathematically (e.g. distance using height and width)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga4c23fc" class="outline-2"&gt;
&lt;h2 id="orga4c23fc"&gt;Categorical And Ordinal Features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga4c23fc"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Ordinal Features have an ordering but even if they are labeled numerically (e.g. 1, 2, 3) they aren't numeric because there is no implication about the distance between them&lt;/li&gt;
&lt;li&gt;label and frequency encoding are commonly used for tree-based models&lt;/li&gt;
&lt;li&gt;one-hot encoding is more common for non-tree-based models&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d87ed2" class="outline-3"&gt;
&lt;h3 id="org9d87ed2"&gt;Label Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d87ed2"&gt;
&lt;p&gt;
Some models need numeric values or numeric values that don't have implied ordering so you want to encode them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"&gt;sklearn.preprocessing.LabelEncoder&lt;/a&gt;
Sorts the labels then encodes them as integers&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html"&gt;Pandas.factorize&lt;/a&gt;
Encodes the labels in the order they appear. This makes more sense if there is a meaning to the order in which labels appear.&lt;/li&gt;
&lt;li&gt;This is more commonly used with tree-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb92baa9" class="outline-3"&gt;
&lt;h3 id="orgb92baa9"&gt;Frequency Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb92baa9"&gt;
&lt;p&gt;
Encode the values to a fraction of all the labels. Can work with linear models if the frequency is correlated with the target value.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;titanic.groupby("Embarked").size()/len(titanic)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from scipy.stats import rankedata&lt;/code&gt; (&lt;a href="https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html"&gt;rankedata&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;More common with tree-based methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf010430" class="outline-3"&gt;
&lt;h3 id="orgf010430"&gt;One Hot Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf010430"&gt;
&lt;p&gt;
Creates one column for each label and puts a 1 in the column that matches. Works with both linear and tree-based models, but can be inefficient with tree-based models.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"&gt;pandas.get_dummies&lt;/a&gt; - converts strings into column encodings&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"&gt;sklearn.preprocessing.OneHotEncoder&lt;/a&gt; - converts numeric categorical data into column encodings&lt;/li&gt;
&lt;li&gt;One hot encoding is better for non-tree models.&lt;/li&gt;
&lt;li&gt;This allows easier feature interactions (encode combined features (e.g. gender and class) rather than encoding them separately) 
&lt;ul class="org-ul"&gt;
&lt;li&gt;This is more common with linear models and KNN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org056dcc2" class="outline-2"&gt;
&lt;h2 id="org056dcc2"&gt;Dates and Times&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org056dcc2"&gt;
&lt;p&gt;
When working with seasonal data, sometimes the relative date-stamps are more important than the actual dates (how close to Christmas?).
Sometimes you want the time between events.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1db1b77" class="outline-2"&gt;
&lt;h2 id="org1db1b77"&gt;Coordinates&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1db1b77"&gt;
&lt;p&gt;
Sometimes you want exact coordinates, but most times you want distances to some center.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge87c8a8" class="outline-2"&gt;
&lt;h2 id="orge87c8a8"&gt;Missing Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge87c8a8"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;"missing" data might mean outliers - values that are probably wrong&lt;/li&gt;
&lt;li&gt;avoid replacing missing values before feature engineering - it can throw off what you do&lt;/li&gt;
&lt;li&gt;Gradient Boost Trees can handle isNaN, so you don't have to do anything&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7bbbbae" class="outline-3"&gt;
&lt;h3 id="org7bbbbae"&gt;Numeric&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7bbbbae"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0c3fa7" class="outline-4"&gt;
&lt;h4 id="orga0c3fa7"&gt;Fill NA Approaches&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga0c3fa7"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;-999, -1, other numbers
&lt;ul class="org-ul"&gt;
&lt;li&gt;lets you categorize missing values&lt;/li&gt;
&lt;li&gt;throws some models off (e.g. linear models and neural networks)&lt;/li&gt;
&lt;li&gt;one solution is to create a new feature for missing values, but this has now increased the amount of data you need (curse of dimensionality)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;mean, median, some central tendency
&lt;ul class="org-ul"&gt;
&lt;li&gt;This can throw the model off&lt;/li&gt;
&lt;li&gt;it is sometimes better to ignore missing data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;recronstructed value&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4f27cf7" class="outline-2"&gt;
&lt;h2 id="org4f27cf7"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4f27cf7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb167197" class="outline-3"&gt;
&lt;h3 id="orgb167197"&gt;Feature Pre-processing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb167197"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html"&gt;SKlearn's Preprocessing Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling"&gt;Andrew Ng on Feature Scaling and its effect on Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"&gt;Sebastian Raschka on Feature Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge3f8582" class="outline-3"&gt;
&lt;h3 id="orge3f8582"&gt;Feature Engineering&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge3f8582"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"&gt;Machine Learning Mastery on Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering"&gt;Quora: What are some best practices in Feature Engineering?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>features preprocessing notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</guid><pubDate>Wed, 08 Aug 2018 04:41:10 GMT</pubDate></item></channel></rss>