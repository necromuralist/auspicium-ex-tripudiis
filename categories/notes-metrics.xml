<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about notes metrics)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/notes-metrics.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 25 Sep 2018 23:01:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Optimizing Metrics</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-metrics/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-metrics/#org0348804"&gt;Mean Squared Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-metrics/#org514dee0"&gt;Mean Absolute Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-metrics/#org05013a2"&gt;Mean Squared Probability Error and Mean Absolute Probability Error&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0348804" class="outline-2"&gt;
&lt;h2 id="org0348804"&gt;Mean Squared Error&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0348804"&gt;
&lt;p&gt;
This works, just use it as the optimization metric.
Sometimes this will be called &lt;i&gt;L2&lt;/i&gt; loss.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org514dee0" class="outline-2"&gt;
&lt;h2 id="org514dee0"&gt;Mean Absolute Error&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org514dee0"&gt;
&lt;p&gt;
Once again, this works so just use it.
Sometimes this will be called &lt;i&gt;L1&lt;/i&gt; loss - it isn't as widely implemented.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org05013a2" class="outline-2"&gt;
&lt;h2 id="org05013a2"&gt;Mean Squared Probability Error and Mean Absolute Probability Error&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org05013a2"&gt;
&lt;p&gt;
Some libraries will let you use them as `sample_weights`
Some libraries (like sklearn) will require you to re-sample the data &lt;code&gt;df.sample(weights=sample_weights)&lt;/code&gt;
Once you re-sample the data you can use any model that optimizes MSE or MAE
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes metrics</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/optimizing-metrics/</guid><pubDate>Sat, 22 Sep 2018 23:26:49 GMT</pubDate></item><item><title>General Approaches to Metrics Optimization</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/general-approaches-to-metrics-optimization/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgb424731" class="outline-2"&gt;
&lt;h2 id="orgb424731"&gt;What do we mean by &lt;i&gt;loss&lt;/i&gt; and &lt;i&gt;metric&lt;/i&gt;?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb424731"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;the &lt;code&gt;metric&lt;/code&gt; is what we &lt;i&gt;want&lt;/i&gt; to optimize - but we sometimes don't really know how to optimize this, this is just how we evaluate the model in the end&lt;/li&gt;
&lt;li&gt;&lt;code&gt;loss&lt;/code&gt; is what the model actually optimizes&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org149a4af" class="outline-2"&gt;
&lt;h2 id="org149a4af"&gt;How do you optimize the target metric?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org149a4af"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;In some cases you can just use them (e.g. &lt;i&gt;MSE&lt;/i&gt;, &lt;i&gt;logloss&lt;/i&gt;)&lt;/li&gt;
&lt;li&gt;In some cases you need to do a preprocessing training step with another metric
(e.g. &lt;i&gt;MSPE&lt;/i&gt;, &lt;i&gt;MAPE&lt;/i&gt;, &lt;i&gt;RMSLE&lt;/i&gt;)&lt;/li&gt;
&lt;li&gt;In osme cases you need to train on a different metric and then use post-processing during the prediction step&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge307b52" class="outline-2"&gt;
&lt;h2 id="orge307b52"&gt;Early Stopping&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge307b52"&gt;
&lt;p&gt;
To work around these problems, use two metrics &lt;i&gt;m1&lt;/i&gt; and &lt;i&gt;m2&lt;/i&gt; and optimize on metric &lt;i&gt;m1&lt;/i&gt; while monitoring &lt;i&gt;m2&lt;/i&gt;, then stop when &lt;i&gt;m2&lt;/i&gt; is optimal.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes metrics</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/general-approaches-to-metrics-optimization/</guid><pubDate>Sat, 22 Sep 2018 23:04:35 GMT</pubDate></item><item><title>Metrics Optimization 2</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics-optimization-2/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orga06ea0a" class="outline-2"&gt;
&lt;h2 id="orga06ea0a"&gt;(R)MSPE, MAPE, and (R)MSLE&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga06ea0a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0125661" class="outline-3"&gt;
&lt;h3 id="org0125661"&gt;An Off-By-One Example&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0125661"&gt;
&lt;p&gt;
Suppose we are predicting sales for two shops and the two shops have different sales volumes but our predictions for both cases are off by one. In this case our Mean-Squared-Error (MSE) might be the same, but they have a different significance.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Shop&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Actual&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Predicted&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;MSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;9&lt;/td&gt;
&lt;td class="org-left"&gt;10&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;999&lt;/td&gt;
&lt;td class="org-left"&gt;1,000&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc555c3a" class="outline-2"&gt;
&lt;h2 id="orgc555c3a"&gt;Root Mean Squared Percentage Error and Mean Absolute Percentage Error&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc555c3a"&gt;
&lt;p&gt;
The MSE and Mean-Absolute-Error (MAE) are absolute errors which don't take into account how significant the error is. There are two relative errors,  Mean-Squared-Percentage-Error (MSPE) and Mean-Absolute-Percentage-Error (MAPE) that divide each error term by the actual value to give you a realive error instead of an absolute error.
&lt;/p&gt;

&lt;p&gt;
\[
MSPE = \frac{1}{N} \sum_{i=1}^n \left( \frac{y_i - \hat{y}}{y_i}\right)^2
\]
&lt;/p&gt;

&lt;p&gt;
\[
MAPE = \frac{1}{N} \sum_{i=1}^n \left| \frac{y_i - \hat{y}}{y_i}\right|
\]
&lt;/p&gt;

&lt;p&gt;
The MAPE will be inversely proportional to its target and the MSPE will be inversely proportional to the square of the target.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgea5e0a2" class="outline-3"&gt;
&lt;h3 id="orgea5e0a2"&gt;Optimal Constant Predictions&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgea5e0a2"&gt;
&lt;p&gt;
The best constant prediction you can make when using the Mean Squared Error is to predict the mean of the target values. The best prediction you can make for the MSPE is to take a weighted mean of the target values. The best constant prediction you can make for the Mean Absolute Percentage Error is the weighted median.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf9c7416" class="outline-2"&gt;
&lt;h2 id="orgf9c7416"&gt;Root Mean Squared Logarithmic Error (MSLE)&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf9c7416"&gt;
&lt;p&gt;
\[
MSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N (\log (y_i + 1) - \log(\hat{y}_i + 1))^2}\\
= \sqrt{MSE(\log(y_i + 1), \log(\hat{y}_i + 1))}
\]
&lt;/p&gt;

&lt;p&gt;
You add a 1 to each term to prevent you from trying to take the &lt;i&gt;log&lt;/i&gt; of 0, which is undefined. The RMSLE is biased toward predictions that are higher than the actual values rather than lower.
&lt;/p&gt;

&lt;p&gt;
These are the best constant predictions you can make for the competition data set.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Constant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;MSE&lt;/td&gt;
&lt;td class="org-right"&gt;11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;RMSLE&lt;/td&gt;
&lt;td class="org-right"&gt;9.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MAE&lt;/td&gt;
&lt;td class="org-right"&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MSPE&lt;/td&gt;
&lt;td class="org-right"&gt;6.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MAPE&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes metrics</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics-optimization-2/</guid><pubDate>Wed, 19 Sep 2018 15:04:33 GMT</pubDate></item></channel></rss>