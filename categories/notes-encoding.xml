<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about notes encoding)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/notes-encoding.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 07 Oct 2018 01:39:02 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Mean Encoding</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/#org01e0376"&gt;Mean Encoding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/#org95524d8"&gt;Regularization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/#org5fd0890"&gt;Generalizations and Extensions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/#org07d67f5"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org01e0376" class="outline-2"&gt;
&lt;h2 id="org01e0376"&gt;Mean Encoding&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org01e0376"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org11b83a0" class="outline-3"&gt;
&lt;h3 id="org11b83a0"&gt;Introduction&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org11b83a0"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;also called target encoding and likelihood encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
It is a way to encode a categorical feature. Uses the fraction of times the feature is 1 out of all the times the feature is in the data set (for binary classification).
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd5dfb59" class="outline-4"&gt;
&lt;h4 id="orgd5dfb59"&gt;Why does it work?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd5dfb59"&gt;
&lt;p&gt;
Unlike regular encoding, which has no real meaning to the labels, mean encoding imposes an ordering. This allows you to reduce your loss while using shorter trees.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4bd42a6" class="outline-4"&gt;
&lt;h4 id="org4bd42a6"&gt;How do you calculate it?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4bd42a6"&gt;
&lt;p&gt;
There are multiple ways.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Likelihood = \(\frac{count of ones}/{total count}\) = mean(target)&lt;/li&gt;
&lt;li&gt;Weight of evidence = \(\ln\left(\frac{count of ones}{count of zeros}\right)\)&lt;/li&gt;
&lt;li&gt;Count = sum(target) = count of ones&lt;/li&gt;
&lt;li&gt;Diff = count of ones - count of zeros&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb960867" class="outline-4"&gt;
&lt;h4 id="orgb960867"&gt;When does it fail?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb960867"&gt;
&lt;p&gt;
If you have lots of feature instances with few cases it will tend to overfit.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org95524d8" class="outline-2"&gt;
&lt;h2 id="org95524d8"&gt;Regularization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org95524d8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2289034" class="outline-3"&gt;
&lt;h3 id="org2289034"&gt;Four Types&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2289034"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Cross-validation loop inside the training data&lt;/li&gt;
&lt;li&gt;Smoothing&lt;/li&gt;
&lt;li&gt;Adding random noise&lt;/li&gt;
&lt;li&gt;Sorting and calculating the expanding mean&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org94e0c5f" class="outline-3"&gt;
&lt;h3 id="org94e0c5f"&gt;Cross Validation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org94e0c5f"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Usually 4 or 5 folds are enough&lt;/li&gt;
&lt;li&gt;Need to watch out for extreme cases like leave-out-one (LOO)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Here's an example of this method using sklearn.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;folds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;training_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;folds&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;x_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;training_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;x_validation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;validation_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# 'columns' is a list of columns to encode&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
	&lt;span class="n"&gt;x_validation&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coulmn&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"_mean_target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;means&lt;/span&gt;
    &lt;span class="c1"&gt;# train_new is a dataframe copy we made of the training data&lt;/span&gt;
    &lt;span class="n"&gt;train_new&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_validation&lt;/span&gt;

&lt;span class="n"&gt;global_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# replace nans with the global mean&lt;/span&gt;
&lt;span class="n"&gt;train_new&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;global_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7ee9a96" class="outline-3"&gt;
&lt;h3 id="org7ee9a96"&gt;Smoothing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7ee9a96"&gt;
&lt;p&gt;
Use a value \(\alpha\) to control the amount of regularization. This isn't a regularization method in and of itself, you use it with other methods.
&lt;/p&gt;

&lt;p&gt;
\[
\frac{mean(targte) \times n_{rows} + \textit{global mean} \times \alpha}{n_{rows} + \alpha}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org773f7ae" class="outline-3"&gt;
&lt;h3 id="org773f7ae"&gt;Noise&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org773f7ae"&gt;
&lt;p&gt;
Adding noise degrades the quality of the encoding in the training data. This is usually used with &lt;i&gt;leave-one-out&lt;/i&gt; encoding to prevent overfitting. You have to figure out how much noise to add through experimentation.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org92848c2" class="outline-3"&gt;
&lt;h3 id="org92848c2"&gt;Expanding Mean&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org92848c2"&gt;
&lt;p&gt;
This introduces the least amount of leakage from the target variable and doesn't require hyper-parameters for you to tune. The downside is that the encoding quality is irregular. There is a built-in implementation in the &lt;code&gt;CatBoost&lt;/code&gt; library.
&lt;/p&gt;

&lt;p&gt;
Here's a pandas implementation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;cumulative_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s2"&gt;"target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;cumulative_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumcount&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;train_new&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"_mean_target"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cumulative_sum&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cumulative_count&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8a972ae" class="outline-3"&gt;
&lt;h3 id="org8a972ae"&gt;Which one should you use?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8a972ae"&gt;
&lt;p&gt;
Cross Validation Loops and Expanding Means are the most practical to use.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5fd0890" class="outline-2"&gt;
&lt;h2 id="org5fd0890"&gt;Generalizations and Extensions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5fd0890"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9713263" class="outline-3"&gt;
&lt;h3 id="org9713263"&gt;Regression and Multiclass&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org07d67f5" class="outline-2"&gt;
&lt;h2 id="org07d67f5"&gt;Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org07d67f5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbbf575d" class="outline-3"&gt;
&lt;h3 id="orgbbf575d"&gt;Advantages&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbbf575d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Compact transformation of categorical variables&lt;/li&gt;
&lt;li&gt;Powerful basis for feature engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org28db812" class="outline-3"&gt;
&lt;h3 id="org28db812"&gt;Disadvantages&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org28db812"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Needs careful validation, it's easy to overfit&lt;/li&gt;
&lt;li&gt;Only certain data sets will show a significant improvement from using it&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes encoding</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/mean-encoding/</guid><pubDate>Mon, 24 Sep 2018 00:56:27 GMT</pubDate></item></channel></rss>