<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about featureextraction text images notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/featureextraction-text-images-notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 07 Sep 2018 19:57:42 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Feature Extraction From Text and Images</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgfeb2b30"&gt;How do you convert text to data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgd0d2c46"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orga47dfcb"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgeb9a2d8"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfeb2b30" class="outline-2"&gt;
&lt;h2 id="orgfeb2b30"&gt;How do you convert text to data?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfeb2b30"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2bfe738" class="outline-3"&gt;
&lt;h3 id="org2bfe738"&gt;Two Main Methods&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2bfe738"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga60dac5" class="outline-4"&gt;
&lt;h4 id="orga60dac5"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga60dac5"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org9bb0d0f"&gt;&lt;/a&gt;Vectorization&lt;br&gt;
&lt;div class="outline-text-5" id="text-org9bb0d0f"&gt;
&lt;p&gt;
This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;CountVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgb771ad6"&gt;&lt;/a&gt;Term Frequency/Inverse Document Frequency (TF/IDF)&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgb771ad6"&gt;
&lt;p&gt;
This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency: Normalize columns to make emphasize more important features&lt;/li&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgb6ea07f"&gt;&lt;/a&gt;N-Grams&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgb6ea07f"&gt;
&lt;p&gt;
This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://sklearn.feature_extraction.text.countvectorizer"&gt;Count Vectorizer&lt;/a&gt; is sklearn's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org0e2c3f7"&gt;&lt;/a&gt;Text Preprocessing&lt;br&gt;
&lt;div class="outline-text-5" id="text-org0e2c3f7"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;lowercase:
Change all the words to lower-case&lt;/li&gt;
&lt;li&gt;lemmatization:
Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)&lt;/li&gt;
&lt;li&gt;stemming:
Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)&lt;/li&gt;
&lt;li&gt;stopwords:
Remove common words (e.g. a, and, or, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org94da243"&gt;&lt;/a&gt;The Bag Of Words Pipeline&lt;br&gt;
&lt;div class="outline-text-5" id="text-org94da243"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Preprocessing (lowercase, lemmatization, stemming, stopwords)&lt;/li&gt;
&lt;li&gt;Create n-grams&lt;/li&gt;
&lt;li&gt;Postprocessing: TF/IDF&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5dfff27" class="outline-4"&gt;
&lt;h4 id="org5dfff27"&gt;Embeddings (e.g. Word2Vec)&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5dfff27"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses neural-nets&lt;/li&gt;
&lt;li&gt;much smaller vectors than bag of words&lt;/li&gt;
&lt;li&gt;But each word gets a vector so many more vectors&lt;/li&gt;
&lt;li&gt;similar words have similar word-vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd0d2c46" class="outline-2"&gt;
&lt;h2 id="orgd0d2c46"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd0d2c46"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org46a9334" class="outline-3"&gt;
&lt;h3 id="org46a9334"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org46a9334"&gt;
&lt;p&gt;
TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. Choose the correct statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; IDF scales features inversely proprotionally to a number of word occurrences over documents&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; IDF scales features proportionally to the frequency of the a word's occurrences&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; TF normalizes sums of the row values to 1&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; TF normalizes sums of the column values to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5c96619" class="outline-3"&gt;
&lt;h3 id="org5c96619"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5c96619"&gt;
&lt;p&gt;
Which of these methods can be used to preprocess text?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; stemming&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lower-case transformation&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lemmatization&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Stopword removal&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumping&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumbing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb29fbea" class="outline-3"&gt;
&lt;h3 id="orgb29fbea"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb29fbea"&gt;
&lt;p&gt;
What is the main purpose of lemmatization and stemming?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to remove words which are not useful&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; to remove inflectional forms and sometimes derivationally related forms of a word to a common base form&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; To reduce the significance of common words&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to induce common word amplification standards to the most useful for machine learning algorithms form&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6e9e549" class="outline-3"&gt;
&lt;h3 id="org6e9e549"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6e9e549"&gt;
&lt;p&gt;
To learn Word2Vec embeddings we need:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; GloVe embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Labels for the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Labels for each word in the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga47dfcb" class="outline-2"&gt;
&lt;h2 id="orga47dfcb"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga47dfcb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9f4bf4d" class="outline-3"&gt;
&lt;h3 id="org9f4bf4d"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9f4bf4d"&gt;
&lt;p&gt;
Select true statements about n-grams.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining should always be applied before computing n-grams (there is no such thing as Levenshteining)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; n-grams always help increase significance of important words (n-grams are about counts, not importance)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams features are typically sparse (n-grams count occurrences of words and not every word will be found in every document)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams can help utilize local context around each word (n-grams encode sequences of words)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org121bb24" class="outline-3"&gt;
&lt;h3 id="org121bb24"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org121bb24"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Bag of words usually produces longer vectors than Word2Vec (The number of features with BOW is equal to the number of unique words, Word2Vec limit is set beforehand)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Semantically similar words usually have similar word2vec embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; You do not need bag of words features in a competition if you have word2vec features (both approaches are useful and can work together)
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The meaning of each value in the Bag of Words matrix is unknown (The meaning of each value is how many times it occurred)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org87d4ddc" class="outline-3"&gt;
&lt;h3 id="org87d4ddc"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org87d4ddc"&gt;
&lt;p&gt;
Suppose in a new competition we are given a dataset of 2D medical images. We want to extract image descriptors from a hidden layer of a neural network pretrained on the ImageNet dataset. We will then use extracted descriptors to train a simple logistic regression model to classify images from our dataset.
&lt;/p&gt;

&lt;p&gt;
We are considering using two networks: ResNet-50 with an ImageNet accuracy of X and VGG-16 with an ImageNet accuracy of Y (X &amp;lt; Y). Select the true statements.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; With one pretrained CNN model you can get only one vector of descriptors for an image&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet 50 will always be better than the ones from VG-16 in our pipeline&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; It is not clear what descriptors are better on our dataset. We should evaluate both.
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet-50 and VGG-16 are always very similar in cosine distance&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; For any image, descriptors from the last hidden layer of ResNet-50 are the same as the descriptors from the last hidden layer of VGG-16&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org95d17e0" class="outline-3"&gt;
&lt;h3 id="org95d17e0"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org95d17e0"&gt;
&lt;p&gt;
Data augmentation can be used at (1) train time and (2) test time
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; True, False&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, True&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True, True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgeb9a2d8" class="outline-2"&gt;
&lt;h2 id="orgeb9a2d8"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgeb9a2d8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9cb224d" class="outline-3"&gt;
&lt;h3 id="org9cb224d"&gt;Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9cb224d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2c40d49" class="outline-4"&gt;
&lt;h4 id="org2c40d49"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2c40d49"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/feature_extraction.html"&gt;SKlearn on feature extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/"&gt;blog post&lt;/a&gt; on extracting features from text&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf58e6c4" class="outline-4"&gt;
&lt;h4 id="orgf58e6c4"&gt;Word2Vec&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf58e6c4"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rare-technologies.com/word2vec-tutorial/"&gt;Blog post tutorial&lt;/a&gt; by the author of gensim&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"&gt;Text Classification post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://taylorwhitten.github.io/blog/word2vec"&gt;Another introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge34e04c" class="outline-4"&gt;
&lt;h4 id="orge34e04c"&gt;Natural Language Processing with Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge34e04c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/"&gt;nltk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfae8717" class="outline-3"&gt;
&lt;h3 id="orgfae8717"&gt;Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfae8717"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1a02b96" class="outline-4"&gt;
&lt;h4 id="org1a02b96"&gt;Pre-trained Models&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1a02b96"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://keras.io/applications/"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"&gt;How To use a pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga780bd9" class="outline-4"&gt;
&lt;h4 id="orga780bd9"&gt;Fine-Tuning&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga780bd9"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/image_retraining"&gt;Re-train a tensorflow image classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html"&gt;Fine-tuning deep learning models in keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>featureextraction text images notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</guid><pubDate>Mon, 13 Aug 2018 14:17:52 GMT</pubDate></item></channel></rss>