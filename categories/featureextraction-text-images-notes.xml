<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about featureextraction text images notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/featureextraction-text-images-notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 17 Sep 2018 22:30:31 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Feature Extraction From Text and Images</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org8cb0aa1"&gt;How do you convert text to data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org34c0118"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org0e0c001"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#orgce3e50a"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8cb0aa1" class="outline-2"&gt;
&lt;h2 id="org8cb0aa1"&gt;How do you convert text to data?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8cb0aa1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org93086c0" class="outline-3"&gt;
&lt;h3 id="org93086c0"&gt;Two Main Methods&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org93086c0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcc4b03a" class="outline-4"&gt;
&lt;h4 id="orgcc4b03a"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcc4b03a"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org6981de9"&gt;&lt;/a&gt;Vectorization&lt;br&gt;
&lt;div class="outline-text-5" id="text-org6981de9"&gt;
&lt;p&gt;
This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;CountVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgb35f8ca"&gt;&lt;/a&gt;Term Frequency/Inverse Document Frequency (TF/IDF)&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgb35f8ca"&gt;
&lt;p&gt;
This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency: Normalize columns to make emphasize more important features&lt;/li&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org1287ef3"&gt;&lt;/a&gt;N-Grams&lt;br&gt;
&lt;div class="outline-text-5" id="text-org1287ef3"&gt;
&lt;p&gt;
This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://sklearn.feature_extraction.text.countvectorizer"&gt;Count Vectorizer&lt;/a&gt; is sklearn's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org2a20580"&gt;&lt;/a&gt;Text Preprocessing&lt;br&gt;
&lt;div class="outline-text-5" id="text-org2a20580"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;lowercase:
Change all the words to lower-case&lt;/li&gt;
&lt;li&gt;lemmatization:
Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)&lt;/li&gt;
&lt;li&gt;stemming:
Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)&lt;/li&gt;
&lt;li&gt;stopwords:
Remove common words (e.g. a, and, or, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org70714c9"&gt;&lt;/a&gt;The Bag Of Words Pipeline&lt;br&gt;
&lt;div class="outline-text-5" id="text-org70714c9"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Preprocessing (lowercase, lemmatization, stemming, stopwords)&lt;/li&gt;
&lt;li&gt;Create n-grams&lt;/li&gt;
&lt;li&gt;Postprocessing: TF/IDF&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab7c48e" class="outline-4"&gt;
&lt;h4 id="orgab7c48e"&gt;Embeddings (e.g. Word2Vec)&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgab7c48e"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses neural-nets&lt;/li&gt;
&lt;li&gt;much smaller vectors than bag of words&lt;/li&gt;
&lt;li&gt;But each word gets a vector so many more vectors&lt;/li&gt;
&lt;li&gt;similar words have similar word-vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org34c0118" class="outline-2"&gt;
&lt;h2 id="org34c0118"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org34c0118"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org813029b" class="outline-3"&gt;
&lt;h3 id="org813029b"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org813029b"&gt;
&lt;p&gt;
TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. Choose the correct statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; IDF scales features inversely proprotionally to a number of word occurrences over documents&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; IDF scales features proportionally to the frequency of the a word's occurrences&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; TF normalizes sums of the row values to 1&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; TF normalizes sums of the column values to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb4c45c" class="outline-3"&gt;
&lt;h3 id="orgdb4c45c"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb4c45c"&gt;
&lt;p&gt;
Which of these methods can be used to preprocess text?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; stemming&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lower-case transformation&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lemmatization&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Stopword removal&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumping&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumbing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0899ed" class="outline-3"&gt;
&lt;h3 id="orga0899ed"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga0899ed"&gt;
&lt;p&gt;
What is the main purpose of lemmatization and stemming?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to remove words which are not useful&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; to remove inflectional forms and sometimes derivationally related forms of a word to a common base form&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; To reduce the significance of common words&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to induce common word amplification standards to the most useful for machine learning algorithms form&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org021da8c" class="outline-3"&gt;
&lt;h3 id="org021da8c"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org021da8c"&gt;
&lt;p&gt;
To learn Word2Vec embeddings we need:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; GloVe embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Labels for the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Labels for each word in the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e0c001" class="outline-2"&gt;
&lt;h2 id="org0e0c001"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0e0c001"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc214beb" class="outline-3"&gt;
&lt;h3 id="orgc214beb"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc214beb"&gt;
&lt;p&gt;
Select true statements about n-grams.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining should always be applied before computing n-grams (there is no such thing as Levenshteining)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; n-grams always help increase significance of important words (n-grams are about counts, not importance)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams features are typically sparse (n-grams count occurrences of words and not every word will be found in every document)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams can help utilize local context around each word (n-grams encode sequences of words)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7a24038" class="outline-3"&gt;
&lt;h3 id="org7a24038"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7a24038"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Bag of words usually produces longer vectors than Word2Vec (The number of features with BOW is equal to the number of unique words, Word2Vec limit is set beforehand)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Semantically similar words usually have similar word2vec embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; You do not need bag of words features in a competition if you have word2vec features (both approaches are useful and can work together)
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The meaning of each value in the Bag of Words matrix is unknown (The meaning of each value is how many times it occurred)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf58bb26" class="outline-3"&gt;
&lt;h3 id="orgf58bb26"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf58bb26"&gt;
&lt;p&gt;
Suppose in a new competition we are given a dataset of 2D medical images. We want to extract image descriptors from a hidden layer of a neural network pretrained on the ImageNet dataset. We will then use extracted descriptors to train a simple logistic regression model to classify images from our dataset.
&lt;/p&gt;

&lt;p&gt;
We are considering using two networks: ResNet-50 with an ImageNet accuracy of X and VGG-16 with an ImageNet accuracy of Y (X &amp;lt; Y). Select the true statements.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; With one pretrained CNN model you can get only one vector of descriptors for an image&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet 50 will always be better than the ones from VG-16 in our pipeline&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; It is not clear what descriptors are better on our dataset. We should evaluate both.
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet-50 and VGG-16 are always very similar in cosine distance&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; For any image, descriptors from the last hidden layer of ResNet-50 are the same as the descriptors from the last hidden layer of VGG-16&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org18afcb8" class="outline-3"&gt;
&lt;h3 id="org18afcb8"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org18afcb8"&gt;
&lt;p&gt;
Data augmentation can be used at (1) train time and (2) test time
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; True, False&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, True&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True, True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgce3e50a" class="outline-2"&gt;
&lt;h2 id="orgce3e50a"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgce3e50a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9232cc7" class="outline-3"&gt;
&lt;h3 id="org9232cc7"&gt;Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9232cc7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge18959d" class="outline-4"&gt;
&lt;h4 id="orge18959d"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge18959d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/feature_extraction.html"&gt;SKlearn on feature extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/"&gt;blog post&lt;/a&gt; on extracting features from text&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68c4857" class="outline-4"&gt;
&lt;h4 id="org68c4857"&gt;Word2Vec&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org68c4857"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rare-technologies.com/word2vec-tutorial/"&gt;Blog post tutorial&lt;/a&gt; by the author of gensim&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"&gt;Text Classification post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://taylorwhitten.github.io/blog/word2vec"&gt;Another introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgae7867c" class="outline-4"&gt;
&lt;h4 id="orgae7867c"&gt;Natural Language Processing with Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgae7867c"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/"&gt;nltk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfdeada8" class="outline-3"&gt;
&lt;h3 id="orgfdeada8"&gt;Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfdeada8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org855838a" class="outline-4"&gt;
&lt;h4 id="org855838a"&gt;Pre-trained Models&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org855838a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://keras.io/applications/"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"&gt;How To use a pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org55e6776" class="outline-4"&gt;
&lt;h4 id="org55e6776"&gt;Fine-Tuning&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org55e6776"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/image_retraining"&gt;Re-train a tensorflow image classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html"&gt;Fine-tuning deep learning models in keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>featureextraction text images notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</guid><pubDate>Mon, 13 Aug 2018 14:17:52 GMT</pubDate></item></channel></rss>