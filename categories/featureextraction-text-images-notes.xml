<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about featureextraction text images notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/featureextraction-text-images-notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 15 Sep 2018 17:42:27 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Feature Extraction From Text and Images</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org9c16155"&gt;How do you convert text to data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org5dffe75"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org4442b30"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/#org91a564a"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9c16155" class="outline-2"&gt;
&lt;h2 id="org9c16155"&gt;How do you convert text to data?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9c16155"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1e94a7a" class="outline-3"&gt;
&lt;h3 id="org1e94a7a"&gt;Two Main Methods&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1e94a7a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgda76e36" class="outline-4"&gt;
&lt;h4 id="orgda76e36"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgda76e36"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org9a9fdf5"&gt;&lt;/a&gt;Vectorization&lt;br&gt;
&lt;div class="outline-text-5" id="text-org9a9fdf5"&gt;
&lt;p&gt;
This method counts the number of occurrences of each word in the source. For each word it creates a column and then in the row puts the counts for that instance of data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;CountVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org7321050"&gt;&lt;/a&gt;Term Frequency/Inverse Document Frequency (TF/IDF)&lt;br&gt;
&lt;div class="outline-text-5" id="text-org7321050"&gt;
&lt;p&gt;
This method tries to make word counts comparable even if the texts are of different sizes and also to emphasize more important words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Term Frequency: Normalize rows so all values are from 0 to 1 to make texts of different sizes comparable&lt;/li&gt;
&lt;li&gt;Inverse Document Frequency: Normalize columns to make emphasize more important features&lt;/li&gt;
&lt;li&gt;Sklearn implements this with &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidVectorizer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orga6e7cf6"&gt;&lt;/a&gt;N-Grams&lt;br&gt;
&lt;div class="outline-text-5" id="text-orga6e7cf6"&gt;
&lt;p&gt;
This method that creates a bag of words by grouping them into sub-sequences of words. A 3-gram, for instance, sweeps the text to create sequences of words made up of 3 adjacent words.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://sklearn.feature_extraction.text.countvectorizer"&gt;Count Vectorizer&lt;/a&gt; is sklearn's implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org8964ede"&gt;&lt;/a&gt;Text Preprocessing&lt;br&gt;
&lt;div class="outline-text-5" id="text-org8964ede"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;lowercase:
Change all the words to lower-case&lt;/li&gt;
&lt;li&gt;lemmatization:
Try to reduce word to a common case (e.g. democratization, democracy, democratic all become democracy)&lt;/li&gt;
&lt;li&gt;stemming:
Try to reduce word to a root (e.g. democratization, democracy, democratic all become democ)&lt;/li&gt;
&lt;li&gt;stopwords:
Remove common words (e.g. a, and, or, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgf4056ec"&gt;&lt;/a&gt;The Bag Of Words Pipeline&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgf4056ec"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Preprocessing (lowercase, lemmatization, stemming, stopwords)&lt;/li&gt;
&lt;li&gt;Create n-grams&lt;/li&gt;
&lt;li&gt;Postprocessing: TF/IDF&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org31719ec" class="outline-4"&gt;
&lt;h4 id="org31719ec"&gt;Embeddings (e.g. Word2Vec)&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org31719ec"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Uses neural-nets&lt;/li&gt;
&lt;li&gt;much smaller vectors than bag of words&lt;/li&gt;
&lt;li&gt;But each word gets a vector so many more vectors&lt;/li&gt;
&lt;li&gt;similar words have similar word-vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5dffe75" class="outline-2"&gt;
&lt;h2 id="org5dffe75"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5dffe75"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1df0b48" class="outline-3"&gt;
&lt;h3 id="org1df0b48"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1df0b48"&gt;
&lt;p&gt;
TF-IDF is applied to a matrix where each column represents a word, each row represents a document, and each value shows the number of times a particular word occurred in a particular document. Choose the correct statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; IDF scales features inversely proprotionally to a number of word occurrences over documents&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; IDF scales features proportionally to the frequency of the a word's occurrences&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; TF normalizes sums of the row values to 1&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; TF normalizes sums of the column values to 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org08ecc1c" class="outline-3"&gt;
&lt;h3 id="org08ecc1c"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org08ecc1c"&gt;
&lt;p&gt;
Which of these methods can be used to preprocess text?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; stemming&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lower-case transformation&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Lemmatization&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Stopword removal&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumping&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; plumbing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3c0d557" class="outline-3"&gt;
&lt;h3 id="org3c0d557"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3c0d557"&gt;
&lt;p&gt;
What is the main purpose of lemmatization and stemming?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to remove words which are not useful&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; to remove inflectional forms and sometimes derivationally related forms of a word to a common base form&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; To reduce the significance of common words&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; to induce common word amplification standards to the most useful for machine learning algorithms form&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbfb5d8c" class="outline-3"&gt;
&lt;h3 id="orgbfb5d8c"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbfb5d8c"&gt;
&lt;p&gt;
To learn Word2Vec embeddings we need:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; GloVe embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Labels for the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Labels for each word in the documents in the corpora&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4442b30" class="outline-2"&gt;
&lt;h2 id="org4442b30"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4442b30"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4aec9c8" class="outline-3"&gt;
&lt;h3 id="org4aec9c8"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4aec9c8"&gt;
&lt;p&gt;
Select true statements about n-grams.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Levenshteining should always be applied before computing n-grams (there is no such thing as Levenshteining)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; n-grams always help increase significance of important words (n-grams are about counts, not importance)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams features are typically sparse (n-grams count occurrences of words and not every word will be found in every document)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; n-grams can help utilize local context around each word (n-grams encode sequences of words)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org54643b5" class="outline-3"&gt;
&lt;h3 id="org54643b5"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org54643b5"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Bag of words usually produces longer vectors than Word2Vec (The number of features with BOW is equal to the number of unique words, Word2Vec limit is set beforehand)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Semantically similar words usually have similar word2vec embeddings&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; You do not need bag of words features in a competition if you have word2vec features (both approaches are useful and can work together)
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The meaning of each value in the Bag of Words matrix is unknown (The meaning of each value is how many times it occurred)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org81e0f32" class="outline-3"&gt;
&lt;h3 id="org81e0f32"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org81e0f32"&gt;
&lt;p&gt;
Suppose in a new competition we are given a dataset of 2D medical images. We want to extract image descriptors from a hidden layer of a neural network pretrained on the ImageNet dataset. We will then use extracted descriptors to train a simple logistic regression model to classify images from our dataset.
&lt;/p&gt;

&lt;p&gt;
We are considering using two networks: ResNet-50 with an ImageNet accuracy of X and VGG-16 with an ImageNet accuracy of Y (X &amp;lt; Y). Select the true statements.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; With one pretrained CNN model you can get only one vector of descriptors for an image&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet 50 will always be better than the ones from VG-16 in our pipeline&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; It is not clear what descriptors are better on our dataset. We should evaluate both.
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Descriptors from ResNet-50 and VGG-16 are always very similar in cosine distance&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; For any image, descriptors from the last hidden layer of ResNet-50 are the same as the descriptors from the last hidden layer of VGG-16&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb5fd47f" class="outline-3"&gt;
&lt;h3 id="orgb5fd47f"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb5fd47f"&gt;
&lt;p&gt;
Data augmentation can be used at (1) train time and (2) test time
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; True, False&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, True&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True, True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False, False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org91a564a" class="outline-2"&gt;
&lt;h2 id="org91a564a"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org91a564a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2dea633" class="outline-3"&gt;
&lt;h3 id="org2dea633"&gt;Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2dea633"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org888604f" class="outline-4"&gt;
&lt;h4 id="org888604f"&gt;Bag Of Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org888604f"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/feature_extraction.html"&gt;SKlearn on feature extraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/"&gt;blog post&lt;/a&gt; on extracting features from text&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org47d387e" class="outline-4"&gt;
&lt;h4 id="org47d387e"&gt;Word2Vec&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org47d387e"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec"&gt;TensorFlow tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rare-technologies.com/word2vec-tutorial/"&gt;Blog post tutorial&lt;/a&gt; by the author of gensim&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"&gt;Text Classification post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://taylorwhitten.github.io/blog/word2vec"&gt;Another introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0fdad3" class="outline-4"&gt;
&lt;h4 id="orga0fdad3"&gt;Natural Language Processing with Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga0fdad3"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/"&gt;nltk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://textblob.readthedocs.io/en/dev/"&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab0ffa7" class="outline-3"&gt;
&lt;h3 id="orgab0ffa7"&gt;Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgab0ffa7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd0cb270" class="outline-4"&gt;
&lt;h4 id="orgd0cb270"&gt;Pre-trained Models&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd0cb270"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://keras.io/applications/"&gt;Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"&gt;How To use a pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7540737" class="outline-4"&gt;
&lt;h4 id="org7540737"&gt;Fine-Tuning&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7540737"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/hub/tutorials/image_retraining"&gt;Re-train a tensorflow image classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html"&gt;Fine-tuning deep learning models in keras&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>featureextraction text images notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-extraction-from-text-and-images/</guid><pubDate>Mon, 13 Aug 2018 14:17:52 GMT</pubDate></item></channel></rss>