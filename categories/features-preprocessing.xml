<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about features preprocessing)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/features-preprocessing.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 11 Aug 2018 20:58:29 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Feature Preprocessing</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org5b56716"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org8d57f03"&gt;Numeric Feature Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org059e47a"&gt;Categorical And Ordinal Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgd99b104"&gt;Dates and Times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgac5d138"&gt;Coordinates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#org0e1ccc3"&gt;Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/#orgb3a7c44"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b56716" class="outline-2"&gt;
&lt;h2 id="org5b56716"&gt;Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5b56716"&gt;
&lt;p&gt;
In every competition you need to:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;pre-process the data&lt;/li&gt;
&lt;li&gt;generate new features from the existing ones&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8d57f03" class="outline-2"&gt;
&lt;h2 id="org8d57f03"&gt;Numeric Feature Preprocessing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8d57f03"&gt;
&lt;p&gt;
Some models (e.g. Linear Classification) need you to convert numeric-looking data to categorical data. Tree-based models don't need pre-processing as much as non tree-based models do.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8bd57f0" class="outline-3"&gt;
&lt;h3 id="org8bd57f0"&gt;Scaling&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8bd57f0"&gt;
&lt;p&gt;
If features have different value ranges then the model will  treat them differently.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn.preprocessing.MinMaxScalar
Scale by the range of values&lt;/li&gt;
&lt;li&gt;sklearn.preprocessing.StandardScalar
Scale the data to have a mean of 0 and a standard deviation of 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org423c307" class="outline-3"&gt;
&lt;h3 id="org423c307"&gt;Outliers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org423c307"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Clip (&lt;code&gt;numpy.clip&lt;/code&gt;)values to get rid of unusual values that mess with the model. (Winsorization)&lt;/li&gt;
&lt;li&gt;Rank Transform them (&lt;code&gt;scipy.stats.rankdata&lt;/code&gt;) so that they&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org550db9e" class="outline-3"&gt;
&lt;h3 id="org550db9e"&gt;Transformation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org550db9e"&gt;
&lt;p&gt;
For linear models and neural networks, transforming the data can sometimes help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Log Transform: &lt;code&gt;numpy.log(1 + x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Power Less Than 1 Transform: =numpy.sqrt(x + 2/3)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org72b3eca" class="outline-3"&gt;
&lt;h3 id="org72b3eca"&gt;Feature Generation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org72b3eca"&gt;
&lt;p&gt;
Sometimes you can convert separate columns to get new ones. This requires exploratory data analysis.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Fractional Parts: people perceive numbers with fractions differently so sometime separating out the fractional part makes the model perform better because it is the more important part&lt;/li&gt;
&lt;li&gt;You can derive new values mathematically (e.g. distance using height and width)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org059e47a" class="outline-2"&gt;
&lt;h2 id="org059e47a"&gt;Categorical And Ordinal Features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org059e47a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Ordinal Features have an ordering but even if they are labeled numerically (e.g. 1, 2, 3) they aren't numeric because there is no implication about the distance between each&lt;/li&gt;
&lt;li&gt;label and frequency encoding are commonly used for tree-based models&lt;/li&gt;
&lt;li&gt;one-hot encoding is more common for non-tree-based models&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3b68a2b" class="outline-3"&gt;
&lt;h3 id="org3b68a2b"&gt;Label Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3b68a2b"&gt;
&lt;p&gt;
Some models need numeric values or numeric values that don't have implied ordering so you want to encode them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;sklearn.preprocessing.LabelEncoder&lt;/code&gt;
Sorts the labels then encodes them as integers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Pandas.factorize&lt;/code&gt;
Encodes the labels in the order they appear. This makes more sense if there is a meaning to the order in which labels appear.&lt;/li&gt;
&lt;li&gt;This is more commonly used with tree-based methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd014be3" class="outline-3"&gt;
&lt;h3 id="orgd014be3"&gt;Frequency Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd014be3"&gt;
&lt;p&gt;
Encode the values to a fraction of all the labels. Can work with linear models if the frequency is correlated with the target value.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;titanic.groupby("Embarked").size()/len(titanic)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from scipy.stats import rankedata&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;More common with tree-based methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd89b1e" class="outline-3"&gt;
&lt;h3 id="orgdd89b1e"&gt;One Hot Encoding&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdd89b1e"&gt;
&lt;p&gt;
Creates one column for each label and puts a 1 in the column that matches. Works with both linear and tree-based models, but can be inefficient with tree-based models.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pandas.get_dummies&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;One hot encoding is better for non-tree models.&lt;/li&gt;
&lt;li&gt;This allows easier feature interactions (encode combined features (e.g. gender and class) rather than encoding them separately) 
&lt;ul class="org-ul"&gt;
&lt;li&gt;This is more common with linear models and KNN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd99b104" class="outline-2"&gt;
&lt;h2 id="orgd99b104"&gt;Dates and Times&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd99b104"&gt;
&lt;p&gt;
When working with seasonal data, sometimes the relative date-stamps are more important than the actual dates (how close to Christmas?).
Sometimes you want the time between events.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgac5d138" class="outline-2"&gt;
&lt;h2 id="orgac5d138"&gt;Coordinates&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgac5d138"&gt;
&lt;p&gt;
Sometimes you want exact coordinates, but most times you want distances to some center.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e1ccc3" class="outline-2"&gt;
&lt;h2 id="org0e1ccc3"&gt;Missing Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0e1ccc3"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;"missing" data might mean outliers - values that are probably wrong&lt;/li&gt;
&lt;li&gt;avoid replacing missing values before feature engineering - it can throw off what you do&lt;/li&gt;
&lt;li&gt;Gradient Boost Trees can handle isNaN, so you don't have to do anything&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgedf1a23" class="outline-3"&gt;
&lt;h3 id="orgedf1a23"&gt;Numeric&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgedf1a23"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org42e038e" class="outline-4"&gt;
&lt;h4 id="org42e038e"&gt;Fill NA Approaches&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org42e038e"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;-999, -1, other numbers
&lt;ul class="org-ul"&gt;
&lt;li&gt;lets you categorize missing values&lt;/li&gt;
&lt;li&gt;throws some models off (e.g. linear models and neural networks)&lt;/li&gt;
&lt;li&gt;one solution is to create a new feature for missing values, but this has now increased the amount of data you need (curse of dimensionality)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;mean, median, some central tendency
&lt;ul class="org-ul"&gt;
&lt;li&gt;This can throw the model off&lt;/li&gt;
&lt;li&gt;it is sometimes better to ignore missing data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;recronstructed valud&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb3a7c44" class="outline-2"&gt;
&lt;h2 id="orgb3a7c44"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb3a7c44"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc40f607" class="outline-3"&gt;
&lt;h3 id="orgc40f607"&gt;Feature Pre-processing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc40f607"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html"&gt;SKlearn's Preprocessing Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling"&gt;Andrew Ng on Feature Scaling and its effect on Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"&gt;Sebastian Raschka on Feature Scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9817e48" class="outline-3"&gt;
&lt;h3 id="org9817e48"&gt;Feature Engineering&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9817e48"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/"&gt;Machine Learning Mastery on Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering"&gt;Quora: What are some best practices in Feature Engineering?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>features preprocessing</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/feature-preprocessing/</guid><pubDate>Wed, 08 Aug 2018 04:41:10 GMT</pubDate></item></channel></rss>