<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle (Posts about basics algorithms notes)</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description></description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/categories/basics-algorithms-notes.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 15 Sep 2018 17:42:26 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Machine Learning Recap</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org3fe09a4" class="outline-2"&gt;
&lt;h2 id="org3fe09a4"&gt;The Main Categories&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3fe09a4"&gt;
&lt;p&gt;
These are the four main categories of supervised machine learning algorithms that you'll encounter in kaggle competitions.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Linear Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;vowpal rabbit (for really large datasets)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tree-Based Models
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;li&gt;xgboost: faster than sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;k-Nearest Neighbors
&lt;ul class="org-ul"&gt;
&lt;li&gt;sklearn&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48f724a" class="outline-2"&gt;
&lt;h2 id="org48f724a"&gt;The No Free Lunch Theorem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org48f724a"&gt;
&lt;blockquote&gt;
&lt;p&gt;
There is no method which outperforms all others for all tasks.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
You cannot assume that an algorithm that did well on one set of data will do well on another. All algorithms have weaknesses, so you have to test multiple algorithms on each data set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org60a669b" class="outline-2"&gt;
&lt;h2 id="org60a669b"&gt;Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org60a669b"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;there is no one algorithm to rule them all&lt;/li&gt;
&lt;li&gt;Linear models split spaces into two sub-spaces&lt;/li&gt;
&lt;li&gt;tree-based models spit spaces into boxes&lt;/li&gt;
&lt;li&gt;kNN relies on measuring the 'closeness' between points&lt;/li&gt;
&lt;li&gt;Neural Networks provide non-linear decision boundaries&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In general the two most powerful methods are &lt;b&gt;Gradient Boosted Decision Trees&lt;/b&gt; and &lt;b&gt;Neural Networks&lt;/b&gt;, but this won't always be the case.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>basics algorithms notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/machine-learning-recap/</guid><pubDate>Sun, 05 Aug 2018 01:13:44 GMT</pubDate></item></channel></rss>