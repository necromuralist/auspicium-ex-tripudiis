<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on Kaggle</title><link>https://necromuralist.github.io/Kaggle-Competitions/</link><description>Notes on studying kaggle.</description><atom:link href="https://necromuralist.github.io/Kaggle-Competitions/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 20 Sep 2018 04:17:50 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Metrics Optimization 2</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics-optimization-2/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-org9a9e527" class="outline-2"&gt;
&lt;h2 id="org9a9e527"&gt;(R)MSPE, MAPE, and (R)MSLE&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9a9e527"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2b0a770" class="outline-3"&gt;
&lt;h3 id="org2b0a770"&gt;An Off-By-One Example&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2b0a770"&gt;
&lt;p&gt;
Suppose we are predicting sales for two shops and the two shops have different sales volumes but our predictions for both cases are off by one. In this case our Mean-Squared-Error (MSE) might be the same, but they have a different significance.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Shop&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Actual&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Predicted&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;MSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;9&lt;/td&gt;
&lt;td class="org-left"&gt;10&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;999&lt;/td&gt;
&lt;td class="org-left"&gt;1,000&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7d129ef" class="outline-2"&gt;
&lt;h2 id="org7d129ef"&gt;Root Mean Squared Percentage Error and Mean Absolute Percentage Error&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7d129ef"&gt;
&lt;p&gt;
The MSE and Mean-Absolute-Error (MAE) are absolute errors which don't take into account how significant the error is. There are two relative errors,  Mean-Squared-Percentage-Error (MSPE) and Mean-Absolute-Percentage-Error (MAPE) that divide each error term by the actual value to give you a realive error instead of an absolute error.
&lt;/p&gt;

&lt;p&gt;
\[
MSPE = \frac{1}{N} \sum_{i=1}^n \left( \frac{y_i - \hat{y}}{y_i}\right)^2
\]
&lt;/p&gt;

&lt;p&gt;
\[
MAPE = \frac{1}{N} \sum_{i=1}^n \left| \frac{y_i - \hat{y}}{y_i}\right|
\]
&lt;/p&gt;

&lt;p&gt;
The MAPE will be inversely proportional to its target and the MSPE will be inversely proportional to the square of the target.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33dedb3" class="outline-3"&gt;
&lt;h3 id="org33dedb3"&gt;Optimal Constant Predictions&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org33dedb3"&gt;
&lt;p&gt;
The best constant prediction you can make when using the Mean Squared Error is to predict the mean of the target values. The best prediction you can make for the MSPE is to take a weighted mean of the target values. The best constant prediction you can make for the Mean Absolute Percentage Error is the weighted median.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org72d9283" class="outline-2"&gt;
&lt;h2 id="org72d9283"&gt;Root Mean Squared Logarithmic Error (MSLE)&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org72d9283"&gt;
&lt;p&gt;
\[
MSLE = \sqrt{\frac{1}{N}\sum_{i=1}^N (\log (y_i + 1) - \log(\hat{y}_i + 1))^2}\\
= \sqrt{MSE(\log(y_i + 1), \log(\hat{y}_i + 1))}
\]
&lt;/p&gt;

&lt;p&gt;
You add a 1 to each term to prevent you from trying to take the &lt;i&gt;log&lt;/i&gt; of 0, which is undefined. The RMSLE is biased toward predictions that are higher than the actual values rather than lower.
&lt;/p&gt;

&lt;p&gt;
These are the best constant predictions you can make for the competition data set.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Constant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;MSE&lt;/td&gt;
&lt;td class="org-right"&gt;11&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;RMSLE&lt;/td&gt;
&lt;td class="org-right"&gt;9.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MAE&lt;/td&gt;
&lt;td class="org-right"&gt;8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MSPE&lt;/td&gt;
&lt;td class="org-right"&gt;6.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;MAPE&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes metrics</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics-optimization-2/</guid><pubDate>Wed, 19 Sep 2018 15:04:33 GMT</pubDate></item><item><title>Metrics</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/metrics/#orgd5014ac"&gt;About Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/metrics/#org4c7b753"&gt;The Most Common Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd5014ac" class="outline-2"&gt;
&lt;h2 id="orgd5014ac"&gt;About Metrics&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd5014ac"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2b7b99d" class="outline-3"&gt;
&lt;h3 id="org2b7b99d"&gt;What are metrics?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2b7b99d"&gt;
&lt;p&gt;
Metrics are numeric values that you are trying to optimize - they are how your model is graded.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8688e79" class="outline-3"&gt;
&lt;h3 id="org8688e79"&gt;Why are there so many metrics?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8688e79"&gt;
&lt;p&gt;
Each metric can tell you something different, so what metric you need depends on the problem you are trying to solve. For competitions, it is sometimes possible to do metrics probing just like you can sometimes do leaderboard probing to find peculiarities created by the chosen metric.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4c7b753" class="outline-2"&gt;
&lt;h2 id="org4c7b753"&gt;The Most Common Metrics&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4c7b753"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb984a1" class="outline-3"&gt;
&lt;h3 id="orgdb984a1"&gt;Regression&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb984a1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc8c1fd7" class="outline-4"&gt;
&lt;h4 id="orgc8c1fd7"&gt;Mean Squared Error&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc8c1fd7"&gt;
&lt;p&gt;
This is the average of the square of the errors.
\[
MSE = \frac{1}{N}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdad37bb" class="outline-4"&gt;
&lt;h4 id="orgdad37bb"&gt;Root Mean Squared Error&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdad37bb"&gt;
&lt;p&gt;
This is the square root of the mean squared error.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;minimizing the RMSE is also minimizes the MSE&lt;/li&gt;
&lt;li&gt;RMSE is more intuitive&lt;/li&gt;
&lt;li&gt;They can differ when used by gradient-based models&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2901caa" class="outline-4"&gt;
&lt;h4 id="org2901caa"&gt;R-squared&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2901caa"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Optimizing r-squared is equivalent to optimizing MSE&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3fed0d1" class="outline-4"&gt;
&lt;h4 id="org3fed0d1"&gt;Mean Absolute Error&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3fed0d1"&gt;
&lt;p&gt;
This is more common when you are explaining it to a non-statistician. It is also more robust. Mathematically, it isn't something that you can calculate the derivate for.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3e12f33" class="outline-4"&gt;
&lt;h4 id="org3e12f33"&gt;MAE vs MSE&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3e12f33"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Outliers: use MAE&lt;/li&gt;
&lt;li&gt;Unexpected Values that we should still care about (not true outliers (mistakes), just rare): use MSE&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8bb886e" class="outline-3"&gt;
&lt;h3 id="org8bb886e"&gt;Classification&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>metrics notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/metrics/</guid><pubDate>Tue, 18 Sep 2018 04:49:35 GMT</pubDate></item><item><title>Data Leakages</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org44df1ae"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org8df2c16"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#orgd38192b"&gt;Helpers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org360df0c"&gt;Load the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org73c6492"&gt;EDA and Leakage Intuition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org9108cf7"&gt;Building a magic feature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org5bfd205"&gt;Bonus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/#org6429bae"&gt;What does it all mean then?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org44df1ae" class="outline-2"&gt;
&lt;h2 id="org44df1ae"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org44df1ae"&gt;
&lt;p&gt;
In this programming assignment we will illustrate a very severe data leakage, of the sort that can often be found in competitions. The task is to score the pairs of objects, i.e. predict &lt;i&gt;1&lt;/i&gt; if two objects belong to the same class and &lt;i&gt;0&lt;/i&gt; otherwise. 
&lt;/p&gt;

&lt;p&gt;
The data in this assignment is taken from a real competition, and  &lt;b&gt;we will not use the training set at all&lt;/b&gt; and still achieve an accuracy score of almost 100% - just by exploiting the leakage.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8df2c16" class="outline-2"&gt;
&lt;h2 id="org8df2c16"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8df2c16"&gt;
&lt;p&gt;
During the importing of pandas (or scipy) you get a warning about a potential binary incompatibility. According to &lt;a href="https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility"&gt;Stack Overflow&lt;/a&gt; you can safely ignore this, so we'll use &lt;a href="https://docs.python.org/3/library/warnings.html"&gt;warnings&lt;/a&gt; to suppress the messages, just so it doesn't keep bringing them up everytime I run this notebook.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;warnings&lt;/span&gt;
&lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filterwarnings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"ignore"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"numpy.dtype size changed"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;warnings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filterwarnings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"ignore"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"numpy.ufunc size changed"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# python standard library&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="c1"&gt;# from pypi&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tabulate&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tabulate&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pyplot&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.sparse&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_style&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;FIGURE_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd38192b" class="outline-2"&gt;
&lt;h2 id="orgd38192b"&gt;Helpers&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd38192b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3bb9e2a" class="outline-3"&gt;
&lt;h3 id="org3bb9e2a"&gt;Paths&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3bb9e2a"&gt;
&lt;p&gt;
Since I'm doing this as posts in nikola, but I'm trying to keep all non-post files outside of the &lt;code&gt;posts&lt;/code&gt; folder, I'm going to use a class to keep the paths to the output (submission) files straight.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Helper to put submission files in the right folder"""&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_test_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="sd"&gt;"""The path to the data set"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"../data/"&lt;/span&gt;
	    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mkdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;submissions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="sd"&gt;"""Path to the submissions"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"submissions/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
		&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mkdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_submissions&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="sd"&gt;"""path to the test-set data"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_test_set&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_test_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"test_pairs.csv"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_test_set&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="sd"&gt;"""Add the filename to the path&lt;/span&gt;

&lt;span class="sd"&gt;	Args:&lt;/span&gt;
&lt;span class="sd"&gt;	 filename (str): name to add to the submissions folder&lt;/span&gt;

&lt;span class="sd"&gt;	Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	 str: path to the file in the submissions folder&lt;/span&gt;
&lt;span class="sd"&gt;	"""&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submissions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgba945b7" class="outline-3"&gt;
&lt;h3 id="orgba945b7"&gt;Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgba945b7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TestSet&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Loads the test-set data&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     paths: object with the path to the test-set&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Paths&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="sd"&gt;"""the test-set data&lt;/span&gt;

&lt;span class="sd"&gt;	Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	 `pandas.DataFrame`: the test-set data&lt;/span&gt;
&lt;span class="sd"&gt;	"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org360df0c" class="outline-2"&gt;
&lt;h2 id="org360df0c"&gt;Load the data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org360df0c"&gt;
&lt;p&gt;
Let's load the test data. Note that we don't have any training data here, just test data. Moreover, &lt;i&gt;we will not use any features of the test set&lt;/i&gt;. All we need to solve this task is the file with the indices for the pairs that we need to compare.
&lt;/p&gt;

&lt;p&gt;
Let's load the data with the test indices.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TestSet&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   pairId  FirstId  SecondId
0       0     1427      8053
1       1    17044      7681
2       2    19237     20966
3       3     8005     20765
4       4    16837       599
5       5     3657     12504
6       6     2836      7582
7       7     6136      6111
8       8    23295      9817
9       9     6621      7672
&lt;/pre&gt;


&lt;p&gt;
We don't know what the data represents in this case, but you can give them an arbitrary meaning. You could, for example, think that there is a test dataset of images, and each image is assigned a unique `ID` from \(0\) to \(N-1\) (N – is the number of images). In the dataframe above &lt;code&gt;FirstId&lt;/code&gt; and &lt;code&gt;SecondId&lt;/code&gt; point to these IDs and define pairs that we should compare: e.g. &lt;i&gt;Do both images in the pair belong to the same class or not?&lt;/i&gt; So, for example for the first row: if images with `ID=1427` and `ID=8053` belong to the same class, we should predict \(1\) and \(0\) if they don't. 
&lt;/p&gt;

&lt;p&gt;
But in our case we don't really care about the images, and how exactly we compare the images (as long as the output is binary).  
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;We suggest you to try to solve the puzzle yourself first.&lt;/b&gt;&lt;/b&gt; You need to submit a `.csv` file with columns `pairId` and `Prediction` to the grader. The number of submissions allowed is made pretty huge to let you explore the data without worries. The returned score should be very close to \(1\).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FIGURE_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First ID vs Second ID"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First ID"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Second ID"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'.'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/first_vs_second.png" alt="first_vs_second.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So this doesn't appear to be a randomized data set. The first half of the Second IDs seem to be completely paired with the entire set of first IDs, while the second half of the second IDs creates some kind of strange diagonal pattern, except for the highest Second IDs which are once again completely matched with the First IDs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org73c6492" class="outline-2"&gt;
&lt;h2 id="org73c6492"&gt;EDA and Leakage Intuition&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org73c6492"&gt;
&lt;p&gt;
As we already know, the key to discovering data leakages is careful Exploratory Data Analysis (EDA). So let's start our work with some basic data exploration and build an intuition about the leakage.
&lt;/p&gt;

&lt;p&gt;
First, check, how many different &lt;i&gt;id&lt;/i&gt;'s are there: concatenate &lt;i&gt;FirstId&lt;/i&gt; and &lt;i&gt;SecondId&lt;/i&gt; and print the number of unique elements. Also print the minimum and maximum value for that vector.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;','&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0      1427,8053
1     17044,7681
2    19237,20966
3     8005,20765
4      16837,599
dtype: object

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Unique Pairs| {}|"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;())))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Total Pairs| {}|"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Lowest Valued Pair (ASCII)| ({})|"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"|Highest Valued Pair| ({})|"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Unique Pairs&lt;/td&gt;
&lt;td class="org-right"&gt;368538&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Total Pairs&lt;/td&gt;
&lt;td class="org-right"&gt;368550&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Lowest Valued Pair (ASCII)&lt;/td&gt;
&lt;td class="org-right"&gt;(0,10552)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Highest Valued Pair&lt;/td&gt;
&lt;td class="org-right"&gt;(9999,8996)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;
and then print how many pairs we need to classify (it is basically the number of rows in the test set)
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smashed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
368550
12

&lt;/pre&gt;


&lt;p&gt;
Now print, how many distinct pairs it would be possible to create out of all "images" in the dataset?   
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;catted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;image_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;catted&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Unique image IDs: {:,}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image_count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Handshakes: {:,}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;image_count&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image_count&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Unique image IDs: 26,325
Handshakes: 346,489,650

&lt;/pre&gt;

&lt;p&gt;
So the number of pairs we are given to classify is very, very small compared to the total number of possible pairs. 
&lt;/p&gt;

&lt;p&gt;
To exploit the leak we need to assume (or prove), that the total number of ID-pairs classified as 1 is small compared to the total number of pairs possible. For example, think about an image dataset with \(1000\) classes, \(N\) images per class. Then if the task was to tell whether a pair of images belongs to the same class or not, we would have \(1000\frac{N(N-1)}{2}\) positive pairs, while the total number of pairs was \(\frac{1000N(1000N - 1)}{2}\).
&lt;/p&gt;

&lt;p&gt;
Another example - in a &lt;a href="https://www.kaggle.com/c/quora-question-pairs"&gt;Quora competitition&lt;/a&gt; the task was to classify whether a pair of questions are duplicates of each other or not. Of course, the total number of question-pairs is huge, while the number of duplicates (positive pairs) is much, much smaller.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org045f69c" class="outline-3"&gt;
&lt;h3 id="org045f69c"&gt;Probing the Leaderboard&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org045f69c"&gt;
&lt;p&gt;
Finally, let's see what fraction of the ID-pairs have a class of `1`. To do this we just need to submit a constant prediction (all ones) and check the accuracy the grader reports. Create a dataframe with columns `pairId` and `Prediction`, fill it and export it to a `.csv` file. Then submit to the Coursera grader and examine the grader's output to get the fraction of 1s in the test-set.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org56f6462" class="outline-4"&gt;
&lt;h4 id="org56f6462"&gt;All Ones&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org56f6462"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Paths&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;all_ones&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"pairId"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;all_ones&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Prediction"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_ones&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_ones&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;all_ones&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"submission_ones.csv"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   pairId  Prediction
0       0           1
1       1           1
2       2           1
3       3           1
4       4           1

&lt;/pre&gt;

&lt;p&gt;
The submission output was:
&lt;/p&gt;

&lt;pre class="example"&gt;
Your accuracy score is 0.500000. It seems too low, try one more time.
&lt;/pre&gt;

&lt;p&gt;
So, we assumed the that there were many more pairs overall than there were pairs of class 1, but it is not the case for the test set. This means that the test set is constructed not by sampling random pairs, but with a specific sampling algorithm which caused the pairs of class `1` to be oversampled.
&lt;/p&gt;

&lt;p&gt;
Now think - how we can exploit this fact? What is the leak here? If you get it now, you may try to get to the final answer yourself, othewise you can follow the instructions below.   
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgee90c14" class="outline-4"&gt;
&lt;h4 id="orgee90c14"&gt;All Zeros&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgee90c14"&gt;
&lt;p&gt;
Although we're told that this was a binary data set (and you could check it just by printing out the unique values), I sort of flaked and submitted a set where all the pairs were classified as zeros anyway. Here's what happened.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;all_zeros&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;"pairId"&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Prediction"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Prediction&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pairId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;all_zeros&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"submission_zeros.csv"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   pairId  Prediction
0       0         0.0
1       1         0.0
2       2         0.0
3       3         0.0
4       4         0.0

&lt;/pre&gt;


&lt;p&gt;
This is the grader's output.
&lt;/p&gt;

&lt;pre class="example"&gt;
Your accuracy score is 0.500000. It seems too low, try one more time.
&lt;/pre&gt;

&lt;p&gt;
So it appears we've confirmed that the dataset is binary, with half the outputs being ones, the other half being zeros.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9108cf7" class="outline-2"&gt;
&lt;h2 id="org9108cf7"&gt;Building a magic feature&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9108cf7"&gt;
&lt;p&gt;
In this section we will build a magic feature that will solve the problem almost perfectly. The instructions will lead you to the correct solution, but please, try to explain the purpose of the steps we do to yourself – it is very important.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb7808f2" class="outline-3"&gt;
&lt;h3 id="orgb7808f2"&gt;Incidence matrix&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb7808f2"&gt;
&lt;p&gt;
First, we need to build an &lt;a href="https://en.wikipedia.org/wiki/Incidence_matrix"&gt;incidence matrix&lt;/a&gt;. You can think of pairs `(FirstId, SecondId)` as edges in an undirected graph. 
&lt;/p&gt;

&lt;p&gt;
The incidence matrix is a matrix of size `(maxId + 1, maxId + 1)`, where each row (column) `i` corresponds `i-th` `Id`. In this matrix we put the value `1` to the position `[i, j]`, if and only if a pair `(i, j)` or `(j, i)` is present in  a given set of pairs `(FirstId, SecondId)`. All the other elements in the incidence matrix are zeros.   
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Important!&lt;/b&gt;&lt;/b&gt; The incidence matrices are typically very, very sparse (there are a small number of non-zero values). At the same time the incidence matrices are usually huge in terms of the total number of elements and it is &lt;b&gt;&lt;b&gt;impossible to store them in memory in the dense format&lt;/b&gt;&lt;/b&gt;. But due to their sparsity, incidence matrices &lt;b&gt;&lt;b&gt;can be easily represented as sparse matrices&lt;/b&gt;&lt;/b&gt;. If you are not familiar with sparse matrices, please see &lt;a href="https://en.wikipedia.org/wiki/Sparse_matrix"&gt;wikipedia&lt;/a&gt; and &lt;a href="https://docs.scipy.org/doc/scipy/reference/sparse.html"&gt;scipy.sparse reference&lt;/a&gt;. Use any of the `scipy.sparse` constructors to build incidence matrix. 
&lt;/p&gt;

&lt;p&gt;
For example, you can use this constructor: `scipy.sparse.coo_matrix((data, (i, j)))`. We highly recommend you learn to use different `scipy.sparse` constuctors, and matrices types, but if you feel you don't want to use them, you can always build this matrix with a simple `for` loop. You will need to first create a matrix using `scipy.sparse.coo_matrix((M, N), [dtype])` with an appropriate shape `(M, N)` and then iterate through `(FirstId, SecondId)` pairs and fill the corresponding elements in the matrix with ones. 
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Note&lt;/b&gt;&lt;/b&gt;, that the matrix should be symmetric and consist only of zeros and ones. This is something you can use to check your matrix.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org564fb4d" class="outline-4"&gt;
&lt;h4 id="org564fb4d"&gt;De-duplicating the Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org564fb4d"&gt;
&lt;p&gt;
The test data turns out to have duplicate ID pairs, which will cause our incidence matrix to produce numbers greater than 1 if we leave them in, so we need to remove them (using the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.duplicated.html"&gt;duplicated&lt;/a&gt; method).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pairs_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pairs_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pairs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pairs_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pairs_2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;pairs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;duplicated&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;duplicated&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pair_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;pair_count&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;736872&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
736872

&lt;/pre&gt;

&lt;p&gt;
Which is the value provided to test the length of the matrix. Now we need to get the indices.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;i_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;j_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;i_indices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_count&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;j_indices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_count&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we create a sparse matrix where the row indices are our FirstIds and the column indices are our Second Ids and each of their pairs &lt;code&gt;(i, j)&lt;/code&gt; is set to 1.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair_count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;inc_mat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coo_matrix&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_indices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j_indices&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="c1"&gt;# Sanity checks&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inc_mat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;inc_mat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;736872&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It is more convenient to have the incidence matrix in &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"&gt;Compressed Sparse Row (CSR)&lt;/a&gt; format, so convert it here.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;inc_mat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inc_mat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tocsr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3967cf8" class="outline-3"&gt;
&lt;h3 id="org3967cf8"&gt;Now To Build the Magic Feature&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3967cf8"&gt;
&lt;p&gt;
Why did we build the incidence matrix? We can think of the rows in this matrix as a representation for the objects. The `i-th` row is a representation for an object with `Id = i`. Then, to measure the similarity between two objects we can measure similarity between their representations. And we will see that these representations are very good.
&lt;/p&gt;

&lt;p&gt;
Now select the rows from the incidence matrix, that correspond to `test.FirstId`'s, and `test.SecondId`'s.
&lt;/p&gt;

&lt;p&gt;
Note, scipy goes crazy if a matrix is indexed with pandas' series. So do not forget to convert `pd.series` to `np.array`.
These lines should normally run very quickly.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rows_FirstId&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inc_mat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FirstId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;rows_SecondId&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inc_mat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SecondId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Our magic feature will be the &lt;b&gt;dot product&lt;/b&gt; between representations of a pair of objects. Dot product can be regarded as similarity measure – for our non-negative representations the dot product is close to 0 when the representations are different, and is huge, when representations are similar. 
&lt;/p&gt;

&lt;p&gt;
Now compute the dot product between corresponding rows in the &lt;code&gt;rows_FirstId&lt;/code&gt; and &lt;code&gt;rows_SecondId&lt;/code&gt; matrices.
&lt;/p&gt;

&lt;p&gt;
Note, that in order to do pointwise multiplication in scipy.sparse you need to use the &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.multiply.html#scipy.sparse.csr_matrix.multiply"&gt;multiply&lt;/a&gt; function (along with a sum), the regular `*` operator corresponds to matrix-matrix multiplication. Also, the expected shape provided is only an array, not a matrix, so we can use &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.squeeze.html"&gt;numpy.squeeze&lt;/a&gt; to get change it (from having a single column to not having a column).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows_FirstId&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows_SecondId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="c1"&gt;# Sanity check&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;368550&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
That is it! &lt;b&gt;&lt;b&gt;We've built our magic feature.&lt;/b&gt;&lt;/b&gt; 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FIGURE_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Distribution of Similarity Matrix (f)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;distplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb960c50" class="outline-4"&gt;
&lt;h4 id="orgb960c50"&gt;From magic feature to binary predictions&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb960c50"&gt;
&lt;p&gt;
But how do we convert this feature into binary predictions? We do not have a train set to learn a model, but we have a piece of information about test set: the baseline accuracy score that you got, when submitting constant. And we also have a very strong considerations about the data generative process, so probably we will be fine even without a training set. 
&lt;/p&gt;

&lt;p&gt;
We may try to choose a thresold, and set the predictions to 1, if the feature value `f` is higher than the threshold, and 0 otherwise. What threshold would you choose? 
&lt;/p&gt;

&lt;p&gt;
How do we find a right threshold? Let's first examine this feature: print frequencies (or counts) of each value in the feature `f`.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;f_frame&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_frame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value_counts&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Value"&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Count"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tabulate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"keys"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tablefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"orgtbl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	       &lt;span class="n"&gt;showindex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;20&lt;/td&gt;
&lt;td class="org-right"&gt;183799&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;14&lt;/td&gt;
&lt;td class="org-right"&gt;183279&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;15&lt;/td&gt;
&lt;td class="org-right"&gt;852&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;19&lt;/td&gt;
&lt;td class="org-right"&gt;546&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;28&lt;/td&gt;
&lt;td class="org-right"&gt;54&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;35&lt;/td&gt;
&lt;td class="org-right"&gt;14&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;21&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fractions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;fractions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Value"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Value&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tabulate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fractions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"keys"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tablefmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"orgtbl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;showindex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	       &lt;span class="n"&gt;floatfmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;".3f"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;20.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.499&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;14.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.497&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;15.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.002&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;19.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.001&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;28.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;35.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;21.000&lt;/td&gt;
&lt;td class="org-right"&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So it looks like half the values are below 20 and half are above. We'll make our predictions by first getting a boolean array testing this case and then casting it to integers (0 is False, 1 is True).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predict_twenty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,[&lt;/span&gt;&lt;span class="s1"&gt;'pairId'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Prediction'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predict_twenty&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'predict_twenty.csv'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
But if you look at the table, it looks like 20 alone accounts for exactly half the values.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predict_only_twenty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,[&lt;/span&gt;&lt;span class="s1"&gt;'pairId'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Prediction'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predict_only_twenty&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'predict_only_twenty.csv'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is the grader output.
&lt;/p&gt;

&lt;pre class="example"&gt;
Well done! Your accuracy score is 0.998128 
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;predict_fourteen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[:,[&lt;/span&gt;&lt;span class="s1"&gt;'pairId'&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'Prediction'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predict_fourteen&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;submit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'predict_fourteen.csv'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This was the grader output.
&lt;/p&gt;
&lt;pre class="example"&gt;
Well done! Your accuracy score is 0.997298
&lt;/pre&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Finally:&lt;/b&gt;&lt;/b&gt; try to explain to yourself, why the whole thing worked out. In fact, there is no magic in this feature, and the idea to use rows in the incidence matrix can be intuitively justified.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5bfd205" class="outline-2"&gt;
&lt;h2 id="org5bfd205"&gt;Bonus&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5bfd205"&gt;
&lt;p&gt;
Interestingly, it is not the only leak in this dataset. There is another totally different way to get almost 100% accuracy. Try to find it!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6429bae" class="outline-2"&gt;
&lt;h2 id="org6429bae"&gt;What does it all mean then?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6429bae"&gt;
&lt;p&gt;
From our initial check uploading all the submissions as one (so all the ID-pairs were classified as having IDs from the same class) we saw that half the entries were 1's and half were 0's. Our incidence matrix showed that half the vectors had a similarity of 20 or more, so by predicting that all the pairs whose incidence matrix dot-products were 20 or greater were of the same class, we could predict with greater than 99% accuracy which IDs were from the same class.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>assignment dataleaks</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leakages/</guid><pubDate>Sun, 09 Sep 2018 01:31:29 GMT</pubDate></item><item><title>Data Leaks</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leaks/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgb6c4200" class="outline-2"&gt;
&lt;h2 id="orgb6c4200"&gt;What are data leaks?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb6c4200"&gt;
&lt;p&gt;
&lt;i&gt;Data Leaks&lt;/i&gt; are unexpected errors that expose extra information that wouldn't be available in production.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgafbb943" class="outline-2"&gt;
&lt;h2 id="orgafbb943"&gt;Time Series&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgafbb943"&gt;
&lt;p&gt;
Check if there are points in the training set that are in the future with regard to the test set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org538834b" class="outline-2"&gt;
&lt;h2 id="org538834b"&gt;Unexpected Information&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org538834b"&gt;
&lt;p&gt;
Sometimes what looks like a non-predictive feature might prove to be useful because of the way the data-set was created.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;ID&lt;/li&gt;
&lt;li&gt;Row Order&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org165f78d" class="outline-2"&gt;
&lt;h2 id="org165f78d"&gt;Leaderboard Probing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org165f78d"&gt;
&lt;p&gt;
This is a method to look for dataleaks based on the leader board.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org33ada27" class="outline-2"&gt;
&lt;h2 id="org33ada27"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org33ada27"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3c7ca32" class="outline-3"&gt;
&lt;h3 id="org3c7ca32"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3c7ca32"&gt;
&lt;p&gt;
Suppose that you have a credit scoring task where you have to create a machine learning model that approximates expert evaluation of an individual's creditworthiness. Which of the following can potentially be a data leakage?
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; An ID of a data point (row) in the train set corellates with the target variable (the data wasn't shuffled, this information won't work in a real-world scenario)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The first half of the data points in the train set have a score of 0 while the second half has scores &amp;gt; 0. (same as above)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Among he features you have a &lt;code&gt;company_id&lt;/code&gt;, an identifier of a company where this person works. It turns out that this feature is important and adding it to your model improves your score. (this is a normal feature, the fact that it improves your score just means it's an important feature)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2711f73" class="outline-3"&gt;
&lt;h3 id="org2711f73"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2711f73"&gt;
&lt;p&gt;
What is the most foolproof way to set up a time-series competition?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Split, train, public and private parts of the data by time. Remove all features except IDs from the test set so that participants will generate all the features based on the past and join them themselves. (you need to remove all features tfrom the test set to guarantee there isn't a data-leakage)&lt;/li&gt;

&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Make a time based split for train/test and a random split for publit/private. (this is vulnerable to leaderboard probing)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; split public and private by time, remove time from the test set. (it is possible to reverse engineer the time-ordering and exploit future-peeking)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org37344f4" class="outline-3"&gt;
&lt;h3 id="org37344f4"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org37344f4"&gt;
&lt;p&gt;
Suppose you have a binary feature classification task and it is evaluated by the logloss metric. You know that there are 10,000 entries in the public chunk of the test set and that a constant prediction of 0.3 gets a score of 1.01. The mean of the target variable in the training set is 0.44. What is the mean of the target variable in the public part  of the test data (up to 4 decimal places.)
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;0.44 (wrong)&lt;/li&gt;
&lt;li&gt;0.132 (wrong)&lt;/li&gt;
&lt;li&gt;1.33 (wrong)&lt;/li&gt;
&lt;li&gt;0.7712&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
\[
\frac{N_1}{N} = \frac{-L - \ln (1-C)}{\ln C - \ln (1 - C)}\\
= 0.7712\\
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7e705bf" class="outline-3"&gt;
&lt;h3 id="org7e705bf"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7e705bf"&gt;
&lt;p&gt;
Suppose you are solving an image classification task, what is the classification of the logo for this course?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Angry robot (wrong - should be a number)&lt;/li&gt;
&lt;li&gt;1 (wrong - check image name)&lt;/li&gt;
&lt;li&gt;3 (the URL for the image had the answer in it)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>dataleaks notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/data-leaks/</guid><pubDate>Sat, 08 Sep 2018 23:11:07 GMT</pubDate></item><item><title>Validation</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#orgdaf62d0"&gt;Validation and Overfitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org674da7d"&gt;Three Main Methods of Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#orgb5cdb96"&gt;Stratification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org388f244"&gt;Data Splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org36f5ea8"&gt;Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org8238b42"&gt;Practice Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org03ebc6f"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/validation/#org746ed2d"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdaf62d0" class="outline-2"&gt;
&lt;h2 id="orgdaf62d0"&gt;Validation and Overfitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdaf62d0"&gt;
&lt;p&gt;
To prevent your model from overfitting to the training set, you can hold out some of the training set and use it to validate the model after it has been fit to the rest of the training set.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Underfitting: your model isn't complex enough to capture the data&lt;/li&gt;
&lt;li&gt;Overfitting: your model is too complex and it is modelling noise in the data&lt;/li&gt;
&lt;li&gt;In a competition, if your model does well on the validation set but not on the test set, then it probably overfit the data you had&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org674da7d" class="outline-2"&gt;
&lt;h2 id="org674da7d"&gt;Three Main Methods of Splitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org674da7d"&gt;
&lt;p&gt;
These are methods for splitting your training set into training and validation sets. Once you have a model, re-train it over the entire training set before applying it to the test set.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdf752cb" class="outline-3"&gt;
&lt;h3 id="orgdf752cb"&gt;Holdout&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdf752cb"&gt;
&lt;p&gt;
This method just splits the data into one training and one validation set.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html"&gt;sklearn.model_selection.ShuffleSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ngroups=1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7e9228" class="outline-3"&gt;
&lt;h3 id="orge7e9228"&gt;K-Fold&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge7e9228"&gt;
&lt;p&gt;
Make &lt;i&gt;k&lt;/i&gt; splits of the training set, then use each of the validation sets while training on all the data not in the validation set. This differs from doing holdout k-times since we guarantee that the validation sets don't overlap. Uses the average score for the k-folds.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"&gt;sklearn.model_selection.KFold&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0dbec8d" class="outline-3"&gt;
&lt;h3 id="org0dbec8d"&gt;Leave One Out&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0dbec8d"&gt;
&lt;p&gt;
This is like k-folds except we always use a validation set of size 1 - so we are iterating over each point in the data set and using it as the training set. This can be useful if the data set is small.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html"&gt;sklearn.model_selection.LeaveOneOut&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb5cdb96" class="outline-2"&gt;
&lt;h2 id="orgb5cdb96"&gt;Stratification&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb5cdb96"&gt;
&lt;p&gt;
Sometimes you need to make sure your validation sets have the same distribution as your set as a whole.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;small datasets&lt;/li&gt;
&lt;li&gt;unbalanced datasects&lt;/li&gt;
&lt;li&gt;multiclass classification&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"&gt;StratifieShuffleSplit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org388f244" class="outline-2"&gt;
&lt;h2 id="org388f244"&gt;Data Splitting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org388f244"&gt;
&lt;p&gt;
If you have time-based data, there's two ways to split the training data - randomly within the entire timespan, or put the first part of the data in the training set and put the second part of the data in the validation set. If the test-set is a time that is beyond the training data, then using the time-based split will produce a model that is better for the testing data.
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Row-wise split
This is the most common case, where rows are randomly chosen from the training data. This assumes the rows are independent.&lt;/li&gt;
&lt;li&gt;Time-wise split
This is the case where you are predicting future values of a time-series. In this case, the further back in time a row is, the less like the future value it is.&lt;/li&gt;
&lt;li&gt;By-ID
In this case several rows map to one ID, and the ID maps to a target. For example, you might have several x-rays for one patient that map to one diagnosis.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
The main point of this is that you want to set up your validation set to match the way the train-test sets were split.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org36f5ea8" class="outline-2"&gt;
&lt;h2 id="org36f5ea8"&gt;Problems&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org36f5ea8"&gt;
&lt;p&gt;
The point of doing the training-validation split is that you think the validation set(s) will approximate the test set. But what if that's not true?
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6727bfa" class="outline-3"&gt;
&lt;h3 id="org6727bfa"&gt;Causes of score differences&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6727bfa"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Too little data&lt;/li&gt;
&lt;li&gt;The data is too diverse and inconsistent&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf901a8f" class="outline-3"&gt;
&lt;h3 id="orgf901a8f"&gt;Submission Differs from Validation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf901a8f"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Even K-fold validation has variation (check that the standard deviation across folds encompasses the leaderboard values)&lt;/li&gt;
&lt;li&gt;Too little data on leaderboard (nothing you can do)&lt;/li&gt;
&lt;li&gt;Train and test are from different distributions&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8238b42" class="outline-2"&gt;
&lt;h2 id="org8238b42"&gt;Practice Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8238b42"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org352f592" class="outline-3"&gt;
&lt;h3 id="org352f592"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org352f592"&gt;
&lt;p&gt;
We did a K-Fold cross validation on a huge dataset and noticed that scores on each fold are roughly the same. Which validation type is of most practical use?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; K-Fold&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Holdout&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Leave one out&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0326285" class="outline-3"&gt;
&lt;h3 id="org0326285"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0326285"&gt;
&lt;p&gt;
We did a K-fold cross validation on a medium sized dataset and noticed that the validation scores varied widely. Which validation type is the most practical to use?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Leave One Out&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; K Fold&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Houldout&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc71db56" class="outline-3"&gt;
&lt;h3 id="orgc71db56"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc71db56"&gt;
&lt;p&gt;
The features we generate depend on the train-test split. True or False?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; True&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; False&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd543efe" class="outline-3"&gt;
&lt;h3 id="orgd543efe"&gt;Which of these can indicate an expected leaderboard shuffle in a competition?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd543efe"&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Little training and/or testing data&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Most of the competitors have similar scores&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Different public/private data or target distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org03ebc6f" class="outline-2"&gt;
&lt;h2 id="org03ebc6f"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org03ebc6f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org10427ff" class="outline-3"&gt;
&lt;h3 id="org10427ff"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org10427ff"&gt;
&lt;p&gt;
Select the true statements.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; A performance increase on a fixed cross-validation split guarantees a performance increase on any cross-validation split. (You might be overfitting. You should change the splits to check for overfitting.)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The logic behind the validation split should mimic the logic behind the train-test split (this is the main rule for making a reliable validation)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Underfitting refers to not capturing enough patterns in the data&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; We use validation to estimate the quality of our model (this is the main purpose of validation)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; The model that does on the validation set is guaranteed to do the best on the test set. (The test and validation sets might have different distributions, in which case the validation won't predict the test set score)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf19d4e6" class="outline-3"&gt;
&lt;h3 id="orgf19d4e6"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf19d4e6"&gt;
&lt;p&gt;
Kaggle usually allows you to submit two final submissions that will be checked against the private leader board. One common practice is to use a model that did the best on the validation scores and another that did best on the public leader board. What is the logic behind using these two models?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; People rarely overfit the public leaderboard. You almost always have a lot of test data and it is hard to overfit.&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Validation is rarely valid in competitions. You must account for the case where validation worked and where it didn't.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The test set may have a different distribution than the target data. If this is true, then the model that did better on the public leaderboard will do better. If not, then the model that did better in validation will do better.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c9f4e2" class="outline-3"&gt;
&lt;h3 id="org8c9f4e2"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c9f4e2"&gt;
&lt;p&gt;
Suppose we have a dataset of marketing campaigns. Each campain runs for a few weeks and for each campaign our target is the number of new customers. A row in the dataset looks like this:
&lt;/p&gt;

&lt;p&gt;
&lt;i&gt;Campaign ID, Date, {some features},Number of New Customers&lt;/i&gt;
&lt;/p&gt;

&lt;p&gt;
The dataset contains multiple campaigns where the training set has the dates at the start of each campaign and the test set has the dates at the end of each campaign. Which train/test split should you use?
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Random Split&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Combined Split (Each train and test set are divided by a date and the date might be for different campaigns, so it is a combination of campaign ID and date)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; ID-based split (wrong)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Time-based split (wrong)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1d64e74" class="outline-3"&gt;
&lt;h3 id="org1d64e74"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1d64e74"&gt;
&lt;p&gt;
Which of the following can you usually identify without the leaderboard?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Different scores/optimal parameters between different folds (this is determined during validation)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Train and test target data are from different distributions (You would need the test target values to figure this out, which you won't have)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The public leaderboard score will be unreliable because there is too little data (you can check this by making the size of the folds match the size of the public test set and see the variability)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The train and test data are from different distributions (you can often figure this out during Exploratory Data Analysis)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org746ed2d" class="outline-2"&gt;
&lt;h2 id="org746ed2d"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org746ed2d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/cross_validation.html"&gt;Cross-validation in sklearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/"&gt;Model Selection for Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes validation</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/validation/</guid><pubDate>Tue, 04 Sep 2018 15:01:59 GMT</pubDate></item><item><title>Springleaf Competition</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgd3acf9c" class="outline-2"&gt;
&lt;h2 id="orgd3acf9c"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd3acf9c"&gt;
&lt;p&gt;
The data comes from the &lt;a href="https://www.kaggle.com/c/springleaf-marketing-response"&gt;Springleaf Marketing Response&lt;/a&gt; competition. Springleaf makes personal loans and wanted to be able to predict who would respond to their direct mail offers. Submissions are evaluated on the area under the ROC curve between the predicted probability and the target. The submission file should have an ID and the probability that the person would respond.
&lt;/p&gt;

&lt;pre class="example"&gt;
ID,target
1,0.35
3,0.01
6,0.93333
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>example competition notes</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/springleaf-competition/</guid><pubDate>Tue, 04 Sep 2018 13:35:25 GMT</pubDate></item><item><title>Exploratory Data Analysis</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#orga7ca7b0"&gt;Building Your Intuition About the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org6ae1a4e"&gt;Exploring Anonymized Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#orga42a7b7"&gt;Visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#org70345c8"&gt;Data Set Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#orgf6f6742"&gt;Quiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/#orgeedcf2a"&gt;Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga7ca7b0" class="outline-2"&gt;
&lt;h2 id="orga7ca7b0"&gt;Building Your Intuition About the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga7ca7b0"&gt;
&lt;p&gt;
The first thing to do is to see if you can build up a mental model of what the data is about.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0a1a14d" class="outline-3"&gt;
&lt;h3 id="org0a1a14d"&gt;Get Domain Knowledge&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0a1a14d"&gt;
&lt;p&gt;
Find out something about the topic that the data describes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge4755c2" class="outline-3"&gt;
&lt;h3 id="orge4755c2"&gt;Check if the Data is intuitive&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge4755c2"&gt;
&lt;p&gt;
See if there are any strange values and see if it makes sense given the data description.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4a6688e" class="outline-3"&gt;
&lt;h3 id="org4a6688e"&gt;Understand how the data was generated&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4a6688e"&gt;
&lt;p&gt;
Is any of it synthetic? Was the training data generated the same way as the testing data?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6ae1a4e" class="outline-2"&gt;
&lt;h2 id="org6ae1a4e"&gt;Exploring Anonymized Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6ae1a4e"&gt;
&lt;p&gt;
Organizers sometimes encode data so you can't tell what it is.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6c5093d" class="outline-3"&gt;
&lt;h3 id="org6c5093d"&gt;Try To Decode the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6c5093d"&gt;
&lt;p&gt;
You generally won't be able to figure out what the data is if it's encoded, but sometimes they were created using simple shifting schemes that will let you figure out their original values.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3a18170" class="outline-3"&gt;
&lt;h3 id="org3a18170"&gt;Explore Individual Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3a18170"&gt;
&lt;p&gt;
Even if you don't know what the data is, it's important to know what type of data it is so that you can use the right data preprocessing for your model.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dtypes.html"&gt;df.dtypes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html"&gt;df.info()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html"&gt;x.value_counts()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html"&gt;x.isnull()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga42a7b7" class="outline-2"&gt;
&lt;h2 id="orga42a7b7"&gt;Visualizations&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga42a7b7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0a8d1b7" class="outline-3"&gt;
&lt;h3 id="org0a8d1b7"&gt;Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0a8d1b7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0324b8b" class="outline-4"&gt;
&lt;h4 id="org0324b8b"&gt;Histograms&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0324b8b"&gt;
&lt;p&gt;
This is useful for seeing the shape of the data.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.hist(x)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge6ca6ac" class="outline-4"&gt;
&lt;h4 id="orge6ca6ac"&gt;Index vs Value&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge6ca6ac"&gt;
&lt;p&gt;
This will show you how well distributed the data is within the data frame. Horizontal lines indicate repeated values and empty bands show areas where none of the data had this value. If you don't have vertical lines then the data was probably shuffled.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.plot(x, '.')&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9059dc9" class="outline-4"&gt;
&lt;h4 id="org9059dc9"&gt;Index vs Value Colored By Class Label&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9059dc9"&gt;
&lt;p&gt;
If you plot the classes with different colors you can see if there are clusters and clear separations between them. (&lt;code&gt;y&lt;/code&gt; in the function call has the target values).
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(range(len(x)), x, c=y)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf7a10aa" class="outline-4"&gt;
&lt;h4 id="orgf7a10aa"&gt;Plot Other Things&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf7a10aa"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Row vs Feature&lt;/li&gt;
&lt;li&gt;Nan-values&lt;/li&gt;
&lt;li&gt;Value Counts&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org90bcaff" class="outline-3"&gt;
&lt;h3 id="org90bcaff"&gt;Relationships&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org90bcaff"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5fbfc32" class="outline-4"&gt;
&lt;h4 id="org5fbfc32"&gt;Scatter Plots&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5fbfc32"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org2ba358a"&gt;&lt;/a&gt;Pairwise Relationships&lt;br&gt;
&lt;div class="outline-text-5" id="text-org2ba358a"&gt;
&lt;p&gt;
To make it eaiser to see relationships, you can plot them in pairs.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pyplot.scatter(x1, x2)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org0c1ba14"&gt;&lt;/a&gt;Compare the Test Set&lt;br&gt;
&lt;div class="outline-text-5" id="text-org0c1ba14"&gt;
&lt;p&gt;
Plot the features and the test-set (using different colors) to see how well your training data matches your test data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgd7983ec"&gt;&lt;/a&gt;Scatter Matrix&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgd7983ec"&gt;
&lt;p&gt;
Pandas has a convenience function that will plot all the pairwise scatter-plots for you.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;pandas.scatter_matrix(X)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge4c5609" class="outline-4"&gt;
&lt;h4 id="orge4c5609"&gt;Corellation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge4c5609"&gt;
&lt;p&gt;
Plot the corellation matrix to see if there are feature-pairs that are related so you can make a new feature out of them and see if they help.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;X.corr(), pyplot.matshow()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
A straight correllation matrix might give you a sense of how correllated the features are, but it's more useful to use a clustering algorithm to see if there are groups within the correlation matrix.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0f78f2b" class="outline-4"&gt;
&lt;h4 id="org0f78f2b"&gt;Plot Statistics&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0f78f2b"&gt;
&lt;p&gt;
Try plotting mean, differences, combination counts, etc. and see if you can create groups out of them.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org70345c8" class="outline-2"&gt;
&lt;h2 id="org70345c8"&gt;Data Set Cleaning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org70345c8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgae0f01b" class="outline-3"&gt;
&lt;h3 id="orgae0f01b"&gt;Duplicated and Constant Features&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgae0f01b"&gt;
&lt;p&gt;
Sometimes a feature will have the same value in all the rows. If it's this way in both the training and test sets you can just remove it, but if there are different values in the test set you have to figure out how to handle them.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.nunique(axis&lt;/code&gt;"columns") &lt;code&gt;= 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Sometimes columns will get duplicated in which case you should drop one of them.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;x_train.T.drop_duplicates()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
This can happen with rows as well, but it can be harder to decide whether this is a mistake or not.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org601f9d8" class="outline-3"&gt;
&lt;h3 id="org601f9d8"&gt;Non-shuffled Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org601f9d8"&gt;
&lt;p&gt;
If you plot the mean as a horizontal line the data should be evenly distributed around it, if not it might not have been shuffled and there could be an inadvertent pattern in the data. You might not be able to use it, but you should understand all the things about the data that you can find out.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf6f6742" class="outline-2"&gt;
&lt;h2 id="orgf6f6742"&gt;Quiz&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf6f6742"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfb0f911" class="outline-3"&gt;
&lt;h3 id="orgfb0f911"&gt;One&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfb0f911"&gt;
&lt;p&gt;
Suppose we are given a data set with features &lt;i&gt;X&lt;/i&gt;, &lt;i&gt;Y&lt;/i&gt;, and &lt;i&gt;Z&lt;/i&gt;. Can you recover &lt;i&gt;z&lt;/i&gt; as a function of &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt;?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; Z = X/Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X - Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = X + Y&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Z = XY&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2f5a3ba" class="outline-3"&gt;
&lt;h3 id="org2f5a3ba"&gt;Two&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2f5a3ba"&gt;
&lt;p&gt;
What value do the red dots have?
0.5 (wrong)
2 (next try)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3c2f040" class="outline-3"&gt;
&lt;h3 id="org3c2f040"&gt;Three&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3c2f040"&gt;
&lt;p&gt;
What hypothesis about X can we not reject based on the plots?
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X is a counter or label encoded categorical feature&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can be the temperature (in Celsius) in different cities at different times. (the values are probably out of range)&lt;/li&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; X can take a value of zero (The log plot would have values at 0 but it doesn't)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; X takes only discrete values (the horizontal lines indicate that there are repeated values with discrete values)&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; 2 &amp;lt;= X &amp;lt; 3 happens more frequently than 3 &amp;lt;= X &amp;lt; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4560230" class="outline-3"&gt;
&lt;h3 id="org4560230"&gt;Four&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4560230"&gt;
&lt;ul class="org-ul"&gt;
&lt;li class="off"&gt;&lt;code&gt;[ ]&lt;/code&gt; Target is completely determined by coordinates (x,y)(x,y)(x,y), i.e. the label of the point is completely determined by point's position (x,y)(x,y)(x,y). Saying the same in other words: if we only had two features (x,y)(x,y)(x,y), we could build a classifier, that is accurate 100% of time.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; The top right plot is better than the top left in that everything you get from the top left can also be gotten from the top right, but not the other way around.&lt;/li&gt;
&lt;li class="on"&gt;&lt;code&gt;[X]&lt;/code&gt; standard deviation for jittering is the largest on the bottom right.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgeedcf2a" class="outline-2"&gt;
&lt;h2 id="orgeedcf2a"&gt;Links&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgeedcf2a"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html"&gt;Sorting Correlation Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>notes data</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/exploratory-data-analysis/</guid><pubDate>Tue, 04 Sep 2018 04:24:53 GMT</pubDate></item><item><title>References</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/references/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="outline-container-orgcc0d050" class="outline-2"&gt;
&lt;h2 id="orgcc0d050"&gt;Bibliography&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcc0d050"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;[B1] 1. Albon C. Machine learning with Python cookbook: practical solutions from preprocessing to deep learning. First edition. Beijing Boston Farnham: O’Reilly; 2018. 349 p.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bilbiography references</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/references/</guid><pubDate>Sat, 01 Sep 2018 19:16:46 GMT</pubDate></item><item><title>The Target</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/#org0c11099"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/#orgda919d6"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/#org75b6a9b"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/#org6274f29"&gt;Dates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0c11099" class="outline-2"&gt;
&lt;h2 id="org0c11099"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0c11099"&gt;
&lt;p&gt;
This is some stuff about what we are trying to predict. I realized that the description of the competition says that we are trying to predict the number of items sold per shop for the next month after the data, but I never said what that month is, so I'm making this just to put it somewhere.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgda919d6" class="outline-2"&gt;
&lt;h2 id="orgda919d6"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgda919d6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgea5ae6d" class="outline-3"&gt;
&lt;h3 id="orgea5ae6d"&gt;From pypi&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgea5ae6d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import pandas
import matplotlib.pyplot as pyplot
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org25322bc" class="outline-3"&gt;
&lt;h3 id="org25322bc"&gt;This project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org25322bc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from kaggler.helpers.build_training_data import Pickles
from kaggler.helpers.helpers import (
    DataKeys,
    Helpers,
    )
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge225aa1" class="outline-3"&gt;
&lt;h3 id="orge225aa1"&gt;Setting up the plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge225aa1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;% matplotlib inline
seaborn.set_style("whitegrid")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org75b6a9b" class="outline-2"&gt;
&lt;h2 id="org75b6a9b"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org75b6a9b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x_train = Helpers.unpickle(Pickles.x_train)
y_train = Helpers.unpickle(Pickles.y_train)
x_test = Helpers.unpickle(Pickles.x_test)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Although it seem unlikely, it's possible that there are months in either the test or training set that aren't shared, so I'm going to concatenate them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;concatenated = pandas.concat([x_train, x_test], axis="rows")
assert concatenated.shape == (len(x_train) + len(x_test), len(x_train.columns))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6274f29" class="outline-2"&gt;
&lt;h2 id="org6274f29"&gt;Dates&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6274f29"&gt;
&lt;p&gt;
Unfortunately I got rid of the days, which are required by the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html"&gt;to_datetime&lt;/a&gt; function provided by pandas, so I'm going to construct a new date column with a day of 1 for each date.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DataKeys.date = "Date"
DataKeys.datetime = "Date Time"
concatenated[DataKeys.date] = "01-" + concatenated.month + '-' + concatenated.year
concatenated[DataKeys.datetime] = pandas.to_datetime(
    concatenated[DataKeys.date], format="%d-%m-%Y")
Helpers.print_head(concatenated)
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;date_block_num&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;shop_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_price&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_category_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;month&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;year&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Date&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Date Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;11&lt;/td&gt;
&lt;td class="org-right"&gt;15&lt;/td&gt;
&lt;td class="org-right"&gt;1324&lt;/td&gt;
&lt;td class="org-right"&gt;499&lt;/td&gt;
&lt;td class="org-right"&gt;55&lt;/td&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;2013&lt;/td&gt;
&lt;td class="org-right"&gt;01-12-2013&lt;/td&gt;
&lt;td class="org-left"&gt;2013-12-01 00:00:00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;18&lt;/td&gt;
&lt;td class="org-right"&gt;31&lt;/td&gt;
&lt;td class="org-right"&gt;19981&lt;/td&gt;
&lt;td class="org-right"&gt;499&lt;/td&gt;
&lt;td class="org-right"&gt;43&lt;/td&gt;
&lt;td class="org-right"&gt;07&lt;/td&gt;
&lt;td class="org-right"&gt;2014&lt;/td&gt;
&lt;td class="org-right"&gt;01-07-2014&lt;/td&gt;
&lt;td class="org-left"&gt;2014-07-01 00:00:00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;32&lt;/td&gt;
&lt;td class="org-right"&gt;28&lt;/td&gt;
&lt;td class="org-right"&gt;7934&lt;/td&gt;
&lt;td class="org-right"&gt;398&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;td class="org-right"&gt;09&lt;/td&gt;
&lt;td class="org-right"&gt;2015&lt;/td&gt;
&lt;td class="org-right"&gt;01-09-2015&lt;/td&gt;
&lt;td class="org-left"&gt;2015-09-01 00:00:00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;43&lt;/td&gt;
&lt;td class="org-right"&gt;13518&lt;/td&gt;
&lt;td class="org-right"&gt;1499&lt;/td&gt;
&lt;td class="org-right"&gt;19&lt;/td&gt;
&lt;td class="org-right"&gt;01&lt;/td&gt;
&lt;td class="org-right"&gt;2014&lt;/td&gt;
&lt;td class="org-right"&gt;01-01-2014&lt;/td&gt;
&lt;td class="org-left"&gt;2014-01-01 00:00:00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;28&lt;/td&gt;
&lt;td class="org-right"&gt;25&lt;/td&gt;
&lt;td class="org-right"&gt;19927&lt;/td&gt;
&lt;td class="org-right"&gt;329&lt;/td&gt;
&lt;td class="org-right"&gt;57&lt;/td&gt;
&lt;td class="org-right"&gt;05&lt;/td&gt;
&lt;td class="org-right"&gt;2015&lt;/td&gt;
&lt;td class="org-right"&gt;01-05-2015&lt;/td&gt;
&lt;td class="org-left"&gt;2015-05-01 00:00:00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(concatenated[DataKeys.datetime].max())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So the last month in the data-set is October 2015, and we want to predict what the counts will be for November 2015.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>competition data target</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/the-target/</guid><pubDate>Fri, 31 Aug 2018 20:29:42 GMT</pubDate></item><item><title>Some Plots of the Data</title><link>https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/#org1d5cc58"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/#orgfe84b29"&gt;Build the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/#orgbd3eb87"&gt;More Helpers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/#orgfbd0b95"&gt;Features vs Target&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1d5cc58" class="outline-2"&gt;
&lt;h2 id="org1d5cc58"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1d5cc58"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import warnings
warnings.filterwarnings("ignore", message="numpy.dtype size changed")
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge9cf811" class="outline-3"&gt;
&lt;h3 id="orge9cf811"&gt;From pypi&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge9cf811"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import matplotlib.pyplot as pyplot
import numpy
import seaborn
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;%matplotlib inline
seaborn.set_style("whitegrid")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3cb160a" class="outline-3"&gt;
&lt;h3 id="org3cb160a"&gt;This project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3cb160a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from kaggler.helpers.build_training_data import Pickles
from kaggler.helpers.helpers import (
    DataKeys,
    Helpers,
    )
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfe84b29" class="outline-2"&gt;
&lt;h2 id="orgfe84b29"&gt;Build the data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfe84b29"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x_train = Helpers.unpickle(Pickles.x_train)
y_train = Helpers.unpickle(Pickles.y_train)
DataKeys.target = "Month Count"
x_train[DataKeys.target] = y_train.values
Helpers.print_head(x_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;date_block_num&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;shop_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_price&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;item_category_id&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;month&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;year&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Month Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;11&lt;/td&gt;
&lt;td class="org-right"&gt;15&lt;/td&gt;
&lt;td class="org-right"&gt;1324&lt;/td&gt;
&lt;td class="org-right"&gt;499&lt;/td&gt;
&lt;td class="org-right"&gt;55&lt;/td&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;2013&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;18&lt;/td&gt;
&lt;td class="org-right"&gt;31&lt;/td&gt;
&lt;td class="org-right"&gt;19981&lt;/td&gt;
&lt;td class="org-right"&gt;499&lt;/td&gt;
&lt;td class="org-right"&gt;43&lt;/td&gt;
&lt;td class="org-right"&gt;07&lt;/td&gt;
&lt;td class="org-right"&gt;2014&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;32&lt;/td&gt;
&lt;td class="org-right"&gt;28&lt;/td&gt;
&lt;td class="org-right"&gt;7934&lt;/td&gt;
&lt;td class="org-right"&gt;398&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;td class="org-right"&gt;09&lt;/td&gt;
&lt;td class="org-right"&gt;2015&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;43&lt;/td&gt;
&lt;td class="org-right"&gt;13518&lt;/td&gt;
&lt;td class="org-right"&gt;1499&lt;/td&gt;
&lt;td class="org-right"&gt;19&lt;/td&gt;
&lt;td class="org-right"&gt;01&lt;/td&gt;
&lt;td class="org-right"&gt;2014&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;28&lt;/td&gt;
&lt;td class="org-right"&gt;25&lt;/td&gt;
&lt;td class="org-right"&gt;19927&lt;/td&gt;
&lt;td class="org-right"&gt;329&lt;/td&gt;
&lt;td class="org-right"&gt;57&lt;/td&gt;
&lt;td class="org-right"&gt;05&lt;/td&gt;
&lt;td class="org-right"&gt;2015&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbd3eb87" class="outline-2"&gt;
&lt;h2 id="orgbd3eb87"&gt;More Helpers&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbd3eb87"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def make_figure_and_axis(x_label, y_label, title, figsize=(10, 8)):
    """make a matplotlib figure

    Args:
     x_label (str): label for the x-axis
     y_label (str): label for the y-axis
     title (str): title for the plot
     figsize: tuple of width, height
    Returns:
     tuple: figure, axis
    """
    figure = pyplot.figure(figsize=figsize)
    axe = figure.gca()
    axe.set_xlabel(x_label)
    axe.set_ylabel(y_label)
    axe.set_title(title)
    return figure, axe
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfbd0b95" class="outline-2"&gt;
&lt;h2 id="orgfbd0b95"&gt;Features vs Target&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfbd0b95"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga768b2f" class="outline-3"&gt;
&lt;h3 id="orga768b2f"&gt;Date Block&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga768b2f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axis = make_figure_and_axis("Date Block", "Count", "Date-Block Counts")
grid = seaborn.catplot(ax=axis, x=DataKeys.date_block, kind="count", data=x_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The counts represent the number of shop-product pairs per-month, which isn't really what we want. We want the count per-product per month. As an intermediary step, why don't we look at the total count as it changes over time.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;date_group = x_train.groupby(DataKeys.date_block)
summed = date_group.sum()
summed = summed.reset_index()
figure, axis = make_figure_and_axis("Date Block", "Monthly Count",
				    "Total Items Sold Per Month")
grid = seaborn.relplot(x=DataKeys.date_block, y=DataKeys.target, ax=axis,
		       data=summed, kind="line")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This seems to indicate that sales are going down overall over time. Those spikes are interesting, maybe the months would be interesting. First we need to re-join the month and year together so the sorting of the x-axis will work okay.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DataKeys.date = "Date"
x_train[DataKeys.date] = x_train.year + "-" + x_train.month
month_grouped = x_train.groupby(DataKeys.date)
month_summed = month_grouped.sum().reset_index()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;top_two = month_summed.sort_values(DataKeys.target, ascending=False)[:2]
figure, axis = make_figure_and_axis("Month", "Count", "Items Sold Per Month",
				    figsize=(12,10))

pyplot.xticks(rotation=45, ha="right")
grid = seaborn.relplot(x=DataKeys.date, y=DataKeys.target, data=month_summed,
		       ax=axis, kind="line")
axis.axvline(top_two.index[0], color='r')
line = axis.axvline(top_two.index[1], color='r')
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Helpers.print_table(top_two[[DataKeys.date, DataKeys.target]])
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt; &lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Date&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Month Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;11&lt;/td&gt;
&lt;td class="org-right"&gt;2013-12&lt;/td&gt;
&lt;td class="org-right"&gt;147909&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;23&lt;/td&gt;
&lt;td class="org-right"&gt;2014-12&lt;/td&gt;
&lt;td class="org-right"&gt;134785&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Perhaps not surprisingly, the month of December, when holiday sales spike, is the month with the most sales.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgaaae86d" class="outline-3"&gt;
&lt;h3 id="orgaaae86d"&gt;Shop&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgaaae86d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;group = x_train.groupby(DataKeys.shop).sum().reset_index().sort_values(DataKeys.target)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axis = make_figure_and_axis("Shop", "Count", "Shop Sales For Total Time")
grid = seaborn.relplot(x=DataKeys.shop, y=DataKeys.target, data=group,
		       ax=axis)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
There appears to be seven or eight shops that dominate the sales.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org66e3953" class="outline-3"&gt;
&lt;h3 id="org66e3953"&gt;Item Category&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org66e3953"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;group = x_train.groupby(DataKeys.item_category).sum().reset_index()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axis = make_figure_and_axis("Category", "Count", "Category Sales For Total Time")
grid = seaborn.relplot(x=DataKeys.item_category, y=DataKeys.target, data=group,
		       ax=axis)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Some kind of weirdly coincidental linear relationship between the category ID and the total sales. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga08f673" class="outline-3"&gt;
&lt;h3 id="orga08f673"&gt;Biggest Over Time&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga08f673"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga9f648f" class="outline-4"&gt;
&lt;h4 id="orga9f648f"&gt;Category&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga9f648f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;category_group = x_train.groupby([DataKeys.date, DataKeys.item_category]).sum().reset_index()
biggest = category_group.iloc[category_group[DataKeys.target].idxmax()]
biggest_category = category_group[category_group.item_category_id == biggest.item_category_id]
figure, axis = make_figure_and_axis("Month", "Count", "Category {} Sales For Total Time".format(biggest[DataKeys.item_category]))
pyplot.xticks(rotation=45, ha="right")
grid = seaborn.relplot(x=DataKeys.date, y=DataKeys.target, data=biggest_category,
		       ax=axis, kind="line")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Even the most popular item declines over time.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;big = biggest_category[DataKeys.target]
big_max = big.max()
big_min = big.min()
difference = big_max - big_min
print("Max: {}".format(big_max))
print("Min: {}".format(big_min))
print("Difference: {}".format(difference))
print("Percent Decline: {:.1f} %".format(100 * (difference/big_max)))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Max: 28987.0
Min: 5413.0
Difference: 23574.0
Percent Decline: 81.3 %
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>competition plotting data</category><guid>https://necromuralist.github.io/Kaggle-Competitions/posts/some-plots-of-the-data/</guid><pubDate>Wed, 29 Aug 2018 21:19:26 GMT</pubDate></item></channel></rss>